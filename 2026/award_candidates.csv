ID,Title,Authors,Abstract
135,MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion,"Bardienus Pieter Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, Jerome Revaud","Structure-from-Motion (SfM), a task aiming at jointly recovering camera poses and 3D geometry of a scene given a set of images, remains a hard problem with still many open challenges despite decades of significant progress. The traditional solution for SfM consists of a complex pipeline of minimal solvers which tends to propagate errors and fails when images do not sufficiently overlap, have too little motion, etc. Recent methods have attempted to revisit this paradigm, but we empirically show that they fall short of fixing these core issues. In this paper, we propose instead to build upon a recently released foundation model for 3D vision that can robustly produce local 3D reconstructions and accurate matches. We introduce a low-memory approach to accurately align these local reconstructions in a global coordinate system. We further show that such foundation models can serve as efficient image retrievers without any overhead, reducing the overall complexity from quadratic to linear. Overall, our novel SfM pipeline is simple, scalable, fast and truly unconstrained, i.e. it can handle any collection of images, ordered or not. Extensive experiments on multiple benchmarks show that our method provides steady performance across diverse settings, especially outperforming existing methods in small- and medium-scale settings."
205,"ARC-Flow : Articulated, Resolution-Agnostic, Correspondence-Free Matching and Interpolation of 3D Shapes under Flow Fields","Adam Hartshorne, Allen Paul, Tony Shardlow, Neill D. F. Campbell","This work presents a unified framework for the unsupervised prediction of physically plausible interpolations between two 3D articulated shapes and the automatic estimation of dense correspondence between them. Interpolation is modelled as a diffeomorphic transformation using a smooth, time-varying flow field governed by Neural Ordinary Differential Equations (ODEs). This ensures topological consistency and non-intersecting trajectories while accommodating hard constraints, such as volume preservation, and soft constraints, e.g physical priors. Correspondence is recovered using an efficient Varifold formulation, that is effective on high-fidelity surfaces with differing parameterizations. A simple skeleton structure augments the source shape, imposing physically motivated constraints on the deformation field and aiding in resolving symmetric ambiguities, without requiring skinning weights or prior knowledge of the skeleton's target pose configuration.Qualitative and quantitative results demonstrate competitive or superior performance over existing state-of-the-art approaches in both shape correspondence and interpolation tasks across standard datasets."
230,Spurfies: Sparse-View Surface Reconstruction using Local Geometry Priors,"Kevin Raj, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Eric Lenssen","We introduce Spurfies, a novel method for sparse-view surface reconstruction that disentangles appearance and geometry information to utilize local geometry priors trained on synthetic data. Recent research heavily focuses on 3D reconstruction using dense multi-view setups, typically requiring hundreds of images. However, these methods often struggle with few-view scenarios. Existing sparse-view reconstruction techniques often rely on multi-view stereo networks that need to learn joint priors for geometry and appearance from a large amount of data. In contrast, we introduce a neural point representation that disentangles geometry and appearance to train a local geometry prior using a subset of the synthetic ShapeNet dataset only. During inference, we utilize this surface prior as additional constraint for surface and appearance reconstruction from sparse input views via differentiable volume rendering, restricting the space of possible solutions. We validate the effectiveness of our method on the DTU dataset and demonstrate that it outperforms previous work by 35\% in surface quality while achieving competitive novel view synthesis quality. Moreover, in contrast to previous works, our method can be applied to larger, unbounded scenes, such as Mip-NeRF360."
256,3D Reconstruction with Spatial Memory,"Hengyi Wang, Lourdes Agapito","We present Spann3R, a novel approach for dense 3D reconstruction from ordered or unordered image collections. Built on the DUSt3R paradigm, Spann3R uses a transformer-based architecture to directly regress a global pointmap from images without any prior knowledge of the scene or camera parameters. Unlike DUSt3R, which predicts per image-pair pointmaps each expressed in its local coordinate frame, Spann3R can predict a per-image pointmaps expressed in a global coordinate system, thus eliminating the need for optimization-based global alignment. The key idea of Spann3R is to manage an external spatial memory that learns to keep track of all previous relevant 3D information. Spann3R then queries this spatial memory to predict the 3D structure of the next frame in a global coordinate system. Taking advantage of DUSt3R's pre-trained weights, and further fine-tuning on a subset of datasets, Spann3R shows competitive performance and generalization ability on various unseen datasets and is able to process ordered image collections in real-time."
286,OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of Microstructures by Fusing White Light Interferometry and Optical Microscopy,"Shuo Chen, Yijin Li, Guofeng Zhang","White Light Interferometry (WLI) is a precise optical tool for measuring the 3D topography of microstructures. However, conventional WLI cannot capture the natural color of a sample's surface, which is essential for many microscale research applications that require both 3D geometry and color information. Previous methods have attempted to overcome this limitation by modifying WLI hardware and analysis software, but these solutions are often costly. In this work, we address this challenge from a computer vision multi-modal reconstruction perspective for the first time. We introduce OpticFusion, a novel approach that uses an additional digital optical microscope (OM) to achieve 3D reconstruction with natural color textures using multi-view WLI and OM images. Our method employs a two-step data association process to obtain the poses of WLI and OM data. By leveraging the neural implicit representation, we fuse multi-modal data and apply color decomposition technology to extract the sample's natural color. Tested on our multi-modal dataset of various microscale samples, OpticFusion achieves detailed 3D reconstructions with color textures. Our method provides an effective tool for practical applications across numerous microscale research fields. The source code and our real-world dataset are available at https://github.com/zju3dv/OpticFusion."
202,LangOcc: Open Vocabulary Occupancy Estimation via Volume Rendering,"Simon Boeder, Fabian Gigengack, Benjamin Risse","The 3D occupancy estimation task has become an important challenge in the area of vision-based autonomous driving recently.However, most existing camera-based methods rely on costly 3D voxel labels or LiDAR scans for training, limiting their practicality and scalability.Moreover, most methods are tied to a predefined set of classes which they can detect.In this work we present a novel approach for open vocabulary occupancy estimation called LangOcc, that is trained only via camera images, and can detect arbitrary semantics via vision-language alignment.In particular, we distill the knowledge of the strong vision-language aligned encoder CLIP into a 3D occupancy model via differentiable volume rendering. Our model estimates vision-language aligned features in a 3D voxel grid using only images.It is trained in a weakly-supervised manner by rendering our estimations back to 2D space, where features can easily be aligned with CLIP.This training mechanism automatically supervises the scene geometry, allowing for a straight-forward and powerful training method without any explicit geometry supervision.LangOcc outperforms LiDAR-supervised competitors in open vocabulary occupancy with a mAP of $22.7$ by a large margin ($+4.3 \%$), solely relying on vision-based training.We also achieve a mIoU score of $11.84$ on the Occ3D-nuScenes dataset, surpassing previous vision-only semantic occupancy estimation methods ($+1.71\%$), despite not being limited to a specific set of categories."
138,MeshUp: Multi-Target Mesh Deformation via Blended Score Distillation,"Hyunwoo Kim, Itai Lang, Thibault Groueix, Noam Aigerman, Vladimir Kim, Rana Hanocka","We propose MeshUp, a technique that deforms a 3D mesh towards multipletarget concepts, and intuitively controls the region where each concept isexpressed. Conveniently, the concepts can be defined as either text queries,e.g., “a dog” and “a turtle,” or inspirational images, and the local regions canbe selected as any number of vertices on the mesh. We can effectively controlthe influence of the concepts and mix them together using a novel scoredistillation approach, referred to as the Blended Score Distillation (BSD). BSDoperates on each attention layer of the denoising U-Net of a diffusion modelas it extracts and injects the per-objective activations into a unified denoisingpipeline from which the deformation gradients are calculated. To localize theexpression of these activations, we create a probabilistic Region of Interest(ROI) map on the surface of the mesh, and turn it into 3D-consistent masksthat we use to control the expression of these activations. We demonstratethe effectiveness of BSD empirically and show that it can deform variousmeshes towards multiple objectives."
500,An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion,"Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang","We introduce a new approach for generating realistic 3D models with UV maps through a representation termed 'Object Images'. This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation."