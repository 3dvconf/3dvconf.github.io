poster,session,number,title,authors,abstract,forum
Poster 1,Oral 1,237,Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization,"Daniel Lichy, Hang Su, Abhishek Badki, Jan Kautz, Orazio Gallo","Wide field-of-view (FoV) cameras efficiently capture large portions of the scene, which makes them attractive in multiple domains, such as automotive and robotics. For such applications, estimating depth from multiple images is a critical task, and therefore, a large amount of ground truth (GT) data is available. Unfortunately, most of the GT data is for pinhole cameras, making it impossible to properly train depth estimation models for large-FoV cameras. We propose the first method to train a stereo depth estimation model on the widely available pinhole data, and to generalize it to data captured with larger FoVs. Our intuition is simple: We warp the training data to a canonical, large-FoV representation and augment it to allow a single network to reason about diverse types of distortions that otherwise would prevent generalization. We show strong generalization ability of our approach on both indoor and outdoor datasets, which was not possible with previous methods.",https://openreview.net/forum?id=R3wd1ZvU6F
Poster 1,Oral 1,80,LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D Signals,"Arjun Karpur, Guilherme Perrotta, Ricardo Martin-Brualla, Howard Zhou, André Araujo","Finding localized correspondences across different images of the same object is crucial to understand its geometry. In recent years, this problem has seen remarkable progress with the advent of deep learning-based local image features and learnable matchers. Still, learnable matchers often underperform when there exists only small regions of co-visibility between image pairs (i.e. wide camera baselines). To address this problem, we leverage recent progress in coarse single-view geometry estimation methods. We propose LFM-3D, a Learnable Feature Matching framework that uses models based on graph neural networks and enhances their capabilities by integrating noisy, estimated 3D signals to boost correspondence estimation. When integrating 3D signals into the matcher model, we show that a suitable positional encoding is critical to effectively make use of the low-dimensional 3D information. We experiment with two different 3D signals - normalized object coordinates and monocular depth estimates - and evaluate our method on large-scale (synthetic and real) datasets containing object-centric image pairs across wide baselines. We observe strong feature matching improvements compared to 2D-only methods, with up to +6% total recall and +28% precision at fixed recall. Additionally, we demonstrate that the resulting improved correspondences lead to much higher relative posing accuracy for in-the-wild image pairs - up to 8.6% compared to the 2D-only approach.",https://openreview.net/forum?id=cJX2drc7y4
Poster 1,Oral 1,198,SCENES: Subpixel Correspondence Estimation with Epipolar Supervision,"Dominik A. Kloepfer, João F. Henriques, Dylan Campbell","Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.",https://openreview.net/forum?id=RMgrY57XLH
Poster 2,Oral 2,361,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,"Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R. Oswald, Andreas Geiger, Marc Pollefeys","Neural implicit representations have recently become popular in simultaneous localization and mapping (SLAM), especially in dense visual SLAM. However, previous works in this direction either rely on RGB-D sensors, or require a separate monocular SLAM approach for camera tracking and do not produce high-fidelity dense 3D scene reconstruction. In this paper, we present NICER-SLAM, a dense RGB SLAM system that simultaneously optimizes for camera poses and a hierarchical neural implicit map representation, which also allows for high-quality novel view synthesis. To facilitate the optimization process for mapping, we integrate additional supervision signals including easy-to-obtain monocular geometric cues and optical flow, and also introduce a simple warping loss to further enforce geometry consistency. Moreover, to further boost performance in complicated indoor scenes, we also propose a local adaptive transformation from signed distance functions (SDFs) to density in the volume rendering equation. On both synthetic and real-world datasets we demonstrate strong performance in dense mapping, tracking, and novel view synthesis, even competitive with recent RGB-D SLAM systems.",https://openreview.net/forum?id=GKqofA1izi
Poster 2,Oral 2,337,SlimmeRF: Slimmable Radiance Fields,"Shiran Yuan, Hao Zhao","Neural Radiance Field (NeRF) and its variants have recently emerged as successful methods for novel view synthesis and 3D scene reconstruction. However, most current NeRF models either achieve high accuracy using large model sizes, or achieve high memory-efficiency by trading off accuracy. This limits the applicable scope of any single model, since high-accuracy models might not fit in low-memory devices, and memory-efficient models might not satisfy high-quality requirements. To this end, we present SlimmeRF, a model that allows for instant test-time trade-offs between model size and accuracy through slimming, thus making the model simultaneously suitable for scenarios with different computing budgets. We achieve this through a newly proposed algorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank of the model's tensorial representation gradually during training. We also observe that our model allows for more effective trade-offs in sparse-view scenarios, at times even achieving higher accuracy after being slimmed. We credit this to the fact that erroneous information such as floaters tend to be stored in components corresponding to higher ranks.",https://openreview.net/forum?id=52JGaL9Vhp
Poster 2,Oral 2,324,RaNeuS: Ray-adaptive Neural Surface Reconstruction,,,https://openreview.net/forum?id=Pply4Vv6Np
Poster 2,Oral 2,107,Few-View Object Reconstruction with Unknown Categories and Camera Poses,"Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, Yuke Zhu","While object reconstruction has made great strides in recent years, current methods typically require densely captured images and/or known camera poses, and generalize poorly to novel object categories. To step toward object reconstruction in the wild, this work explores reconstructing general real-world objects from a few images without known camera poses or object categories. The crux of our work is solving two fundamental 3D vision problems -- shape reconstruction and pose estimation -- in a unified approach. Our approach captures the synergies of these two problems: reliable camera pose estimation gives rise to accurate shape reconstruction, and the accurate reconstruction, in turn, induces robust correspondence between different views and facilitates pose estimation. Our method FORGE predicts 3D features from each view and leverages them in conjunction with the input images to establish cross-view correspondence for estimating relative camera poses. The 3D features are then transformed by the estimated poses into a shared space and are fused into a neural radiance field. The reconstruction results are rendered by volume rendering techniques, enabling us to train the model without 3D shape ground-truth. Our experiments show that FORGE reliably reconstructs objects from five views. Our pose estimation method outperforms existing ones by a large margin. The reconstruction results under predicted poses are comparable to the ones using ground-truth poses. The performance on novel testing categories matches the results on categories seen during training.",https://openreview.net/forum?id=cbqMMORP3p
Poster 3,Oral 3,316,Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case,"Min Li, Jiaqi Yang, Laurent Kneip","Multi-perspective cameras with potentially non-overlapping fields of view have become an important exteroceptive sensing modality in a number of applications such as intelligent vehicles, drones, and mixed reality headsets. In this work, we challenge one of the basic assumptions made in these scenarios, which is that the multi-camera rig is rigid. More specifically, we are considering the problem of estimating the relative pose between a static non-rigid rig in different spatial orientations while taking into account the effect of gravity onto the system. The deformable physical connections between each camera and the body center are approximated by a simple cantilever model, and inserted into the generalized epipolar constraint. Our results lead us to the important insight that the latent parameters of the deformation model, meaning the gravity vector in both views, become observable. We present a concise analysis of the observability of all variables based on noise, outliers, and rig rigidity for two different algorithms. The first one is a vision-only alternative, while the second one makes use of additional gravity measurements. To conclude, we demonstrate the ability to sense gravity in a real-world example, and discuss practical implications.",https://openreview.net/forum?id=A1NWxAAEon
Poster 3,Oral 3,44,POCO: 3D Pose and Shape Estimation with Confidence,"Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas","The regression of 3D Human Pose and Shape (HPS) from an image is becoming increasingly accurate. This makes the results useful for downstream tasks like human action recognition or 3D graphics. Yet, no regressor is perfect, and accuracy can be affected by ambiguous image evidence or by poses and appearance that are unseen during training. Most current HPS regressors, however, do not report the confidence of their outputs, meaning that downstream tasks cannot differentiate accurate estimates from inaccurate ones. To address this, we develop POCO, a novel framework for training HPS regressors to estimate not only a 3D human body, but also their confidence, in a single feed-forward pass. Specifically, POCO estimates both the 3D body pose and a per-sample variance. The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing uncertainty that is highly correlated to pose reconstruction quality. The POCO framework can be applied to any HPS regressor and here we evaluate it by modifying HMR, PARE, and CLIFF. In all cases, training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose. While this was not our goal, the improvement is modest but consistent. Our main motivation is to provide uncertainty estimates for downstream tasks; we demonstrate this in two ways: (1) We use the confidence estimates to bootstrap HPS training. Given unlabelled image data, we take the confident estimates of a POCO-trained regressor as pseudo ground truth. Retraining with this automatically-curated data improves accuracy. (2) We exploit uncertainty in video pose estimation by automatically identifying uncertain frames (e.g. due to occlusion) and inpainting these from confident frames.",https://openreview.net/forum?id=ost30uBkNa
Poster 3,Oral 3,69,RelPose++: Recovering 6D Poses from Sparse-view Observations,"Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani","We address the task of estimating 6D camera poses from sparse-view image sets (2-8 images). This task is a vital pre-processing stage for nearly all contemporary (neural) reconstruction algorithms but remains challenging given sparse views, especially for objects with visual symmetries and texture-less surfaces. We build on the recent RelPose framework which learns a network that infers distributions over relative rotations over image pairs. We extend this approach in two key ways; first, we use attentional transformer layers to process multiple images jointly, since additional views of an object may resolve ambiguous symmetries in any given image pair (such as the handle of a mug that becomes visible in a third view). Second, we augment this network to also report camera translations by defining an appropriate coordinate system that decouples the ambiguity in rotation estimation from translation prediction. Our final system results in large improvements in 6D pose prediction over prior art on both seen and unseen object categories and also enables pose estimation and 3D reconstruction for in-the-wild objects.",https://openreview.net/forum?id=uZ8ESZnyxh
Poster 3,Oral 3,226,Fusing Directions and Displacements in Translation Averaging,"Manam, Lalit and Govindu, Venu Madhav",,https://openreview.net/forum?id=WUdsYz44zR
Poster 4,Oral 4,108,Multi-Body Neural Scene Flow,"Kavisha Vidanapathirana, Shin-Fang Chng, Xueqian Li, Simon Lucey","The test-time optimization of scene flow - using a coordinate network as a neural prior - has gained popularity due to its simplicity, lack of dataset bias, and state-of-the-art performance. We observe, however, that although coordinate networks capture general motions by implicitly regularizing the scene flow predictions to be spatially smooth, the neural prior by itself is unable to identify the underlying multi-body rigid motions present in real-world data. To address this, we show that multi-body rigidity can be achieved without the cumbersome and brittle strategy of constraining the SE(3) parameters of each rigid body as done in previous works. This is achieved by regularizing the scene flow optimization to encourage isometry in flow predictions for rigid bodies. This strategy enables multi-body rigidity in scene flow while maintaining a continuous flow field, hence allowing dense long-term scene flow integration across a sequence of point clouds. We conduct extensive experiments on real-world datasets and demonstrate that our approach outperforms the state-of-the-art in 3D scene flow and long-term point-wise 4D trajectory prediction.",https://openreview.net/forum?id=vk4NxoZXJm
Poster 4,Oral 4,273,"GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment Draping","Ruochen Chen, Liming Chen, Shaifali Parashar","Recent neural, physics-based modeling of garment deformations allows faster and visually aesthetic results as opposed to the existing methods. Material-specific parameters are used by the formulation to control the garment inextensibility. This delivers unrealistic results with physically implausible stretching. Oftentimes, the draped garment is pushed inside the body which is either corrected by an expensive post-processing, thus adding to further inconsistent stretching; or by deploying a separate training regime for each body type, restricting its scalability. Additionally, the flawed skinning process deployed by existing methods produces incorrect results on loose garments.",https://openreview.net/forum?id=qzhH0bElx9
Poster 4,Oral 4,370,Objects with Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting,"Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng, Stephan Richter, Shenlong Wang, German Ros","Reconstructing an object from photos and placing it virtually in a new environment goes beyond the standard novel view synthesis task as the appearance of the object has to not only adapt to the novel viewpoint but also to the new lighting conditions and yet evaluations of inverse rendering methods rely on novel view synthesis data or simplistic synthetic datasets for quantitative analysis. This work presents a real-world dataset for measuring the reconstruction and rendering of objects for relighting. To this end, we capture the environment lighting and ground truth images of the same objects in multiple environments allowing to reconstruct the objects from images taken in one environment and quantify the quality of the rendered views for the unseen lighting environments. Further, we introduce a simple baseline composed of off-the-shelf methods and test several state-of-the-art methods on the relighting task and show that novel view synthesis is not a reliable proxy to measure performance.",https://openreview.net/forum?id=wjOunOQJLk
Poster 5,Oral 5,72,Enhancing Generalizability of Representation Learning for Data-Efficient 3D Scene Understanding,,,https://openreview.net/forum?id=bhu0moRrkv
Poster 5,Oral 5,173,OPDMulti: Openable Part Detection for Multiple Objects,"Xiaohao Sun, Hanxiao Jiang, Manolis Savva, Angel Xuan Chang","Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.",https://openreview.net/forum?id=zPo32mHBDr
Poster 5,Oral 5,19,Scalable 3D Panoptic Segmentation with Superpoint Graph Clustering,"Damien Robert, Hugo Raguet, Loic Landrieu","We introduce a highly efficient method for panoptic segmentation of large 3D point clouds by redefining this task as a scalable graph clustering problem. This approach can be trained using only local auxiliary tasks, thereby eliminating the resource-intensive instance-matching step during training. Moreover, our formulation can easily be adapted to the superpoint paradigm, further increasing its efficiency. This allows our model to process scenes with millions of points and thousands of objects in a single inference. Our method, called SuperCluster, achieves a new state-of-the-art panoptic segmentation performance for two indoor scanning datasets: 50.1 PQ (+7.8) for S3DIS Area~5, and 58.7 PQ (+25.2) for ScanNetV2. We also set the first state-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and DALES. With only 209k parameters, our model is over 30 times smaller than the best-competing method and trains up to 15 times faster.",https://openreview.net/forum?id=YfXQpAl64X
Poster 5,Oral 5,7,"DeDoDe: Detect, Don't Describe --- Describe, Don't Detect for Local Feature Matching","Johan Edstedt, Georg Bökman, Mårten Wadenbäck, Michael Felsberg","Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks.",https://openreview.net/forum?id=ZrKy10MyR3
Poster 6,Oral 6,229,ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation,"Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang","We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method significantly improves the performance on joint hand and articulated object poses estimation over the existing state-of-the-art methods.",https://openreview.net/forum?id=NaktXABbkQ
Poster 6,Oral 6,117,SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling,"Eduard Gabriel Bazavan, Andrei Zanfir, Thiemo Alldieck, Teodor Alexandru Szente, Mihai Zanfir, Cristian Sminchisescu","We present \emph{SPHEAR}, an accurate, differentiable parametric statistical 3D human head model, enabled by a novel 3D registration method based on spherical embeddings. We shift the paradigm away from the classical Non-Rigid Registration methods, which operate under various surface priors, increasing reconstruction fidelity and minimizing required human intervention. Additionally, SPHEAR is a \emph{complete} model that allows not only to sample diverse synthetic head shapes and facial expressions, but also gaze directions, high-resolution color textures, surface normal maps, and hair cuts represented in detail, as strands. SPHEAR can be used for automatic realistic visual data generation, semantic annotation, and general reconstruction tasks. Compared to state-of-the-art approaches, our components are fast and memory efficient, and experiments support the validity of our design choices and the accuracy of registration, reconstruction and generation techniques.",https://openreview.net/forum?id=iPMFkcozz2
Poster 6,Oral 6,169,A Local Appearance Model for Volumetric Capture of Diverse Hairstyles,"Ziyan Wang, Giljoo Nam, Aljaz Bozic, Chen Cao, Jason Saragih, Michael Zollhoefer, Jessica Hodgins","Hair plays a significant role in personal identity and appearance, making it an essential component of high-quality, photorealistic avatars. Existing approaches either focus on modeling the facial region only or rely on personalized models, limiting their generalizability and scalability. In this paper, we present a novel method for creating high-fidelity avatars with diverse hairstyles. Our method leverages the local similarity across different hairstyles and learns a universal hair appearance prior from multi-view captures of hundreds of people. This prior model takes 3D-aligned features as input and generates dense radiance fields conditioned on a sparse point cloud with color. As our model splits different hairstyles into local primitives and builds prior at that level, it is capable of handling various hair topologies. Through experiments, we demonstrate that our model captures a diverse range of hairstyles and generalizes well to challenging new hairstyles. Empirical results show that our method improves the state-of-the-art approaches in capturing and generating photorealistic, personalized avatars with complete hair.",https://openreview.net/forum?id=YMMnd2urj9
,,,,,,
Poster 1,Spotlight 1,253,ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation,,,https://openreview.net/forum?id=vrizS6bdW0
Poster 1,Spotlight 1,81,DehazeNeRF: Multi-image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields,,,https://openreview.net/forum?id=Z2Ip2XGv0D
Poster 1,Spotlight 1,175,Efficient 3D Articulated Human Generation with Layered Surface Volumes,,,https://openreview.net/forum?id=ulSI1Ikc7q
Poster 1,Spotlight 1,89,FastHuman: Reconstructing High-Quality Clothed Human in Minutes,,,https://openreview.net/forum?id=dCfj39ZwvR
Poster 1,Spotlight 1,265,Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains,,,https://openreview.net/forum?id=2kHcblZUx8
Poster 1,Spotlight 1,315,3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data,,,https://openreview.net/forum?id=J25q5XSu9Y
Poster 2,Spotlight 2,325,PRAGO: Differentiable multi-view pose optimization from objectness detections,,,https://openreview.net/forum?id=V7AtGCdZMv
Poster 2,Spotlight 2,244,PathFusion:Path-Consistent Lidar-Camera Deep Feature Fusion,,,https://openreview.net/forum?id=QTllStKk2g
Poster 2,Spotlight 2,269,LumiGAN: Unconditional Generation of Relightable 3D Human Faces,,,https://openreview.net/forum?id=oCKQEroekk
Poster 2,Spotlight 2,67,3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera,,,https://openreview.net/forum?id=nDdvloSVou
Poster 3,Spotlight 3,333,Ray-Patch: An Efficient Querying for Light Field Transformers,,,https://openreview.net/forum?id=mGHqf88CXI
Poster 3,Spotlight 3,65,MELON: NeRF with Unposed Images in SO(3),,,https://openreview.net/forum?id=WTuIZA5wmJ
Poster 3,Spotlight 3,4,LabelMaker: Automatic Semantic Label Generation from RGB-D Trajectories,"Silvan Weder, Hermann Blum, Francis Engelmann, Marc Pollefeys","Semantic annotations are indispensable to train or evaluate perception models, yet very costly to acquire. This work introduces a fully automated 2D/3D labeling framework that, without any human intervention, can generate labels for RGB-D scans at equal (or better) level of accuracy than comparable manually annotated datasets such as ScanNet. Our approach is based on an ensemble of state-of-the-art segmentation models and 3D lifting through neural rendering. We demonstrate the effectiveness of our LabelMaker pipeline by generating significantly better labels for the ScanNet datasets and automatically labelling the previously unlabeled ARKitScenes dataset. Code and models are available at this https URL",https://openreview.net/forum?id=do2AszWcCk
Poster 3,Spotlight 3,249,MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching,,,https://openreview.net/forum?id=WdcwA2qO9N
Poster 4,Spotlight 4,105,PACE: Human and Camera Motion Estimation from in-the-wild Videos,,,https://openreview.net/forum?id=UXotWr7iU7
Poster 4,Spotlight 4,109,Handbook on Leveraging Lines for Two-View Relative Pose Estimation,,,https://openreview.net/forum?id=1EHfJLyuiv
Poster 4,Spotlight 4,26,SALUDA: Surface-based Automotive LiDAR Unsupervised Domain Adaptation,,,https://openreview.net/forum?id=ivrz6O3oPS
Poster 4,Spotlight 4,91,S4C: Self-Supervised Semantic Scene Completion with Neural Fields,,,https://openreview.net/forum?id=BSnmzmGu4P
Poster 4,Spotlight 4,222,NCRF: Neural Contact Radiance Fields for Free-viewpoint Rendering of Hand-Object Interactions,,,https://openreview.net/forum?id=VuAvQQsOME
Poster 4,Spotlight 4,74,Sparse 3D Reconstruction via Object-Centric Ray Sampling,,,https://openreview.net/forum?id=rTlE3hVHGp
Poster 6,Spotlight 6,42,Zero-BEV: Zero-shot Projection of any First-Person Modality to BEV Maps,,,https://openreview.net/forum?id=9S8vVfxNRn
Poster 6,Spotlight 6,217,Select-Sliced Wasserstein Distance for Point Cloud Learning,,,https://openreview.net/forum?id=1bYJBatudn
Poster 6,Spotlight 6,368,Out of the Room: Generalizing Event-based Dynamic Motion Segmentation for Complex Scenes,,,https://openreview.net/forum?id=bysritUPSG
Poster 6,Spotlight 6,376,Self-supervised Learning of Neural Implicit Feature Fields for Camera Pose Refinement,,,https://openreview.net/forum?id=1yrGI0gcQx
Poster 6,Spotlight 6,121,Partial-View Object View Synthesis via Filtering Inversion,,,https://openreview.net/forum?id=1EhpvW7GLU
Poster 6,Spotlight 6,103,Physically Plausible Full-Body Hand-Object Interaction Synthesis,,,https://openreview.net/forum?id=FYxrOXfGCR
,,,,,,
Poster 1,,200,3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual Transformer Learning,,,https://openreview.net/forum?id=IEa36N1ca1
Poster 1,,54,Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching,,,https://openreview.net/forum?id=ZxnPCQ0qW6
Poster 1,,64,SUCRe: Leveraging Scene Structure for Underwater Color Restoration,,,https://openreview.net/forum?id=5Sw0R7YAdA
Poster 1,,180,Quantum-Hybrid Stereo Matching with Nonlinear Regularization and Spatial Pyramids,,,https://openreview.net/forum?id=rd9XyZ4QC9
Poster 1,,323,MACS: Mass Conditioned 3D Hand and Object Motion Synthesis,,,https://openreview.net/forum?id=Garp39P9bi
Poster 1,,230,Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On,,,https://openreview.net/forum?id=NaTYJEZxJr
Poster 1,,43,Classical Photometric Stereo in Point Lighting Environments: Error Analysis and Mitigation,,,https://openreview.net/forum?id=9VCrlkxKz3
Poster 1,,145,Physics-based Indirect Illumination for Inverse Rendering,,,https://openreview.net/forum?id=WPR0seg46C
Poster 1,,162,Test-Time Augmentation for 3D Point Cloud Classification and Segmentation,,,https://openreview.net/forum?id=i3C23LEi2f
Poster 1,,206,Lang3DSG: Language-based contrastive pre-training for 3D scene graph prediction,,,https://openreview.net/forum?id=ijcP5spmjW
Poster 1,,56,InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering,,,https://openreview.net/forum?id=OBAaH5Rqb2
Poster 1,,310,Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration,,,https://openreview.net/forum?id=uETr3JrCD5
Poster 1,,87,Controllable Dynamic Appearance for Neural 3D Portraits,,,https://openreview.net/forum?id=47pk9EN8AU
Poster 1,,78,TextMesh: Generation of Realistic 3D Meshes From Text Prompts,,,https://openreview.net/forum?id=SciaO8QwJp
Poster 1,,353,Neural Field Regularization by Denoising for 3D Sparse-View X-ray Computed Tomography,,,https://openreview.net/forum?id=2QM3W8TZpl
Poster 1,,202,DeepDR: Deep Structure-Aware RGB-D Inpainting for Diminished Reality,,,https://openreview.net/forum?id=0LMRkLo0yI
Poster 1,,68,Interaction Replica: Tracking human–object interaction and scene changes from human motion,,,https://openreview.net/forum?id=woYMlqqJ8h
Poster 1,,259,CombiNeRF: a Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis,,,https://openreview.net/forum?id=yfQzRlz4hG
Poster 2,,277,NeVRF: Neural Video-based Radiance Fields for Long-duration Sequences,,,https://openreview.net/forum?id=uYrb7kpm8L
Poster 2,,77,Fast High Dynamic Range Radiance Fields for Dynamic Scenes,,,https://openreview.net/forum?id=gpUQbYUQJO
Poster 2,,148,Improved Scene Landmark Detection for Camera Localization,,,https://openreview.net/forum?id=hlm9mkxYMV
Poster 2,,143,"Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search",,,https://openreview.net/forum?id=VMC2VBIlBA
Poster 2,,12,Mirror-Aware Neural Humans,,,https://openreview.net/forum?id=3pAF5JuicF
Poster 2,,218,Stable Surface Regularization for Fast Few-Shot NeRF,,,https://openreview.net/forum?id=Al2oUWcUyJ
Poster 2,,396,– Visual Tomography – Physically Faithful Volumetric Models of Partially Translucent Objects,,,https://openreview.net/forum?id=GNVETIyUnH
Poster 2,,171,Unsupervised Representation Learning for Diverse Deformable Shape Collections,,,https://openreview.net/forum?id=cWROnbzuHo
Poster 2,,22,Developability Approximation for Neural Implicits through Rank Minimization,,,https://openreview.net/forum?id=nTc8wsSsmq
Poster 2,,93,Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization,,,https://openreview.net/forum?id=KZWKcXTfn9
Poster 2,,341,Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans,,,https://openreview.net/forum?id=fzS4jyEX1L
Poster 2,,61,PhoMoH: Implicit Photorealistic 3D Models of Human Heads,,,https://openreview.net/forum?id=yqXgkUZPBH
Poster 2,,281,DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown Lighting,,,https://openreview.net/forum?id=Ofii8vYFUH
Poster 2,,203,Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems,,,https://openreview.net/forum?id=0cA20oiVzL
Poster 2,,247,GHuNeRF:Generalizable Human NeRF from a Monocular Video,,,https://openreview.net/forum?id=eCjmS4QKbf
Poster 2,,48,TADA! Text to Animatable Digital Avatars,,,https://openreview.net/forum?id=1pY6b73JNm
Poster 2,,288,Event-based Visual Odometry on Non-holonomic Ground Vehicles,,,https://openreview.net/forum?id=lZ5Ne1EQ9D
Poster 2,,137,Incorporating Rotation Invariance with Non-invariant Networks for Point Clouds,,,https://openreview.net/forum?id=SeBMnTEfk8
Poster 2,,298,UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization,,,https://openreview.net/forum?id=PQfo0DipCF
Poster 3,,160,MuVieCAST: Multi-View Consistent Artistic Style Transfer,,,https://openreview.net/forum?id=tr9QoIRVTh
Poster 3,,221,HOC-Search: Efficient CAD Model and Pose Retrieval from RGB-D Scans,,,https://openreview.net/forum?id=ViFuwg5uxa
Poster 3,,184,Fast Relative Pose Estimation using Relative Depth,,,https://openreview.net/forum?id=52nVZuxrbF
Poster 3,,177,Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models,,,https://openreview.net/forum?id=FmRQVDWgcP
Poster 3,,303,Unsupervised 3D Keypoint Discovery with Multi-View Geometry,,,https://openreview.net/forum?id=GwU3tCpg8d
Poster 3,,260,TECA: Text-Guided Generation and Editing of Compositional 3D Avatars,,,https://openreview.net/forum?id=2xMloZcHn7
Poster 3,,170,Compositional 3D Scene Generation using Locally Conditioned Diffusion,,,https://openreview.net/forum?id=HamFWyuiXb
Poster 3,,36,Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects,,,https://openreview.net/forum?id=X2zWP6qi01
Poster 3,,122,Oriented-grid Encoder for 3D Implicit Representations,,,https://openreview.net/forum?id=Lxpty9BJ5Y
Poster 3,,135,Robust Point Cloud Processing through Positional Embedding,,,https://openreview.net/forum?id=JER79I1hMg
Poster 3,,228,Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture,,,https://openreview.net/forum?id=wgS2rcsD36
Poster 3,,164,Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion,,,https://openreview.net/forum?id=rwv7TyP8cu
Poster 3,,284,GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar,,,https://openreview.net/forum?id=g2UsFOBNH2
Poster 3,,234,NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes,,,https://openreview.net/forum?id=5gYl4Pg3mK
Poster 3,,37,MonoLSS: Learnable Sample Selection For Monocular 3D Detection,,,https://openreview.net/forum?id=DZJgsp7qKD
Poster 3,,92,Continuous Cost Aggregation for Dual-Pixel Disparity Extraction,,,https://openreview.net/forum?id=JXJDGxZvQi
Poster 3,,28,GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues,,,https://openreview.net/forum?id=Gci7GY9vD9
Poster 3,,299,Control3Diff: Learning Controllable 3D Diffusion Models from Single-view Images,,,https://openreview.net/forum?id=AtR5VRRmVG
Poster 4,,174,"Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable, Model-Free Approach using RGB Images",,,https://openreview.net/forum?id=lDTFL2uS13
Poster 4,,276,Learning based Infinite Terrain Generation with Level of Detailing,,,https://openreview.net/forum?id=TKynRJRgAK
Poster 4,,272,A Benchmark Grocery Dataset of Realworld Point Clouds from Single View,,,https://openreview.net/forum?id=ak03s44aMa
Poster 4,,339,PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Urban Scene Reconstruction,,,https://openreview.net/forum?id=zJhBMfrKSo
Poster 4,,20,Dynamic Prototype Adaptation with Distillation for Few-shot Point Cloud Segmentation,,,https://openreview.net/forum?id=tWdFYPtgu3
Poster 4,,216,Mixing-Denoising Generalizable Occupancy Networks,,,https://openreview.net/forum?id=DichCJ3biJ
Poster 4,,196,SimpleEgo: Predicting probabilistic body pose from egocentric cameras,,,https://openreview.net/forum?id=TnmuK3lrcF
Poster 4,,183,Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos,,,https://openreview.net/forum?id=jLC16x9X5q
Poster 4,,189,PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression,,,https://openreview.net/forum?id=jB6lnRl1kG
Poster 4,,373,A Cross Branch Fusion-based Contrastive Framework for Point Cloud Self-supervised Learning,,,https://openreview.net/forum?id=HcsJrOcJBK
Poster 4,,90,Exploit Spatiotemporal Contextual Information for 3D Single Object Tracking via Memory Networks,,,https://openreview.net/forum?id=iDSB0xho0B
Poster 4,,205,Robust and Object-aware Motion Generation using Neural Pose Descriptors,,,https://openreview.net/forum?id=KB6L99zaXy
Poster 4,,128,Purposer: Putting Human Motion Generation in Context,,,https://openreview.net/forum?id=MaKBxaXEb0
Poster 4,,320,NeRF-Feat: 6D Object Pose Estimation using Feature Rendering,,,https://openreview.net/forum?id=QoCkOLpKKE
Poster 4,,47,TeCH: Text-guided Reconstruction of Lifelike Clothed Humans,,,https://openreview.net/forum?id=0ys9E2b8qa
Poster 4,,393,Hyper-SNBRDF: Hypernetwork for Neural BRDF using Sinusoidal Activation,,,https://openreview.net/forum?id=mjLojqHyzv
Poster 4,,246,PU-SDF: Arbitrary-Scale Uniformly Upsampling Point Clouds via Signed Distance Functions,,,https://openreview.net/forum?id=SH2e62Xp9c
Poster 4,,179,BLiSS: Bootstrapped Linear Shape Space,,,https://openreview.net/forum?id=Lpu1vkA1KM
Poster 5,,130,RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation,,,https://openreview.net/forum?id=vQFgkAg6FV
Poster 5,,62,Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding,,,https://openreview.net/forum?id=hE2zNKwfnT
Poster 5,,396,– Visual Tomography – Physically Faithful Volumetric Models of Partially Translucent Objects,,,https://openreview.net/forum?id=GNVETIyUnH
Poster 5,,17,Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis,,,https://openreview.net/forum?id=uN0mE04Umt
Poster 5,,197,Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion,,,https://openreview.net/forum?id=pJxA0zQLvY
Poster 5,,264,Photometric visibility matrix for the automatic selection of optimal viewpoints,,,https://openreview.net/forum?id=tOCTzfjnf4
Poster 5,,331,Self-Supervised Learning of Skeleton-Aware Morphological Representation for 3D Neuron Segments,,,https://openreview.net/forum?id=SD4whyVhPQ
Poster 5,,181,Correspondence-free online human motion retargeting,,,https://openreview.net/forum?id=Cockj7Ux7M
Poster 5,,302,Coherent Enhancement of Depth Images and Normal Maps\\using Second-order Geometric Models on Weighted Finite Graphs,,,https://openreview.net/forum?id=Ty42Er8nLX
Poster 5,,178,Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature,,,https://openreview.net/forum?id=GAnLy7jw8i
Poster 5,,343,BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds,,,https://openreview.net/forum?id=urLpYKlBCi
Poster 5,,208,Generating Continual Human Motion in Diverse 3D Scenes,,,https://openreview.net/forum?id=INbrhygt1K
Poster 5,,150,Practical Measurement and Neural Encoding of Hyperspectral Skin Reflectance,,,https://openreview.net/forum?id=2ikxgJL1wp
Poster 5,,33,Synthesizing physically plausible human motions in 3D scenes,,,https://openreview.net/forum?id=lMyrioN4KC
Poster 5,,317,IS-NEAR: Implicit Semantic Neural Engine and Multi-Sensor Data Rendering with 3D Global Feature,,,https://openreview.net/forum?id=3R3QUaCIvs
Poster 5,,312,ActiveNeuS: Neural Signed Distance Fields for Active Stereo,,,https://openreview.net/forum?id=TZ2fQ50kTU
Poster 5,,156,MixRT: Mixed Neural Representations For Real-Time NeRF Rendering,,,https://openreview.net/forum?id=29bLJOkCNO
Poster 5,,27,Color-NeuS: Reconstructing Neural Implicit Surfaces with Color,,,https://openreview.net/forum?id=QTdCH9H6Yf
Poster 6,,51,HumanReg: Self-supervised Non-rigid Registration of Human Point Cloud,,,https://openreview.net/forum?id=8aRbSTVItQ
Poster 6,,55,LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation,,,https://openreview.net/forum?id=hqgrS1SXMZ
Poster 6,,138,Range-Agnostic Multi-View Depth Estimation with Keyframe Selection,,,https://openreview.net/forum?id=rVIwpCOXrU
Poster 6,,338,DAC: Detector-Agnostic Spatial Covariances for Deep Local Features,,,https://openreview.net/forum?id=LVhQe3WTsZ
Poster 6,,191,OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection,,,https://openreview.net/forum?id=jGifzPNIjo
Poster 6,,291,Deep Event Visual Odometry,,,https://openreview.net/forum?id=r07xjKaFAF
Poster 6,,18,PanoSSC: Exploring Monocular Panoptic 3D Scene  Reconstruction for Autonomous Driving,,,https://openreview.net/forum?id=y2mMDXmWFC
Poster 6,,123,Addressing Low-Shot MVS by Detecting and Completing Planar Surfaces,,,https://openreview.net/forum?id=0UY9S0Khql
Poster 6,,58,YOLO-6D-Pose:Enhancing YOLO for Single-Stage Monocular Multi-Object 6D Pose Estimation,,,https://openreview.net/forum?id=36lRaXHl3D
Poster 6,,330,CoARF: Controllable 3D Artistic Style Transfer for Radiance Fields,,,https://openreview.net/forum?id=g7bBX87MdL
Poster 6,,100,Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor,,,https://openreview.net/forum?id=Pa792k1A7L
Poster 6,,223,Geometrically Consistent Partial Shape Matching,,,https://openreview.net/forum?id=gNki68gZiO
Poster 6,,287,Occlusion Resilient 3D Human Pose Estimation,,,https://openreview.net/forum?id=C46XnAjUvG
Poster 6,,66,RIVQ-VAE: Discrete Rotation-invariant 3D Representation Learning,,,https://openreview.net/forum?id=E4eKEG5nCv
Poster 6,,371,DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid,,,https://openreview.net/forum?id=nh1w8BWYwz
Poster 6,,60,SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes,,,https://openreview.net/forum?id=l0s0IwjiUr
Poster 6,,268,CloSe: A 3D Clothing Segmentation Dataset and Model,"Dimitrije Antić, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin, Gerard Pons-Moll","3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at this https URL.",https://openreview.net/forum?id=i3w5bkQu7Y