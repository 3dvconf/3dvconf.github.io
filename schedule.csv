poster,session,number,title,Authors,absract,forum
Poster 1,Oral 1,237,Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization,"Daniel Lichy, Hang Su, Abhishek Badki, Jan Kautz, Orazio Gallo","Wide field-of-view (FoV) cameras efficiently capture large portions of the scene, which makes them attractive in multiple domains, such as automotive and robotics. For such applications, estimating depth from multiple images is a critical task, and therefore, a large amount of ground truth (GT) data is available. Unfortunately, most of the GT data is for pinhole cameras, making it impossible to properly train depth estimation models for large-FoV cameras. We propose the first method that allows us to train a model on the widely available pinhole data, and to generalize to data captured with larger FoVs. Our intuition is simple: The training data can be warped to a large-FoV, canonical representation, where the network can learn to reason about distortions, provided that the appropriate data augmentation is used. Qualitative and quantitative analysis shows the strong generalization ability of our approach on both indoors and outdoors datasets, which was not possible with previous methods.",https://openreview.net/forum?id=R3wd1ZvU6F
Poster 1,Oral 1,80,LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D Signals,"Arjun Karpur, Guilherme Mendeleh Perrotta, Ricardo Martin Brualla, Howard Zhou, Andre Araujo","Finding localized correspondences across different images of the same object is crucial to understand its geometry. In recent years, this problem has seen remarkable progress with the advent of deep learning based local image features and learnable matchers. Still, learnable matchers often underperform when there exists only small regions of co-visibility between image pairs (i.e., wide camera baselines). To address this problem, we leverage recent progress in coarse single-view geometry estimation methods. We propose LFM-3D, a Learnable Feature Matching framework that uses models based on graph neural networks and enhances their capabilities by integrating noisy, estimated 3D signals to boost correspondence estimation. When integrating 3D signals into the matcher model, we show that a suitable positional encoding is critical to effectively make use of the low-dimensional 3D information. We experiment with two different 3D signals - normalized object coordinates and monocular depth estimates - and evaluate our method on large-scale (synthetic and real) datasets containing object-centric image pairs across wide baselines. We observe strong feature matching improvements compared to 2D-only methods, with up to +6 total recall and +28 precision at fixed recall. Additionally, we demonstrate that the resulting improved correspondences lead to much higher relative posing accuracy for in-the-wild image pairs - up to 8.6 compared to the 2D-only approach.",https://openreview.net/forum?id=cJX2drc7y4
Poster 1,Oral 1,198,SCENES: Subpixel Correspondence Estimation with Epipolar Supervision,"Dominik Kloepfer, Dylan Campbell, Joao F. Henriques","Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, \eg, depth maps or point clouds, and only require camera pose information, which can be obtained from odometry or GPS. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.",https://openreview.net/forum?id=RMgrY57XLH
Poster 2,Oral 2,361,NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM,"Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R. Oswald, Andreas Geiger, Marc Pollefeys","Neural implicit representations have recently become popular in simultaneous localization and mapping (SLAM), especially in dense visual SLAM. However, existing works either rely on RGB-D sensors or require a separate monocular SLAM approach for camera tracking, and fail to produce high-fidelity 3D dense reconstructions. To address these shortcomings, we present NICER-SLAM, a dense RGB SLAM system that simultaneously optimizes for camera poses and a hierarchical neural implicit map representation, which also allows for high-quality novel view synthesis. To facilitate the optimization process for mapping, we integrate additional supervision signals including easy-to-obtain monocular geometric cues and optical flow, and also introduce a simple warping loss to further enforce geometric consistency. Moreover, to further boost performance in complex large-scale scenes, we also propose a local adaptive transformation from signed distance functions (SDFs) to density in the volume rendering equation. On multiple challenging indoor and outdoor datasets, NICER-SLAM demonstrates strong performance in dense mapping, novel view synthesis, and tracking, even competitive with recent RGB-D SLAM systems.",https://openreview.net/forum?id=GKqofA1izi
Poster 2,Oral 2,337,SlimmeRF: Slimmable Radiance Fields,"Shiran Yuan, Hao Zhao","Neural Radiance Field (NeRF) and its variants have recently emerged as successful methods for novel view synthesis and 3D scene reconstruction. However, most current NeRF models either achieve high accuracy using large model sizes, or achieve high memory-efficiency by trading off accuracy. This limits the applicable scope of any single model, since high-accuracy models might not fit in low-memory devices, and memory-efficient models might not satisfy high-quality requirements. To this end, we present SlimmeRF, a model that allows for instant test-time trade-offs between model size and accuracy through slimming, thus making the model simultaneously suitable for scenarios with different computing budgets. We achieve this through a newly proposed algorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank of the model's tensorial representation gradually during training. We also observe that our model allows for more effective trade-offs in sparse-view scenarios, at times even achieving higher accuracy after being slimmed. We credit this to the fact that erroneous information such as floaters tend to be stored in components corresponding to higher ranks. Our implementation is available at anonymous.4open.science/r/SlimmeRF-E593.",https://openreview.net/forum?id=52JGaL9Vhp
Poster 2,Oral 2,324,RaNeuS: Ray-adaptive Neural Surface Reconstruction,"Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari","Our objective is to leverage the radiance field \eg NeRF to reconstruct detailed 3D surfaces in addition to producing the standard novel view renderings.There have been related methods that perform such task, usually by utilizing a signed distance field (SDF). However, the state-of-the-art approaches still fail to correctly reconstruct the finer details such as the leaves, ropes or nets. Aiming at solving this problem, we propose to adaptively adjust the regularization on signed distance field (SDF) with respect to pixel-wise PSNR consequently, balancing the two objectives in order to generate accurate and detailed meshes.Considering that different methods formulate the projection from SDF to radiance with a global factor, we improve this with a local function so that the projection becomes adjustable with respect to different 3D locations during optimization and allowing the gradients from regions with well-learned radiance renderings effectively back-propagate to SDF fields.The two contributions are extensively evaluated on both synthetic and real datasets. Among the related work attaining similar results, we achieve the state-of-the-art on both novel view synthesis and geometric reconstruction accuracy.",https://openreview.net/forum?id=Pply4Vv6Np
Poster 2,Oral 2,107,Few-View Object Reconstruction with Unknown Categories and Camera Poses,"Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, Yuke Zhu","While object reconstruction has made great strides in recent years, current methods typically require densely captured images and/or known camera poses, and generalize poorly to novel object categories. To step toward object reconstruction in the wild, this work explores reconstructing general real-world objects from a few images without known camera poses or object categories. The crux of our work is solving two fundamental 3D vision problems --- shape reconstruction and pose estimation --- in a unified approach. Our approach captures the synergies of these two problems: reliable camera pose estimation gives rise to accurate shape reconstruction, and the accurate reconstruction, in turn, induces robust correlations between different views and facilitates pose estimation. Our method FORGE predicts 3D features from each view and leverages them in conjunction with the input images to establish cross-view correlations for estimating relative camera poses. The 3D features are then transformed by the estimated poses into a shared space and are fused into a neural radiance field. The reconstruction results are rendered by volume rendering techniques, enabling us to train the model without 3D shape ground-truth. Our experiments on both real and synthetic datasets, as well as in the wild images, show that FORGE reliably reconstructs objects from five views. Our pose estimation method outperforms existing ones by a large margin. The reconstruction results under predicted poses are comparable to the ones using ground-truth poses. And the performance on novel testing categories matches the results on categories seen during training.",https://openreview.net/forum?id=cbqMMORP3p
Poster 3,Oral 3,316,Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case,"Min Li, Jiaqi Yang, Laurent Kneip","Multi-perspective cameras with potentially non-overlapping fields of view have become an important exteroceptive sensing modality in a number of applications such as intelligent vehicles, drones, and mixed reality headsets. In this work, we challenge one of the basic assumptions made in these scenarios, which is that the multi-camera rig is rigid. More specifically, we are considering the problem of estimating the relative pose between a static non-rigid rig in different spatial orientations while taking into account the effect of gravity onto the system. The deformable physical connections between each camera and the body center are approximated by a simple cantilever model, and inserted into the generalized epipolar constraint. Our results lead us to the important insight that the latent parameters of the deformation model, meaning the gravity vector in both views, become observable. We present a concise analysis of the observability of all variables based on noise, outliers, and rig rigidity for two different algorithms. The first one is a vision-only alternative, while the second one makes use of additional gravity measurements. To conclude, we demonstrate the ability to sense gravity in a real-world example, and discuss practical implications.",https://openreview.net/forum?id=A1NWxAAEon
Poster 3,Oral 3,44,POCO: 3D Pose and Shape Estimation with Confidence,"Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas","The regression of 3D Human Pose and Shape (HPS) from an image is becoming increasingly accurate. This makes the results useful for downstream tasks like human action recognition or 3D graphics. Yet, no regressor is perfect, and accuracy can be affected by ambiguous image evidence or by poses and appearance that are unseen during training. Most current HPS regressors, however, do not report the confidence of their outputs, meaning that downstream tasks cannot differentiate accurate estimates from inaccurate ones. To address this, we develop POCO, a novel framework for training HPS regressors to estimate not only a 3D human body, but also their confidence, in a single feed-forward pass. Specifically, POCO estimates both the 3D body pose and a per-sample variance. The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing uncertainty that is highly correlated to pose reconstruction quality. The POCO framework can be applied to any HPS regressor and here we evaluate it by modifying HMR, PARE, and CLIFF. In all cases, training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose. While this was not our goal, the improvement is modest but consistent. Our main motivation is to provide uncertainty estimates for downstream tasks we demonstrate this in two ways: (1) We use the confidence estimates to bootstrap HPS training. Given unlabelled image data, we take the confident estimates of a POCO-trained regressor as pseudo ground truth. Retraining with this automatically-curated data improves accuracy.(2) We exploit uncertainty in video pose estimation by automatically identifying uncertain frames (e.g. due to occlusion) and “inpainting” these from confident frames. Code and models will be available for research.",https://openreview.net/forum?id=ost30uBkNa
Poster 3,Oral 3,69,RelPose++: Recovering 6D Poses from Sparse-view Observations,"Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani","We address the task of estimating 6D camera poses from sparse-view image sets (2-8 images). This task is a vital pre-processing stage for nearly all contemporary (neural) reconstruction algorithms but remains challenging given sparse views, especially for objects with visual symmetries and texture-less surfaces. We build on the recent RelPose framework which learns a network that infers distributions over relative rotations over image pairs. We extend this approach in two key ways first, we use attentional transformer layers to process multiple images jointly, since additional views of an object may resolve ambiguous symmetries in any given image pair (such as the handle of a mug that becomes visible in a third view). Second, we augment this network to also report camera translations by defining an appropriate coordinate system that decouples the ambiguity in rotation estimation from translation prediction. Our final system results in large improvements in 6D pose prediction over prior art on both seen and unseen object categories and also enables pose estimation and 3D reconstruction for in-the-wild objects.",https://openreview.net/forum?id=uZ8ESZnyxh
Poster 3,Oral 3,226,Fusing Directions and Displacements in Translation Averaging,"Lalit Manam, Venu Madhav Govindu","Translation averaging solves for 3D camera translations given many pairwise relative translation directions. The mismatch between inputs (directions) and output estimates (absolute translations) makes translation averaging a challenging problem, which is often addressed by comparing either directions or displacements using relaxed cost functions that are relatively easy to optimize. However, the distinctly different nature of the cost functions leads to varied behaviour under different baselines and noise conditions. In this paper, we argue that translation averaging can benefit from a fusion of the two approaches. Specifically, we recursively fuse the individual updates suggested by direction and displacement-based methods based on uncertainty. The uncertainty of each estimate is modelled by the inverse of the Hessian of the corresponding optimization problem. As a result, our method utilizes the advantages of both methods in a principled manner. The superiority of our translation averaging scheme is demonstrated via the improved accuracies of camera translations on benchmark datasets compared to the state-of-the-art methods.",https://openreview.net/forum?id=WUdsYz44zR
Poster 4,Oral 4,108,Multi-Body Neural Scene Flow,"Kavisha Vidanapathirana, Shin-Fang Chng, Xueqian Li, Simon Lucey","The runtime optimization of scene flow---using a coordinate network as a neural prior---has gained popularity due to its simplicity, lack of dataset bias, and state-of-the-art performance. We observe, however, that although coordinate networks capture general motions by implicitly regularizing the scene flow predictions to be spatially smooth, the neural prior by itself is unable to identify the underlying multi-body rigid motions present in real-world data. To address this, we show that multi-body rigidity can be achieved without the cumbersome and brittle strategy of constraining the $SE(3)$ parameters of each rigid body as done in previous works. This is achieved by regularizing the scene flow optimization to encourage isometry in flow predictions for rigid bodies. This strategy enables multi-body rigidity in scene flow while maintaining a continuous flow field, hence allowing dense long-term scene flow integration across a sequence of point clouds. We conduct extensive experiments on real-world datasets and demonstrate that our approach outperforms the state-of-the-art in 3D scene flow and long-term point-wise 4D trajectory prediction.",https://openreview.net/forum?id=vk4NxoZXJm
Poster 4,Oral 4,273,"GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment Draping","Ruochen Chen, Liming Chen, Shaifali Parashar","Recent neural, physics-based modeling of garment deformations allows faster and visually aesthetic results as opposed to the existing methods. Material-specific parameters are used by the formulation to control the garment inextensibility. This delivers unrealistic results with physically implausible stretching. Oftentimes, the draped garment is pushed inside the body which is either corrected by an expensive post-processing, thus adding to further inconsistent stretching or by deploying a separate training regime for each body type, restricting its scalability. Additionally, the flawed skinning process deployed by existing methods produces incorrect results on loose garments.In this paper, we introduce a geometrical constraint to the existing formulation that is collision-aware and imposes garment inextensibility wherever possible. Thus, we obtain realistic results where draped clothes stretch only while covering bigger body regions. Furthermore, we propose a geometry-aware garment skinning method by defining a body-garment closeness measure which works for all garment types, especially the loose ones.",https://openreview.net/forum?id=qzhH0bElx9
Poster 4,Oral 4,370,Objects with Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting,"Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng, Stephan R. Richter, Shenlong Wang, German Ros","Reconstructing an object from photos and placing it virtually in a new environment goes beyond the standard novel view synthesis task as the appearance of the object has to not only adapt to the novel viewpoint but also to the new lighting conditions and yet evaluations of inverse rendering methods rely on novel view synthesis data or simplistic synthetic datasets for quantitative analysis.This work presents a real-world dataset for measuring the reconstruction and rendering of objects for relighting.To this end, we capture the environment lighting and ground truth images of the same objects in multiple environments allowing to reconstruct the objects from images taken in one environment and quantify the quality of the rendered views for the unseen lighting environments.Further, we introduce a simple baseline composed of off-the-shelf methods and test several state-of-the-art methods on the relighting task and show that novel view synthesis is not a reliable proxy to measure performance.",https://openreview.net/forum?id=wjOunOQJLk
Poster 5,Oral 5,72,Enhancing Generalizability of Representation Learning for Data-Efficient 3D Scene Understanding,"Yunsong Wang, Na Zhao, Gim Hee Lee","The field of self-supervised 3D representation learning has emerged as a promising solution to alleviate the challenge presented by the scarcity of extensive, well-annotated datasets. However, it continues to be hindered by the lack of diverse, large-scale, real-world 3D scene datasets for source data. To address this shortfall, we propose Generalizable Representation Learning (GRL), where we devise a generative Bayesian network to produce diverse synthetic scenes with real-world patterns, and conduct pre-training with a joint objective. By jointly learning a coarse-to-fine contrastive learning task and an occlusion-aware reconstruction task, the model is primed with transferable, geometry-informed representations. Post pre-training on synthetic data, the acquired knowledge of the model can be seamlessly transferred to two principal downstream tasks associated with 3D scene understanding, namely 3D object detection and 3D semantic segmentation, using real-world benchmark datasets. A thorough series of experiments robustly display our method's consistent superiority over existing state-of-the-art pre-training approaches.",https://openreview.net/forum?id=bhu0moRrkv
Poster 5,Oral 5,173,OPDMulti: Openable Part Detection for Multiple Objects,"Xiaohao Sun, Hanxiao Jiang, Manolis Savva, Angel X Chang","Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.",https://openreview.net/forum?id=zPo32mHBDr
Poster 5,Oral 5,19,Scalable 3D Panoptic Segmentation with Superpoint Graph Clustering,"Damien Robert, Hugo Raguet, Loic Landrieu","We introduce a highly efficient method for panoptic segmentation of large 3D point clouds by redefining  this task as a scalable graph clustering problem. This approach can be trained using only local auxiliary tasks, thereby eliminating the resource-intensive instance-matching step during training. Moreover, our formulation can easily be adapted to the superpoint paradigm, further increasing its efficiency. This allows our model to process scenes with millions of points and thousands of objects in a single inference. Our method, called SuperCluster, achieves a new state-of-the-art panoptic segmentation performance for two indoor scanning datasets: 46.3 PQ (+4.0) for S3DIS Area 5, and 54.5 PQ (+21.0) for ScanNetV2.We also set the first state-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and DALES.With only 209k parameters, our model is over 30 times smaller than the best-competing method and trains up 15 times faster. Our code and pretrained models are available at link-upon-publication.",https://openreview.net/forum?id=YfXQpAl64X
Poster 5,Oral 5,7,"DeDoDe: Detect, Don't Describe --- Describe, Don't Detect for Local Feature Matching","Johan Edstedt, Georg Bökman, Mårten Wadenbäck, Michael Felsberg","Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene.One of the main challenges with keypoint detection is the formulation of the learning objective.Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage.In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network.Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided in the supplementary material.",https://openreview.net/forum?id=ZrKy10MyR3
Poster 6,Oral 6,229,ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation,"Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang","We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method significantly improves the performance on joint hand and articulated object poses estimation over the existing state-of-the-art methods.",https://openreview.net/forum?id=NaktXABbkQ
Poster 6,Oral 6,117,SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling,"Eduard Gabriel Bazavan, Andrei Zanfir, Teodor Alexandru Szente, Mihai Zanfir, Thiemo Alldieck, Cristian Sminchisescu","We present $\emph{SPHEAR}$, an accurate, differentiable parametric statistical 3D human head model, enabled by a novel 3D registration method based on spherical embeddings. We shift the paradigm away from the classical Non-Rigid Registration methods, which operate under various surface priors, increasing reconstruction fidelity and minimizing required human intervention. Additionally, SPHEAR is a $\emph{complete}$ model that allows not only to sample diverse synthetic head shapes and facial expressions, but also gaze directions, high-resolution color textures, surface normal maps, and hair cuts represented in detail, as strands. SPHEAR can be used for automatic realistic visual data generation, semantic annotation, and general reconstruction tasks. Compared to state-of-the-art approaches, our components are fast and memory efficient, and experiments support the validity of our design choices and the accuracy of registration, reconstruction and generation techniques.",https://openreview.net/forum?id=iPMFkcozz2
Poster 6,Oral 6,169,A Local Appearance Model for Volumetric Capture of Diverse Hairstyles,"Ziyan Wang, Giljoo Nam, Aljaz Bozic, Chen Cao, Jason Saragih, Michael Zollhöfer, Jessica K. Hodgins","Hair plays a significant role in personal identity and appearance, making it an essential component of high-quality, photorealistic human head avatars. Existing approaches either focus on modeling the facial region only or rely on personalized models, limiting their generalizability and scalability. In this paper, we present a novel method for creating high-fidelity volumetric avatars with photorealistic appearances of diverse hairstyles. Our method leverages the local similarity across different hairstyles and learns a universal hair appearance prior with multi-view captures of hundreds of people. This prior model takes 3D-aligned features as input and generates dense radiance fields conditioned on a sparse point cloud with color. As our model splits different hairstyles into local primitives and build prior at that level, it is capable of handling various hair topology. Through experiments, we demonstrate that our model captures a diverse range of hairstyles and generalizes well to challenging new hairstyles. Empirical results show that our method improves the state-of-the-art approaches in capturing and generating photorealistic, personalized avatars with complete hair.",https://openreview.net/forum?id=YMMnd2urj9
,,,,,,
Poster 1,Spotlight 1,253,ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation,"Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, Otmar Hilliges","We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping, relocation, and articulation. We show our method's efficacy towards this task. We further demonstrate that our method can generate motions with noisy hand-object pose estimates from an off-the-shelf image-based regressor.",https://openreview.net/forum?id=vrizS6bdW0
Poster 1,Spotlight 1,81,DehazeNeRF: Multi-image Haze Removal and 3D Shape Reconstruction using Neural Radiance Fields,"Wei-Ting Chen, Wang Yifan, Sy-Yen Kuo, Gordon Wetzstein","Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance for 3D computer vision tasks, including novel view synthesis and 3D shape reconstruction. However, these methods fail in a scattering medium, such as haze. To address this challenge, we introduce DehazeNeRF as a framework that robustly operates in hazy conditions. DehazeNeRF extends the volume rendering equation by adding physically realistic terms that model atmospheric scattering. By parameterizing these terms using suitable networks that match the physical properties, we introduce effective inductive biases, which, together with the proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view haze removal, novel view synthesis, and 3D shape reconstruction where existing approaches failed.",https://openreview.net/forum?id=Z2Ip2XGv0D
Poster 1,Spotlight 1,175,Efficient 3D Articulated Human Generation with Layered Surface Volumes,"Yinghao Xu, Wang Yifan, Alexander William Bergman, Menglei Chai, Bolei Zhou, Gordon Wetzstein","Access to high-quality and diverse 3D articulated digital human assets is crucial in various applications, ranging from virtual reality to social platforms. Generative approaches, such as 3D generative adversarial networks (GANs), are rapidly replacing laborious manual content creation tools. Existing 3D GAN frameworks typically rely on explicit representations that are fast to render but offer limited quality, or implicit representations, which offer high capacity but are slow to render, thereby limiting the 3D fidelity in GAN settings. In this work, we introduce layered surface volumes (LSVs) as a new 3D representation for articulatable humans. LSVs represent a human body using multiple textured mesh layers around a conventional mesh template. The explicit mesh layers can be interpreted as a discretized volume with finite thickness located around the template manifold, as such it has the capacity to capture off-template details like hair or accessories, while at the same time it can profit from mesh rasterization for fast rendering. LSVs can be articulated, and they exhibit exceptional efficiency in GAN settings. Trained on unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality and view-consistent 3D articulated digital humans without the need for view-inconsistent 2D upsampling networks.",https://openreview.net/forum?id=ulSI1Ikc7q
Poster 1,Spotlight 1,89,FastHuman: Reconstructing High-Quality Clothed Human in Minutes,"Lixiang Lin, Songyou Peng, Qijun Gan, Jianke Zhu","We propose an approach for optimizing high-quality clothed human body shapes in minutes, using multi-view posed images. While traditional neural rendering methods struggle to disentangle geometry and appearance using only rendering loss, and are computationally intensive, our method uses a mesh-based patch warping technique to ensure multi-view photometric consistency, and sphere harmonics (SH) illumination to refine geometric details efficiently. We employ oriented point clouds' shape representation and SH shading, which significantly reduces optimization and rendering times compared to implicit methods. Our approach has demonstrated promising results on both synthetic and real-world datasets, making it an effective solution for rapidly generating high-quality human body shapes.",https://openreview.net/forum?id=dCfj39ZwvR
Poster 1,Spotlight 1,265,Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains,"Zhenzhen Weng, Laura Bravo-Sánchez, Serena Yeung","Recent text-to-image generative models have exhibited remarkable abilities in generating high-fidelity and photo-realistic images. However, despite the visually impressive results, these models often struggle to preserve plausible human structure in the generations. Due to this reason, while generative models have shown promising results in aiding downstream image recognition tasks by generating large volumes of synthetic data, they remain infeasible for improving downstream human pose perception and understanding. In this work, we propose a Diffusion model with Human Pose Correction (Diffusion-HPC), a text-conditioned method that generates photo-realistic images with plausible posed humans by injecting prior knowledge about human body structure. Our generated images are accompanied by 3D meshes that serve as ground truths for improving Human Mesh Recovery tasks, where a shortage of 3D training data has long been an issue. Furthermore, we show that Diffusion-HPC effectively improves the realism of human generations under varying conditioning strategies.",https://openreview.net/forum?id=2kHcblZUx8
Poster 1,Spotlight 1,315,3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data,"Xiting Zhao, Sören Schwertfeger","Reflective surfaces present a persistent challenge for reliable 3D mapping and perception in robotics and autonomous systems. However, existing reflection datasets and benchmarks remain limited to sparse 2D data. This paper introduces the first large-scale 3D reflection detection dataset containing more than 50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic labels across diverse indoor environments with various reflections. Textured 3D meshes ground truth enable automatic point cloud labeling to provide precise ground truth annotations. Detailed benchmarks evaluate three Lidar point cloud segmentation methods, as well as current state-of-the-art image segmentation networks for glass and mirror detection. The proposed dataset advances reflection detection by providing a comprehensive testbed with precise global alignment, multi-modal data, and diverse reflective objects and materials. It will drive future research towards reliable reflection detection. The dataset will be publicly available at http://3dref.github.io.",https://openreview.net/forum?id=J25q5XSu9Y
Poster 2,Spotlight 2,325,PRAGO: Differentiable multi-view pose optimization from objectness detections,"Matteo Taiana, Matteo Toso, Stuart James, Alessio Del Bue","Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs.  To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks.We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pair-wise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges. PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging. We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21 for rotations while achieving similar translation estimates.",https://openreview.net/forum?id=V7AtGCdZMv
Poster 2,Spotlight 2,244,PathFusion:Path-Consistent Lidar-Camera Deep Feature Fusion,"Lemeng Wu, Dilin Wang, Meng Li, Yunyang Xiong, Raghuraman Krishnamoorthi, qiang liu, Vikas Chandra","Fusing 3D LiDAR features with 2D camera features is a promising technique for enhancing the accuracy of 3D detection, thanks to their complementary physical properties.While most of the existing methods focus on directly fusing camera features with raw LiDAR point clouds or shallow-level 3D features,it is observed that directly combining 2D and 3D features in deeper layers actually leads to a decrease in accuracy due to feature misalignment.The misalignment, which stems from the aggregation of features learned from large receptive fields, becomes increasingly more severe as we delve into deeper layers.In this paper, we propose PathFusion as a solution to enable the alignment of semantically coherent LiDAR-camera deep feature fusion. PathFusion introduces a path consistency loss at multiple stages within the network, encouraging the 2D backbone and its fusion path to transform 2D features in a way that aligns semantically with the transformation of the 3D backbone. This ensures semantic consistency between 2D and 3D features, even in deeper layers, and amplifies the usage of the network's learning capacity.We apply PathFusion to improve a prior-art fusion baseline, Focals Conv, and observe an improvement of over 1.2\ in mAP on the nuScenes test split consistently with and without testing-time data augmentations, and moreover, PathFusion also improves KITTI $\text{AP}_{\text{3D}}$ (R11) by about 0.6\ on the moderate level.",https://openreview.net/forum?id=QTllStKk2g
Poster 2,Spotlight 2,269,LumiGAN: Unconditional Generation of Relightable 3D Human Faces,"Boyang Deng, Wang Yifan, Gordon Wetzstein","Unsupervised learning of 3D human faces from unstructured 2D image data is an active research area. While recent works have achieved an impressive level of photorealism, they commonly lack control of lighting, which prevents the generated assets from being deployed in novel environments. To this end, we introduce \moniker{}, an unconditional Generative Adversarial Network (GAN) for 3D human faces with a physically based lighting module that enables relighting under novel illumination at inference time. Unlike prior work, LumiGAN can create realistic shadow effects using an efficient visibility formulation that is learned in a self-supervised manner. LumiGAN generates plausible physical properties for relightable faces, including surface normals, diffuse albedo, and specular tint without any ground truth data. In addition to relightability, we demonstrate significantly improved geometry generation compared to state-of-the-art non-relightable 3D GANs and notably better photorealism than existing relightable GANs.",https://openreview.net/forum?id=oCKQEroekk
Poster 2,Spotlight 2,67,3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera,"Christen Millerdurai, Diogo Luvizon, Viktor Rudnev, André Jonas, Jiayi Wang, Christian Theobalt, Vladislav Golyanik","3D hand tracking from a monocular video is a very challenging problem due to hand interactions, occlusions, left-right hand ambiguity, and fast motion. Most existing methods rely on RGB inputs, which have severe limitations under low-light conditions and suffer from motion blur. In contrast, event cameras capture local brightness changes instead of full image frames and do not suffer from the described effects, but, unfortunately, existing image-based techniques cannot be directly applied to events due to significant differences in the data modalities. In response to these challenges, this paper introduces the first framework for 3D tracking of two fast-moving and interacting hands from a single monocular event camera. Our approach tackles the left-right hand ambiguity with a novel semi-supervised feature-wise attention mechanism and integrates an intersection loss to fix hand collisions. To facilitate advances in this research domain, we release a new synthetic large-scale dataset of two interacting hands, Ev2Hands-S, and a new real benchmark with real event streams and ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing methods in terms of the 3D reconstruction accuracy and generalizes to real data under severe light conditions.",https://openreview.net/forum?id=nDdvloSVou
Poster 3,Spotlight 3,333,Ray-Patch: An Efficient Querying for Light Field Transformers,"Tomas Berriel Martins, Javier Civera","In this paper we propose the Ray-Patch querying, a novel model to efficiently query transformers to decode implicit representations into target views. Our Ray-Patch decoding reduces the computational footprint and increases inference speed up to one order of magnitude compared to previous models, without losing global attention, and hence maintaining specific task metrics. The key idea of our novel querying is to split the target image into a set of patches, then querying the transformer for each patch to extract a set of feature vectors, which are finally decoded into the target image using convolutional layers. Our experimental results, implementing Ray-Patch in 3 different architectures and evaluating it in 2 different tasks and datasets, demonstrate and quantify the effectiveness of our method, specifically a notable boost in rendering speed for the same task metrics.",https://openreview.net/forum?id=mGHqf88CXI
Poster 3,Spotlight 3,65,MELON: NeRF with Unposed Images in SO(3),"Axel Levy, Mark J. Matthews, Matan Sela, Gordon Wetzstein, Dmitry Lagun","Neural radiance fields enable novel-view synthesis and scene reconstruction with photorealistic quality from a few images, but require known and accurate camera poses. Conventional pose estimation algorithms fail on smooth or self-similar scenes, while methods performing inverse rendering from unposed views require a rough initialization of the camera orientations. The main difficulty of pose estimation lies in real-life objects being almost invariant under certain transformations, making the photometric distance between rendered views non-convex with respect to the camera parameters. Using an equivalence relation that matches the distribution of local minima in camera space, we reduce this space to its quotient set, in which pose estimation becomes a more convex problem. Using a neural-network to regularize pose estimation, we demonstrate that our method - MELON - can reconstruct a neural radiance field from unposed images with state-of-the-art accuracy while requiring ten times fewer views than adversarial approaches.",https://openreview.net/forum?id=WTuIZA5wmJ
Poster 3,Spotlight 3,4,LabelMaker: Automatic Semantic Label Generation from RGB-D Trajectories,"Silvan Weder, Hermann Blum, Francis Engelmann, Marc Pollefeys","Semantic annotations are indispensable to train or evaluate perception models, yet very costly to acquire.This work introduces a fully automated 2D/3D labeling framework that, without any human intervention, can generate labels for RGB-D scans at equal (or better) level of accuracy than comparable manually annotated datasets such as ScanNet.Our approach is based on an ensemble of state-of-the-art segmentation models and 3D lifting through neural rendering.We demonstrate the effectiveness of our LabelMaker pipeline by generating significantly better labels for the ScanNet datasets and automatically labelling the previously unlabeled ARKitScenes dataset.",https://openreview.net/forum?id=do2AszWcCk
Poster 3,Spotlight 3,249,MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching,"Miaojie Feng, JunDa Cheng, Hao Jia, Longliang Liu, Gangwei Xu, Xin Yang","Stereo matching is a fundamental task in scene comprehension. In recent years, the method based on iterative optimization has shown promise in stereo matching. However, the current iteration framework employs a single-peak lookup, which struggles to handle the multi-peak problem effectively. Additionally, the fixed search range used during the iteration process limits the final convergence effects. To address these issues, we present a novel iterative optimization architecture called MC-Stereo. This architecture mitigates the multi-peak distribution problem in matching through the multi-peak lookup strategy, and integrates the coarse-to-fine concept into the iterative framework via the cascade search range. Furthermore, given that feature representation learning is crucial for successful learn-based stereo matching, we introduce a pre-trained network to serve as the feature extractor, enhancing the front end of the stereo matching pipeline. Based on these improvements, MC-Stereo ranks first among all publicly available methods on the KITTI-2012 and KITTI-2015 benchmarks, and also achieves state-of-the-art performance on ETH3D. The code will be open sourced after the publication of this paper.",https://openreview.net/forum?id=WdcwA2qO9N
Poster 4,Spotlight 4,105,PACE: Human and Camera Motion Estimation from in-the-wild Videos,"Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, Umar Iqbal","We present a method to estimate human motion in a global scene from moving cameras. This is a highly challenging task due to the coupling of human and camera motions in the video. To address this problem, we propose a joint optimization framework that disentangles human and camera motions using both foreground human motion priors and background scene features. Unlike existing methods that use SLAM as initialization, we propose to tightly integrate SLAM and human motion priors in an optimization that is inspired by bundle adjustment. Specifically, we optimize human and camera motions to match both the observed human pose and scene features. This design combines the strengths of SLAM and motion priors, which leads to significant improvements in human and camera motion estimation. We additionally introduce a motion prior that is suitable for batch optimization, making our approach significantly more efficient than existing approaches. Finally, we propose a novel synthetic dataset that enables evaluating camera motion in addition to human motion from dynamic videos. Experiments on the synthetic and real-world RICH datasets demonstrate that our approach substantially outperforms prior art in recovering both human and camera motions.",https://openreview.net/forum?id=UXotWr7iU7
Poster 4,Spotlight 4,109,Handbook on Leveraging Lines for Two-View Relative Pose Estimation,"Petr Hruby, Shaohui Liu, Rémi Pautrat, Marc Pollefeys, Daniel Barath","We propose an approach for estimating the relative pose between calibrated image pairs by jointly exploiting points, lines, and their coincidences in a hybrid manner. We investigate all possible configurations where these data modalities can be used together and review the minimal solvers available in the literature. Our hybrid framework combines the advantages of all configurations, enabling robust and accurate estimation in challenging environments. In addition, we design a method for jointly estimating multiple vanishing point correspondences in two images, and a bundle adjustment that considers all relevant data modalities. Experiments on various indoor and outdoor datasets show that our approach outperforms point-based methods, improving AUC@10$^\circ$ by 1-7 points while running at comparable speeds.The source code of the solvers and hybrid framework will be made public.",https://openreview.net/forum?id=1EHfJLyuiv
Poster 4,Spotlight 4,26,SALUDA: Surface-based Automotive LiDAR Unsupervised Domain Adaptation,"Björn Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet, Nicolas Courty","Learning models on one labeled dataset that generalize well on another domain is a difficult task, as several shifts might happen between the data domains. This is notably the case for lidar data, for which models can exhibit large performance discrepancies due for instance to different lidar patterns or changes in acquisition conditions. This paper addresses the corresponding Unsupervised Domain Adaptation (UDA) task for semantic segmentation. To mitigate this problem, we introduce an unsupervised auxiliary task of learning an implicit underlying surface representation simultaneously on source and target data. As both domains share the same latent representation, the model is forced to accommodate discrepancies between the two sources of data. This novel strategy differs from classical minimization of statistical divergences or lidar-specific domain adaptation techniques. Our experiments demonstrate that our method achieves a better performance than the current state of the art, both in real-to-real and synthetic-to-real scenarios.",https://openreview.net/forum?id=ivrz6O3oPS
Poster 4,Spotlight 4,91,S4C: Self-Supervised Semantic Scene Completion with Neural Fields,"Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, Daniel Cremers","3D semantic scene understanding is a fundamental challenge in computer vision. It enables mobile agents to autonomously plan and navigate arbitrary environments. Semantic scene completion (SSC) formalizes this challenge as jointly estimating dense geometry and semantic information from sparse observations of a scene. Current methods for SSC are generally trained on 3D ground truth based on aggregated LiDAR scans. This process relies on special sensors and annotation by hand which are costly and do not scale well. To overcome this issue, our work presents the first self-supervised approach to SSC, called S4C, that does not rely on 3D ground truth data. Our proposed method can reconstruct a scene from a single image and only relies on videos and pseudo segmentation ground truth generated from off-the-shelf image segmentation network during training. Unlike existing methods, which use discrete voxel grids, we represent scenes as implicit semantic fields. This formulation allows querying any point within the camera frustum for occupancy and semantic class. Our architecture is trained through rendering-based self-supervised losses. Nonetheless, our method achieves performance close to fully supervised state-of-the-art methods. Additionally, our method demonstrates strong generalization capabilities and can synthesize accurate segmentation maps for far away viewpoints.",https://openreview.net/forum?id=BSnmzmGu4P
Poster 4,Spotlight 4,222,NCRF: Neural Contact Radiance Fields for Free-viewpoint Rendering of Hand-Object Interactions,"Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Ales Leonardis","Modelling hand-object interaction is a fundamental challenging task in 3D computer vision. Despite remarkable progress has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between hand and object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between hand and object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction with photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy.",https://openreview.net/forum?id=VuAvQQsOME
Poster 4,Spotlight 4,74,Sparse 3D Reconstruction via Object-Centric Ray Sampling,"Llukman Cerkezi, Paolo Favaro","We propose a novel method for 3D object reconstruction from a sparse set of views captured from a 360-degree calibrated camera rig. We represent the object surface through a hybrid model that uses both an MLP-based neural representation and a triangle mesh. A key contribution in our work is a novel object-centric sampling scheme of the neural representation, where rays are shared among all views. This efficiently concentrates and reduces the number of samples used to update the neural model at each iteration. This sampling scheme relies on the mesh representation to ensure also that samples are well-distributed along its normals.  The rendering is then performed efficiently by a differentiable renderer. We demonstrate that this sampling scheme results in a more effective training of the neural representation, does not require the additional supervision of segmentation masks, yields state of the art 3D reconstructions, and works with sparse views on the Google’s Scanned Objects, Tank and Temples and MVMC Car datasets.",https://openreview.net/forum?id=rTlE3hVHGp
Poster 6,Spotlight 6,42,Zero-BEV: Zero-shot Projection of any First-Person Modality to BEV Maps,"Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf","Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.",https://openreview.net/forum?id=9S8vVfxNRn
Poster 6,Spotlight 6,217,Select-Sliced Wasserstein Distance for Point Cloud Learning,"Bang Du, Kunyao Chen, Haochen Zhang, Truong Nguyen","Quantifying the discrepancy between point sets is a critical component for point cloud learning tasks. The mainstream point cloud learning tasks utilize Chamfer distance and Earth-Mover's distance. Chamfer distance is computationally efficient but may not fully capture differences between sets of points. Earth-Mover's distance, while precise, is computationally expensive and can be impractical to use with high-definition data. Several variants of Sliced Wasserstein distances (SW) are introduced to reduce the computation cost, but bring new problems to the situation: The vanilla SW treats sampled slices equally, resulting in redundant projections Distributional Sliced Wasserstein distance requires gradient-based optimization, offsetting its benefits. To overcome this limitation and leverage the advantages of Sliced Wasserstein distance over EMD, we propose a novel metric, Select-Sliced Wasserstein distance. This new distance analyzes drawn samples of slices and quantifies their informativeness for each point in a single shot, which eliminates unnecessary projections as well as costly optimizations, but perpetuates the performance. Extensive experiments on various point cloud learning tasks to demonstrate the efficiency and effectiveness of the proposed distance metric.",https://openreview.net/forum?id=1bYJBatudn
Poster 6,Spotlight 6,368,Out of the Room: Generalizing Event-based Dynamic Motion Segmentation for Complex Scenes,"Stamatios Georgoulis, weining ren, Alfredo Bochicchio, Daniel Eckert, YUANYOU LI, Abel Roman Gawel","Rapid and reliable identification of dynamic scene parts, also known as motion segmentation, is a key challenge for mobile sensors. Contemporary RGB camera-based methods rely on modelling camera and scene properties however, are often under-constrained and fall short in unknown categories. Event cameras have the potential to overcome these limitations, but corresponding methods have only been demonstrated in smaller-scale indoor environments with simplified dynamic objects. This work presents an event-based method for class-agnostic motion segmentation that can successfully be deployed across complex large-scale outdoor environments too. To this end, we introduce a novel divide-and-conquer pipeline that combines: (a) ego-motion compensated events, computed via a scene understanding module that predicts monocular depth and camera pose as auxiliary tasks, and (b) optical flow from a dedicated optical flow module. These intermediate representations are then fed into a segmentation module that predicts motion segmentation masks. A novel transformer-based temporal attention mechanism in the segmentation module builds correlations across adjacent 'frames' to get temporally consistent segmentation masks. Our method sets the new state-of-the-art on the classic EV-IMO benchmark (indoors), where we achieve improvements of 2.19 moving object IoU (2.22 mIoU) and 4.52 point IoU respectively, as well as on a newly-generated motion segmentation and tracking benchmark (outdoors) based on the DSEC event dataset, termed DSEC-MOTS, where we show improvement of 12.91 moving object IoU.",https://openreview.net/forum?id=bysritUPSG
Poster 6,Spotlight 6,376,Self-supervised Learning of Neural Implicit Feature Fields for Camera Pose Refinement,"Maxime Pietrantoni, Gabriela Csurka, Martin Humenberger, Torsten Sattler","Visual localization techniques rely upon some underlying scene representation to localize against. These representations can be explicit such as 3D SFM map or implicit, such as a neural network that learns to encode the scene. The former require sparse feature extractors and matchers to build the scene representation. The latter might lack geometric grounding not capturing the 3D structure of the scene well enough. This paper proposes to jointly learn the scene representation along with a 3D dense feature field and a 2D feature extractor whose outputs are embedded in the same metric space. Through a contrastive framework we align this volumetric field with the image-based extractor and regularize the latter with a ranking loss from learned surface information. We learn the underlying geometry of the scene with an implicit field through volumetric rendering and design our feature field to leverage intermediate geometric information encoded in the implicit field. The resulting features are discriminative and robust to viewpoint change while maintaining rich encoded information. Visual localization is then achieved by aligning the image based features and the rendered volumetric features. We show the effectiveness of our approach on real-world scenes, demonstrating that our approach outperforms prior and concurrent work on leveraging implicit scene representations for  localization.",https://openreview.net/forum?id=1yrGI0gcQx
Poster 6,Spotlight 6,121,Partial-View Object View Synthesis via Filtering Inversion,"Fan-Yun Sun, Jonathan Tremblay, Valts Blukis, Kevin Lin, Danfei Xu, Boris Ivanovic, Peter Karkus, Stan Birchfield, Dieter Fox, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Marco Pavone, Nick Haber","We propose Filtering Inversion (FINV), a learning framework and optimization process that predicts a renderable 3D object representation from one or few partial views. FINV addresses the challenge of synthesizing novel views of objects from partial observations, spanning cases where the object is not entirely in view, is partially occluded, or is only observed from similar views. To achieve this, FINV learns shape priors by training a 3D generative model. At inference, given one or more views of a novel real-world object, FINV first finds a set of latent codes for the object by inverting the generative model from multiple initial seeds. Maintaining the set of latent codes, FINV filters and resamples them after receiving each new observation, akin to particle filtering. The generator is then finetuned for each latent code on the available views in order to adapt to novel objects. We show that FINV successfully synthesizes novel views of real-world objects (e.g., chairs, tables, and cars), even if the generative prior is trained only on synthetic objects. The ability to address the sim-to-real problem allows FINV to be used for object categories without real-world datasets. FINV achieves state-of-the-art performance on multiple real-world datasets, recovers object shape and texture from partial and sparse views, is robust to occlusion, and is able to incrementally improve its representation with more observations.",https://openreview.net/forum?id=1EhpvW7GLU
Poster 6,Spotlight 6,103,Physically Plausible Full-Body Hand-Object Interaction Synthesis,"Jona Braun, Sammy Christen, Muhammed Kocabas, Emre Aksan, Otmar Hilliges","We propose a physics-based method for synthesizing hand-object interactions in a full-body setting. While recent advancements have addressed specific facets of human-object interactions, a comprehensive physics-based approach remains a challenge. Existing methods often focus on isolated segments of the interaction process and rely on data-driven techniques that may result in artifacts. In contrast, our proposed method embraces reinforcement learning (RL) and physics simulation to mitigate the limitations of data-driven approaches. Through a hierarchical framework, we first learn skill priors for both body and hand movements in a decoupled setting. The generic skill priors learn to decode a latent skill code into the motion of the underlying part. A high-level policy then controls hand-object interactions in these pretrained latent spaces, guided by task objectives of grasping and following 3D target trajectories. It is trained using a novel reward function that combines an adversarial style term with a task reward, encouraging natural motions while fulfilling the task incentives. Our method successfully accomplishes the complete interaction task, from approaching an object to grasp and subsequent manipulation. We compare our approach against kinematics-based baselines and show that our method leads to more physically plausible motions.",https://openreview.net/forum?id=FYxrOXfGCR
,,,,,,
Poster 1,,200,3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual Transformer Learning,"Iyyakutti Iyappan Ganapathi, Sajid Javed, Naoufel Werghi","Analysis of the 3D Texture is indispensable for various tasks, such as retrieval, segmentation, classification, and inspection of sculptures, knitted fabrics, and biological tissues.A 3D texture is a locally repeated surface variation independent of the surface's overall shape and can be determined using the local neighborhood and its characteristics.Existing techniques typically employ computer vision techniques that analyze a 3D mesh globally, derive features, and then utilize the obtained features for retrieval or classification. Several traditional and learning-based methods exist in the literature however, only a few are on 3D texture, and nothing yet, to the best of our knowledge, on the unsupervised schemes. This paper presents an original framework for the unsupervised segmentation of the 3D texture on the mesh manifold. We approach this problem as binary surface segmentation, partitioning the mesh surface into textured and non-textured regions without prior annotation. We devise a mutual transformer-based system comprising a label generator and a cleaner. The two models take geometric image representations of the surface mesh facets and label them as texture or non-texture across an iterative mutual learning scheme. Extensive experiments on three publicly available datasets with diverse texture patterns demonstrate that the proposed framework outperforms standard and SOTA unsupervised techniques and competes reasonably with supervised methods.",https://openreview.net/forum?id=IEa36N1ca1
Poster 1,,54,Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching,"Dongliang Cao, Paul Roetzer, Florian Bernard","We propose a novel unsupervised learning approach for non-rigid 3D shape matching. Our approach improves upon recent state-of-the art deep functional map methods and can be applied to a broad range of different challenging scenarios. Previous deep functional map methods mainly focus on feature extraction and aim exclusively at obtaining more expressive features for functional map computation. However, the importance of the functional map computation itself is often neglected and the relationship between the functional map and point-wise map is underexplored. In this paper, we systematically investigate the coupling relationship between the functional map from the functional map solver and the point-wise map based on feature similarity. To this end, we propose a self-adaptive functional map solver to adjust the functional map regularisation for different shape matching scenarios, together with a vertex-wise contrastive loss to obtain more discriminative features. Using different challenging datasets (including non-isometry, topological noise and partiality), we demonstrate that our method substantially outperforms previous state-of-the-art methods.",https://openreview.net/forum?id=ZxnPCQ0qW6
Poster 1,,64,SUCRe: Leveraging Scene Structure for Underwater Color Restoration,"Clémentin Boittiaux, Ricard Marxer, Claire Dune, Aurelien ARNAUBEC, Maxime Ferrera, Vincent HUGEL","Underwater images are altered by the physical characteristics of the medium through which light rays pass before reaching the optical sensor. Scattering and wavelength-dependent absorption significantly modify the captured colors depending on the distance of observed elements to the image plane. In this paper, we aim to recover an image of the scene as if the water had no effect on light propagation. We introduce SUCRe, a novel method that exploits the scene's 3D structure for underwater color restoration. By following points in multiple images and tracking their intensities at different distances to the sensor, we constrain the optimization of the parameters in an underwater image formation model and retrieve unattenuated pixel intensities. We conduct extensive quantitative and qualitative analyses of our approach in a variety of scenarios ranging from natural light to deep-sea environments using three underwater datasets acquired from real-world scenarios and one synthetic dataset. We also compare the performance of the proposed approach with that of a wide range of existing state-of-the-art methods. The results demonstrate a consistent benefit of exploiting multiple views across a spectrum of objective metrics. Our code will be made publicly available upon publication.",https://openreview.net/forum?id=5Sw0R7YAdA
Poster 1,,180,Quantum-Hybrid Stereo Matching with Nonlinear Regularization and Spatial Pyramids,"Cameron Braunstein, Eddy Ilg, Vladislav Golyanik","Quantum computing will gain significant importance in the future. We show how the state of the art on stereo matching targeted for this technology can be improved. Previous methods do not allow for nonlinear regularizers that constitute NP-hard problems. To this end, they do not exploit the fundamental advantages of the quantum computing paradigm in solving combinatorial optimization objectives. In contrast, our work presents a novel formulation for stereo matching with nonlinear regularizers and spatial pyramids on quantum annealers as a maximum a posteriori inference problem that minimizes the energy of a Markov Random Field. Our approach is hybrid (i.e., quantum-classical) and is compatible with modern D-Wave quantum annealers, i.e., it includes a quadratic unconstrained binary optimization (QUBO) objective. It reveals the full potential of applying quantum annealers in stereo matching and possibly related visual computing problems. On the Middlebury benchmark, our approach achieves an improved root mean squared accuracy between 2 and 22.5 when using different solvers.",https://openreview.net/forum?id=rd9XyZ4QC9
Poster 1,,323,MACS: Mass Conditioned 3D Hand and Object Motion Synthesis,"Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler","The physical properties of an object, such as its mass, significantly affect how we manipulate it with our hands. However, these aspects have so far been neglected in prior work on motion synthesis. This work proposes MACS–the first MAss Conditioned 3D hand and object motion Synthesis approach. Using cascaded diffusion models, we show how MACS generates interactions that plausibly change based on the object’s mass and interaction type. MACS also accepts a manually drawn 3D object trajectory as input and synthesizes the natural 3D hand motions conditioned by the object’s mass. This flexibility enables MACS to be used for various downstream applications, such as generating synthetic training data for ML tasks, fast animation of hands for graphics workflows, and generating character interactions for computer games. In our experiments, MACS reasonably generalizes across interpolated and extrapolated object masses unseen during the training with our limited dataset. Furthermore, MACS shows moderate generalization to unseen objects, thanks to the mass-conditioned contact labels generated by our surface contact synthesis model ConNet. Our comprehensive user study confirms that the synthesized 3D hand-object interactions are highly plausible and realistic.",https://openreview.net/forum?id=Garp39P9bi
Poster 1,,230,Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On,"Daiheng Gao, Xu Chen, Xindi Zhang, Qi WANG, Ke Sun, Bang Zhang, Liefeng Bo, Qixing Huang","Fabricating and designing 3D garments has become extremely demanding with theincreasing need for synthesizing realistic dressed persons for a variety of applications, \eg 3D virtual try-on, digitalization of 2D clothes into 3D apparel, and cloth animation. It thus necessitates a simple and straightforward pipeline to obtain high-quality texture from simple input, such as 2D reference images. Since traditional warping-based texture generation methods require a significant number of control points to be manually selected for each type of garment, which can be a time-consuming and tedious process. We propose a novel method, called \textbf{Cloth2Tex}, which eliminates the human burden in this process. Cloth2Tex is a self-supervised method that generates texture maps with reasonable layout and structural consistency. Another key feature of Cloth2Tex is that it can be used to support high-fidelity texture inpainting. This is done by combining Cloth2Tex with a prevailing latent diffusion model. We evaluate our approach both qualitatively and quantitatively and demonstrate that Cloth2Tex can generate high-quality texture maps and achieve the best visual effects in comparison to other methods. For more details and animated results, please watch our supplemental video.",https://openreview.net/forum?id=NaTYJEZxJr
Poster 1,,43,Classical Photometric Stereo in Point Lighting Environments: Error Analysis and Mitigation,"Simon Brenner, Robert Sablatnig","While classical photometric stereo assumes parallel lighting and thus allows for efficient solutions, this assumption is rarely met in practice. In acquisition setups using point-like light sources such as LEDs, direction and intensity of incident light depend on the unknown surface position, which leads to a non-linear problem. Although numerous approaches explicitly accounting for non-parallel lighting are proposed, photometric stereo with the parallel lighting model is widely used in point lighting scenarios. This is done for reasons of  computational efficiency and ease of system calibration, and justified with the observation that lighting becomes approximately parallel when light sources are placed in a sufficient distance from the object.However, as no account of the relation between light source distance and reconstruction errors is found in literature, it is not clear which light source distance would be 'sufficient' for a given application.  In this work, we propose an upper bound for mean errors resulting from using parallel-lighting models in point light setups, depending on the distance of light sources and the size of the object imaged. This bound is based on analytical considerations and previous results of photometric stereo error analysis, and validated vie a Monte Carlo simulation. These results are then used to justify an error mitigation strategy via local solutions that is applicable in use cases where an approximate depth is known a priori.The theoretical propositions are demonstrated on real-world data.",https://openreview.net/forum?id=9VCrlkxKz3
Poster 1,,145,Physics-based Indirect Illumination for Inverse Rendering,"Youming Deng, Xueting Li, Sifei Liu, Ming-Hsuan Yang","We present a physics-based inverse rendering method that learns the illumination, geometry, and materials of a scene from posed multi-view RGB images. To model the illumination of a scene, existing inverse rendering works either completely ignore the indirect illumination or model it by coarse approximations, leading to sub-optimal illumination, geometry, and material prediction of the scene. In this work, we propose a physics-based illumination model that first locates surface points through an efficient refined sphere tracing algorithm, then explicitly traces the incoming indirect lights at each surface point based on reflection. Then, we estimate each identified indirect light through an efficient neural network. Moreover, we utilize the Leibniz's integral rule to resolve non-differentiability in the proposed illumination model caused by boundary lights inspired by differentiable irradiance in computer graphics. As a result, the proposed differentiable illumination model can be learned end-to-end together with geometry and materials estimation. As a side product, our physics-based inverse rendering model also facilitates flexible and realistic material editing as well as relighting. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method performs favorably against existing inverse rendering methods on novel view synthesis and inverse rendering. The source code and trained models will be released to the public.",https://openreview.net/forum?id=WPR0seg46C
Poster 1,,162,Test-Time Augmentation for 3D Point Cloud Classification and Segmentation,"Tuan-Anh Vu, Srinjay Sarkar, Zhiyuan Zhang, Binh-Son Hua, Sai-Kit Yeung","Data augmentation is a powerful technique to enhance the performance of a deep learning task but has received less attention in 3D deep learning. It is well known that when 3D shapes are sparsely represented with low point density, the performance of the downstream tasks drops significantly. In this work, we explore test-time augmentation (TTA) for 3D point clouds. We are inspired by the recent revolution of learning implicit representation and point cloud upsampling, which can produce high-quality 3D surface reconstruction and proximity-to-surface, respectively. Our idea is to leverage the implicit field reconstruction or point cloud upsampling techniques as a systematic way to augment point cloud data. Particularly, we test both strategies by sampling points from the reconstructed results and using the sampled point cloud as test-time augmented data. We show that both strategies are effective in improving accuracy. We observed that point cloud upsampling for test-time augmentation can lead to more significant performance improvement on downstream tasks such as object classification and segmentation on the ModelNet40, ShapeNet, ScanObjectNN, and SemanticKITTI datasets, especially for sparse point clouds.",https://openreview.net/forum?id=i3C23LEi2f
Poster 1,,206,Lang3DSG: Language-based contrastive pre-training for 3D scene graph prediction,"Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski","3D scene graphs are an emerging 3D scene representation, that models both the objects present in the scene as well as their relationships. However, learning 3D scene graphs is a challenging task because it requires not only object labels but also relationship annotations, which are very scarce in datasets.While it is widely accepted that pre-training is an effective approach to improve model performance in low data regimes, in this paper, we find that existing pre-training methods are ill-suited for 3D scene graphs. To solve this issue, we present the first language-based pre-training approach for 3D scene graphs, whereby we exploit the strong relationship between scene graphs and language. To this end, we leverage the language encoder of CLIP, a popular vision-language model, to distill its knowledge into our graph-based network. We formulate a contrastive pre-training, which aligns text embeddings of relationships (subject-predicate-object triplets) and predicted 3D graph features. Our method achieves state-of-the-art results on the main semantic 3D scene graph benchmark by showing improved effectiveness over pre-training baselines and outperforming all the existing fully supervised scene graph prediction methods by a significant margin.Furthermore, since our scene graph features are language-aligned, it allows us to query the language space of the features in a zero-shot manner. In this paper, we show an example of utilizing this property of the features to predict the room type of a scene without further training.",https://openreview.net/forum?id=ijcP5spmjW
Poster 1,,56,InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering,"Antonio Canela, Pol Caselles, Ibrar Malik, Gil Triginer Garces, Eduard Ramon, Jaime Garcia, Jordi Sanchez-Riera, Francesc Moreno-Noguer","Recent advances in full-head reconstruction have been obtained by optimizing a neural field through differentiable surface or volume rendering to represent a single scene. While these techniques achieve an unprecedented accuracy, they take several minutes, or even hours, due to the expensive optimization process required. In this work, we introduce InstantAvatar, a method that recovers full-head avatars from few images (down to just one) in a few seconds on commodity hardware. In order to speed up the reconstruction process, we propose a system that combines, for the first time, a voxel-grid neural field representation with a surface renderer. Notably, a naive combination of these two techniques leads to unstable optimizations that do not converge to valid solutions. In order to overcome this limitation, we present a novel statistical model that learns a prior distribution over 3D head signed distance functions using a voxel-grid based architecture. The use of this prior model, in combination with other design choices (e.g. normals regularization), results into a system that achieves 3D head reconstructions with comparable accuracy as the state-of-the-art with a 100× speed-up.",https://openreview.net/forum?id=OBAaH5Rqb2
Poster 1,,310,Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration,"Jingfan Guo, Fabian Prada, Donglai Xiang, Javier Romero, Chenglei Wu, Hyun Soo Park, Takaaki Shiratori, Shunsuke Saito","Registering clothes from 4D scans with vertex-accurate correspondence is challenging, yet important for dynamic appearance modeling and physics parameter estimation from real-world data. However, previous methods either rely on texture information, which is not always reliable, or achieve only coarse-level alignment. In this work, we present a novel approach to enabling accurate surface registration of texture-less clothes with large deformation. Our key idea is to effectively leverage a shape prior learned from pre-captured clothing using diffusion models. We also propose a multi-stage guidance scheme based on learned functional maps, which stabilizes registration for large-scale deformation even when they vary significantly from training data. Using high-fidelity real captured clothes, our experiments show that the proposed approach based on diffusion models generalizes better than surface registration with VAE or PCA-based priors, outperforming both optimization-based and learning-based non-rigid registration methods for both interpolation and extrapolation tests.",https://openreview.net/forum?id=uETr3JrCD5
Poster 1,,87,Controllable Dynamic Appearance for Neural 3D Portraits,"ShahRukh Athar, Zhixin Shu, Zexiang Xu, Fujun Luan, Sai Bi, Kalyan Sunkavalli, Dimitris Samaras","Recent advances in Neural Radiance Fields (NeRFs) have made it possible to reconstruct and reanimate dynamic portrait scenes with control over head-pose, facial expressions and viewing direction. However, training such models assumes photometric consistency over the deformed region e.g. the face must be evenly lit as it deforms with changing head-pose and facial expression. Such photometric consistency across frames of a video is hard to maintain, even in studio environments, thus making the created reanimatable neural portraits prone to artefacts during reanimation. In this work, we propose CoDyNeRF, a system that enables the creation of fully controllable 3D portraits in real-world capture conditions. CoDyNeRF learns to approximate illumination dependent effects via a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and the facial expressions and head-pose deformations. The surface normals prediction is guided using 3DMM normals that act as a coarse prior for the normals of the human head, where direct prediction of normals is hard due to rigid and non-rigid deformations induced by head-pose and facial expression changes. Using only a smartphone-captured short video of a subject for training, we demonstrate the effectiveness of our method on free view synthesis of a portrait scene with explicit head pose and expression controls, and realistic lighting effects.",https://openreview.net/forum?id=47pk9EN8AU
Poster 1,,78,TextMesh: Generation of Realistic 3D Meshes From Text Prompts,"Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari","Being able to generate highly realistic 2D images from mere text prompts has recently made huge progress in terms of speed and quality, thanks to the advent of image diffusion models. Naturally, the question arises if this can be also achieved in the generation of 3D content from such text prompts. To this end, a new line of methods recently emerged trying to harness diffusion models, trained on 2D images, for supervision of 3D model generation using view dependant prompts. While achieving impressive results, these methods, however, have two major drawbacks. First, rather than commonly used 3D meshes, they instead generate neural radiance fields (NeRFs), making them impractical for real applications. Second, these approaches tend to produce very over-saturated models, giving the output a cartoonish looking effect. Therefore, in this work we propose a novel method for generation of highly realistic 3D meshes. To this end, we extend NeRF to employ an SDF backbone, leading to much improved 3D mesh extraction. In addition, we propose a novel way to finetune the mesh texture, removing the effect of high saturation and improving the details of the output 3D mesh.",https://openreview.net/forum?id=SciaO8QwJp
Poster 1,,353,Neural Field Regularization by Denoising for 3D Sparse-View X-ray Computed Tomography,"Romain Vo, Julie Escoda, Caroline Vienne, Etienne Decencière","In this paper we present a method which allows conditioning of Neural Fields using Regularization by Denoising (RED). As opposed to learning a joint convolutional neural network to condition the output of a neural field, the RED framework is memory-efficient and allows to entirely decouple the conditionning network optimization and the neural field optimization. We focus our work on applications for 3D sparse-view X-ray Computed Tomography (CT) and propose a flexible procedure which does not assume coordinate-friendly partitioning of the forward operator. Indeed our method is applicable to any CT geometry, in particular to Cone-Beam CT which is the most common setup in industrial inspection. We quantitatively evaluate our approach and show that our method is either better or on par with the state of the art in terms of reconstruction quality while being the most memory-efficient.",https://openreview.net/forum?id=2QM3W8TZpl
Poster 1,,202,DeepDR: Deep Structure-Aware RGB-D Inpainting for Diminished Reality,"Christina Schwarz-Gsaxner, Shohei Mori, Dieter Schmalstieg, Jan Egger, Gerhard Paar, Werner Bailer, Denis Kalkofen","Diminished reality (DR) refers to the removal of real objects from the environment by virtually replacing them with their background. Modern DR frameworks use inpainting to hallucinate unobserved regions. While recent deep learning-based inpainting is promising, the DR use case is complicated by the need to generate coherent structure and 3D geometry (i.e., depth), in particular for advanced applications, such as 3D scene editing. In this paper, we propose DeepDR, a first RGB-D inpainting framework fulfilling all requirements of DR: Plausible image and geometry inpainting with coherent structure, running at real-time frame rates, with minimal temporal artifacts. Our structure-aware generative network allows us to explicitly condition color and depth outputs on the scene semantics, overcoming the difficulty of reconstructing sharp and consistent boundaries in regions with complex backgrounds. Experimental results show that the proposed framework can outperform related work qualitatively and quantitatively.",https://openreview.net/forum?id=0LMRkLo0yI
Poster 1,,68,Interaction Replica: Tracking human–object interaction and scene changes from human motion,"Vladimir Guzov, Julian Chibane, Riccardo Marin, Yannan He, Yunus Nail Saracoglu, Torsten Sattler, Gerard Pons-Moll","Our world is not static and humans naturally cause changes in their environments through interactions, e.g., opening doors or moving furniture. Modeling changes caused by humans is essential for building digital twins, e.g., in the context of shared physical-virtual spaces (metaverses) and robotics. In order for widespread adoption of such emerging applications, the sensor setup used to capture the interactions needs to be inexpensive and easy-to-use for non-expert users. I.e., interactions should be captured and modeled by simple ego-centric sensors such as a combination of cameras and IMU sensors. Yet, to the best of our knowledge, no work tackling the challenging problem of modeling human-object interactions via such an ego-centric sensor setup exists. This paper closes this gap in the literature by developing a novel approach that combines visual localization of humans in the scene with contact-based reasoning about human-object interactions from IMU data. Interestingly, we are able to show that even without visual observations of the interactions, human-object contacts and interactions can be realistically predicted from human motion. Our method, iReplica (Interaction Replica), is an essential first step towards the egocentric capture of human interactions and modeling of dynamic scenes, which is required for future AR/VR applications in immersive virtual universes and for training machines to behave like humans. To encourage the community to work on this challenging and important problem, we will make our new datasets and our code available.",https://openreview.net/forum?id=woYMlqqJ8h
Poster 1,,259,CombiNeRF: a Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis,"Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto","Neural Radiance Fields (NeRF) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, most of the recent NeRF regularization techniques aim each one to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distribution and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipshitz regularization on both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and Nerf-Synthetic datasets that supports the choices made. We release with this paper the open-source implementation of our framework.",https://openreview.net/forum?id=yfQzRlz4hG
Poster 2,,277,NeVRF: Neural Video-based Radiance Fields for Long-duration Sequences,"Minye Wu, Tinne Tuytelaars","Neural Radiance Fields (NeRF) have made remarkable strides in novel view synthesis. However, adopting NeRF to long-duration dynamic sequences has been challenging.Existing methods struggle to balance between quality and storage size and encounter difficulties with complex scene changes such as topological changes and large motions.  To tackle these issues, we propose a novel neural video-based radiance fields (NeVRF) representation.  NeVRF marries neural radiance field with image-based rendering and continual learning techniques to support photo-realistic novel view synthesis on long-duration dynamic scenes.  We introduce a novel multi-view radiance blending approach to predict radiance directly from multi-view videos. By incorporating continual learning techniques, NeVRF can efficiently reconstruct frames from sequential data without revisiting previous frames, enabling long-duration free-viewpoint video.  Furthermore, with a tailored compression approach, NeVRF can compactly represent dynamic scenes, making dynamic radiance fields more practical in real-world scenarios.  Our extensive experiments demonstrate the effectiveness of NeVRF in enabling long-duration sequence rendering, sequential data reconstruction, and compact data storage.",https://openreview.net/forum?id=uYrb7kpm8L
Poster 2,,77,Fast High Dynamic Range Radiance Fields for Dynamic Scenes,"guanjun Wu, Taoran Yi, Jiemin Fang, Xinggang Wang, Wenyu Liu","Neural Radiances Fields (NeRF) and its extensions have shown great success to represent 3D scenes and synthesis novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target at static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named as HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically-increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code will be released.",https://openreview.net/forum?id=gpUQbYUQJO
Poster 2,,148,Improved Scene Landmark Detection for Camera Localization,"Tien Do, Sudipta N. Sinha","Camera localization methods based on retrieval, local feature matching, and 3D structure-based pose estimation are accurate but require high storage, are slow, and are not privacy-preserving. A method based on scene landmark detection (SLD) was recently proposed to address these limitations. It involves training a convolutional neural network (CNN) to detect a few predetermined, salient, scene-specific 3D points or landmarks and computing camera pose from the associated 2D--3D correspondences. Although SLD outperformed existing learning-based approaches, it was notably less accurate than 3D structure-based methods. In this paper, we show that the accuracy gap was due to insufficient model capacity and noisy labels during training. To mitigate the capacity issue, we propose to split the landmarks into subgroups and train a separate network for each subgroup. To generate better training labels, we propose using dense reconstructions to estimate visibility of scene landmarks. Finally, we present a compact architecture to improve memory efficiency. Accuracy wise, our approach is on par with state of the art structure-based methods on the Indoor-6 dataset but runs significantly faster and uses less storage. We will release code and the trained models.",https://openreview.net/forum?id=hlm9mkxYMV
Poster 2,,143,"Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search","Chanhyeok Park, Minhyuk Sung","Achieving tight bounding boxes of a shape while guaranteeing complete boundness is an essential task for efficient geometric operations and unsupervised semantic part detection. But previous methods fail to achieve both full coverage and tightness. Neural-network-based methods are not suitable for these goals due to the non-differentiability of the objective, while classic iterative search methods suffer from their sensitivity to the initialization. We propose a novel framework for finding a set of tight bounding boxes of a 3D shape via over-segmentation and iterative merging and refinement. Our result shows that utilizing effective search methods with appropriate objectives is the key to producing bounding boxes with both properties. We employ an existing pre-segmentation to split the shape and obtain over-segmentation. Then, we apply hierarchical merging with our novel tightness-aware merging and stopping criteria. To overcome the sensitivity to the initialization, we also refine the bounding box parameters in an MDP setup with a soft reward function promoting a wider exploration. Lastly, we further improve the refinement step with MCTS-based multi-action space exploration. By thoughtful evaluation on diverse 3D shapes, we demonstrate full coverage, tightness, and an adequate number of bounding boxes of our method without requiring any training data or supervision. It thus can be applied to various downstream tasks in computer vision and graphics.",https://openreview.net/forum?id=VMC2VBIlBA
Poster 2,,12,Mirror-Aware Neural Humans,"Daniel Ajisafe, James Tang, Shih-Yang Su, Bastian Wandt, Helge Rhodin","Human motion capture either requires multi-camera systems or is unreliable using single-view input due to depth ambiguities. Meanwhile, mirrors are readily available in urban environments and form an affordable alternative by recording two views with only a single camera. However, the mirror setting poses the additional challenge of handling occlusions of real and mirror image. Going beyond existing mirror approaches for 3D human pose estimation, we utilize mirrors for learning a complete body model, including shape and dense appearance. Our main contributions are extending articulated neural radiance fields to include a notion of a mirror, making it sample-efficient over potential occlusion regions. Together, our contributions realize a consumer-level 3D motion capture system that starts from off-the-shelf 2D poses by automatically calibrating the camera, estimating mirror orientation, and subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to condition the mirror-aware NeRF. We empirically demonstrate the benefit of learning a body model and accounting for occlusion in challenging mirror scenes.",https://openreview.net/forum?id=3pAF5JuicF
Poster 2,,218,Stable Surface Regularization for Fast Few-Shot NeRF,"ByeongIn Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In-So Kweon, Kuk-Jin Yoon","This paper proposes an algorithm for synthesizing novel views under few-shot setup. The main concept is to develop a stable surface regularization technique called Annealing Signed Distance Function (ASDF), which anneals the surface in a coarse-to-fine manner to accelerate convergence speed. We observe that the Eikonal loss -- which is a widely known geometric regularization -- requires dense training signal to shape different level-sets of SDF, leading to low-fidelity results under few-shot training. In contrast, the proposed surface regularization successfully reconstructs scenes and produce high-fidelity geometry with stable training. Our method is further accelerated by utilizing grid representation and monocular geometric priors. Finally, the proposed approach is up to 45 times faster than existing few-shot novel view synthesis methods, and it produces comparable results in the ScanNet dataset and NeRF-Real dataset. The code will be made open.",https://openreview.net/forum?id=Al2oUWcUyJ
Poster 2,,396,– Visual Tomography – Physically Faithful Volumetric Models of Partially Translucent Objects,"David Nakath, Xiangyu Weng, Mengkun She, Kevin Köser","Digital 3D representations of objects and scenes are required for visualization and simulation. When created faithfully from real-world data they can be useful for human or computer-assisted analysis e.g. in natural sciences. Such models can also serve for generating (extra) training data for machine learning approaches in settings where data is expensive or difficult to obtain or where too few training data exists, e.g. to generate novel views, or images in different conditions such as in different media or illuminataion, for training detection and classification tasks. While the vast amount of visual 3D reconstruction approaches focusses on textured object surfaces or shapes, in this contribution we propose a volumetric reconstruction approach that obtains a physical model including the interior of partially translucent objects such as plankton or insects. Our technique photographs the object under different poses in front of a bright white light source and computes absorption and scattering per voxel. It can be interpreted as visual tomography that we solve by inverse raytracing. We suggest a method to convert NeRF representations into a volumetric grid for initialization and illustrate the usefulness of the approach and its steps using two real-world plankton validation sets, the lab-scanned models being finally also relighted and virtually submerged in an underwater scenarios with augmented medium and illumination conditions.",https://openreview.net/forum?id=GNVETIyUnH
Poster 2,,171,Unsupervised Representation Learning for Diverse Deformable Shape Collections,"Sara Hahner, Souhaib Attaiki, Jochen Garcke, Maks Ovsjanikov","We introduce a novel learning-based method for encoding and manipulating 3D surface meshes. Our method is specifically designed to create an interpretable embedding space for deformable shape collections. Unlike previous 3D mesh autoencoders that require meshes to be in a 1-to-1 correspondence, our approach is trained on diverse meshes in an unsupervised manner. Central to our method is a spectral pooling technique that establishes a universal latent space, breaking free from traditional constraints of mesh connectivity and shape categories. The entire process consists of two stages. In the first stage, we employ the functional map paradigm to extract point-to-point (p2p) maps between a collection of shapes in an unsupervised manner. These p2p maps are then utilized to construct a common latent space, which ensures straightforward interpretation and independence from mesh connectivity and shape category. Through extensive experiments, we demonstrate that our method achieves excellent reconstructions and produces more realistic and smoother interpolations compared to baseline approaches. Our code and data will be made available upon acceptance.",https://openreview.net/forum?id=cWROnbzuHo
Poster 2,,22,Developability Approximation for Neural Implicits through Rank Minimization,Pratheba Selvaraju,"Developability refers to the process of creating a surface without any tearing or shearing from a two-dimensional plane, and it finds practical applications in the fabrication industry. An essential characteristic of a developable 3D surface is its zero Gaussian curvature, which means that either one or both of the principal curvatures are zero. This paper introduces a method for reconstructing an approximate developable surface from a neural implicit surface. The central idea of our method involves incorporating a regularization term that operates on the second-order derivatives of the neural implicits, effectively promoting zero Gaussian curvature. We draw inspiration from the properties of surface curvature and employ rank minimization techniques derived from compressed sensing. Experimental results on both developable and non-developable surfaces, including those affected by noise, validate the generalizability of our method. Implicit surfaces offer the advantage of flexible resolution, surpassing state-of-the-art methods that are constrained by high polygonal surfaces",https://openreview.net/forum?id=nTc8wsSsmq
Poster 2,,93,Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization,"Luca Bartolomei, Matteo Poggi, Andrea Conti, Fabio Tosi, Stefano Mattoccia","This paper proposes a new framework for depth completion robust against domain-shifting issues. It exploits the generalization capability of modern stereo networks to face depth completion, by processing fictitious stereo pairs obtained through a virtual pattern projection paradigm.Any stereo network or traditional stereo matcher can be seamlessly plugged into our framework, allowing for the deployment of a virtual stereo setup that is future-proof against advancement in the stereo field. Exhaustive experiments on cross-domain generalization support our claims. Hence, we argue that our framework can help depth completion to reach new deployment scenarios.",https://openreview.net/forum?id=KZWKcXTfn9
Poster 2,,341,Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans,"Taiki Miyanishi, Daichi Azuma, Shuhei Kurita, Motoaki Kawanabe","We present Cross3DVG, a novel task for cross-dataset visual grounding in 3D scenes, revealing the limitations of existing 3D visual grounding models using restricted 3D resources and thus easily overfit to a specific 3D dataset.To facilitate Cross3DVG, we have created RIORefer, a large-scale 3D visual grounding dataset containing more than 63k diverse descriptions of 3D objects within 1,380 indoor RGB-D scans from 3RScan with human annotations.paired with the existing 52k descriptions on ScanRefer~\cite{chen2020scanrefer}.We perform Cross3DVG by training a model on the source 3D visual grounding dataset and then evaluating it on the target dataset constructed differently (e.g., different sensors, 3D reconstruction methods, and language annotators) without using target labels.We conduct comprehensive experiments using established visual grounding models, as well as a CLIP-based 2D-3D integration method designed to bridge the gaps between 3D datasets.By performing Cross3DVG tasks, we found that (i) cross-dataset 3D visual grounding has significantly lower performance than learning and evaluating a single dataset due to the 3D data and language variants across datasets and (ii) better object detector and localization modules and fusing 3D data and multi-view CLIP-based image features can alleviate this problem.Our Cross3DVG task will provide a benchmark for developing robust 3D visual grounding models capable of handling diverse 3D scenes while leveraging deep language understanding.",https://openreview.net/forum?id=fzS4jyEX1L
Poster 2,,61,PhoMoH: Implicit Photorealistic 3D Models of Human Heads,"Mihai Zanfir, Thiemo Alldieck, Cristian Sminchisescu","We present PhoMoH, a neural network methodology to construct generative models of photo-realistic 3D geometry and appearance of human heads including hair, beards, an oral cavity, and clothing.   In contrast to prior work, PhoMoH models the human head using neural fields, thus supporting complex topology.   Instead of learning a head model from scratch, we propose to augment an existing expressive head model with new features.   Concretely, we learn a highly detailed geometry network layered on top of a mid-resolution head model together with a detailed, local geometry-aware, and disentangled color field.   Our proposed architecture allows us to learn photo-realistic human head models from relatively little data.   The learned generative geometry and appearance networks can be sampled individually and enable the creation of diverse and realistic human heads.   Extensive experiments validate our method qualitatively and across different metrics.",https://openreview.net/forum?id=yqXgkUZPBH
Poster 2,,281,DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown Lighting,"Kohei Yamashita, Shohei Nobuhara, Ko Nishino","Geometry reconstruction of textureless, non-Lambertian objects under unknown natural illumination (i.e., in the wild) remains challenging as correspondences cannot be established and the reflectance cannot be expressed in simple analytical forms. We derive a novel multi-view method, DeepShaRM, that achieves state-of-the-art accuracy on this challenging task. Unlike past methods that formulate this as inverse-rendering, i.e., estimation of reflectance, illumination, and geometry from images, our key idea is to realize that reflectance and illumination need not be disentangled and instead estimated as a compound reflectance map. We introduce a novel deep reflectance map estimation network that recovers the camera-view reflectance maps from the surface normals of the current geometry estimate and the input multi-view images. The network also explicitly estimates per-pixel confidence scores to handle global light transport effects. A deep shape-from-shading network then updates the geometry estimate expressed with a signed distance function using the recovered reflectance maps. By alternating between these two, and, most important, by bypassing the ill-posed problem of reflectance and illumination decomposition, the method accurately recovers object geometry in these challenging settings. Extensive experiments on both synthetic and real-world data clearly demonstrate its state-of-the-art accuracy.",https://openreview.net/forum?id=Ofii8vYFUH
Poster 2,,203,Depth Reconstruction with Neural Signed Distance Fields in Structured Light Systems,"Rukun Qiao, Hiroshi Kawasaki, Hongbin Zha","We introduce a novel depth estimation technique for multi-frame structured light setups using neural implicit representations of 3D space. Our approach employs a neural signed distance field (SDF), trained through self-supervised differentiable rendering. Unlike passive vision, where joint estimation of radiance and geometry fields is necessary, we capitalize on known radiance fields from projected patterns in structured light systems. This enables isolated optimization of the geometry field, ensuring convergence and network efficacy with fixed device positioning. To enhance geometric fidelity, we incorporate an additional color loss based on object surfaces during training. Real-world experiments demonstrate our method's superiority in geometric performance for few-shot scenarios, while achieving comparable results with increased pattern availability.",https://openreview.net/forum?id=0cA20oiVzL
Poster 2,,247,GHuNeRF:Generalizable Human NeRF from a Monocular Video,"Chen Li, Jiahao Lin, Gim Hee Lee","In this paper, we tackle the challenging task of learning a generalizable human NeRF model from a monocular video. Although existing generalizable human NeRFs have achieved impressive results, they require muti-view images or videos which might not be always available. On the other hand, some works on free-viewpoint rendering of human from monocular videos cannot be generalized to unseen identities. In view of these limitations, we propose MonoHumanto learn a generalizable human NeRF model from a monocular video of the human performer. We first introduce a visibility-aware aggregationscheme to compute vertex-wise features, which is used to construct a 3D feature volume.  The feature volume can only represent the overall geometry of the human performer with insufficient accuracy due to the limited resolution. To solve this,  we further enhance the volume feature with temporally aligned point-wise features using an attention mechanism. Finally, the enhanced feature is used for predicting density and color for each sampled point. A surface-guided sampling strategy is also adopted to improve the efficiency for both training and inference. We validate our approach on the widely-used ZJU-MoCap dataset, where we achieve comparable performance with existing multi-view video based approaches. We also test on the monocular People-Snapshot dataset and achieve better performance than existing works when only monocular video is used. Our code will be made publicly available upon the acceptance of this paper.",https://openreview.net/forum?id=eCjmS4QKbf
Poster 2,,48,TADA! Text to Animatable Digital Avatars,"Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, Michael J. Black","We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent align-007ment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.",https://openreview.net/forum?id=1pY6b73JNm
Poster 2,,288,Event-based Visual Odometry on Non-holonomic Ground Vehicles,"Wanting Xu, Si'ao Zhang, Li Cui, Xin Peng, Laurent Kneip","Despite the promise of superior performance under challenging conditions, event-based motion estimation remains a hard problem owing to the difficulty of extracting and tracking stable features from event streams. In order to robustify the estimation, it is generally believed that fusion with other sensors is a requirement. In this work, we demonstrate reliable, purely event-based visual odometry on planar ground vehicles by employing the constrained non-holonomic motion model of Ackermann steering platforms. We extend single feature n-linearities for regular frame-based cameras to the case of quasi time-continuous event-tracks, and achieve a polynomial form via variable degree Taylor expansions. Robust averaging over multiple event tracks is simply achieved via histogram voting. As demonstrated on both simulated and real data, our algorithm achieves accurate and robust estimates of the vehicle's instantaneous rotational velocity, and thus results that are comparable to the delta rotations obtained by frame-based sensors under normal conditions. We furthermore significantly outperform the more traditional alternatives in challenging illumination scenarios.",https://openreview.net/forum?id=lZ5Ne1EQ9D
Poster 2,,137,Incorporating Rotation Invariance with Non-invariant Networks for Point Clouds,"Jiajun Fei, Zhidong Deng","Rotation invariance is a fundamental requirement of point cloud processing when input point clouds are not aligned. Many non-invariant networks performing well on aligned point clouds do not perform equivalent to rotated ones. Thus non-invariant and invariant networks are developed separately and only benefit a little from each other, leading to repetitive and wasteful research efforts. In this paper, we aim to bridge this gap and incorporate rotation invariance with non-invariant networks for point clouds. To this end, we propose a novel rotation invariant learning method based on efficient invariant poses (EIPs). EIPs do not rely on novel features or operations. Instead, they only rotate input point clouds into invariant poses and apply non-invariant networks in feature processing. As the name implies, EIPs have negligible complexities (efficient) and solid theoretical foundations (invariant). Experimental results demonstrate that EIPs have competitive performances on several tasks. Without using new features or operations, EIPs yield the best results on ScanObjectNN (PB\_T50\_RS) classification and ShapeNetPart segmentation task.",https://openreview.net/forum?id=SeBMnTEfk8
Poster 2,,298,UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization,"Rouwan Wu, Xiaoya Cheng, Juelin Zhu, Yuxiang Liu, Maojun Zhang, Shen Yan","Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets. Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data. To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization. Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space. Experimental results on the new dataset demonstrate the effectiveness of the proposed approach. The code and dataset will be publicly released upon the publish of the paper.",https://openreview.net/forum?id=PQfo0DipCF
Poster 3,,160,MuVieCAST: Multi-View Consistent Artistic Style Transfer,"Nail Ibrahimli, Julian F. P. Kooij, Liangliang Nan","We introduce MuVieCAST, a modular multi-view consistent style transfer network architecture that enables consistent style transfer between multiple viewpoints of the same scene. This network architecture supports both sparse and dense views, making it versatile enough to handle a wide range of multi-view image datasets. The approach consists of three modules that perform specific tasks related to style transfer, namely content preservation, image transformation, and multi-view consistency enforcement. We extensively evaluate our approach across multiple application domains including depth-map-based point cloud fusion, mesh reconstruction, and novel-view synthesis. Our experiments reveal that the proposed framework achieves an exceptional generation of stylized images, exhibiting consistent outcomes across perspectives. A user study focusing on novel-view synthesis further confirms these results, with approximately 68\ of cases participants expressing a  preference for our generated outputs compared to the recent state-of-the-art method. Our modular framework is extensible and can easily be integrated with various backbone architectures, making it a flexible solution for multi-view style transfer.More results are demonstrated on our project page:  [muviecast.github.io](https://muviecast.github.io/)",https://openreview.net/forum?id=tr9QoIRVTh
Poster 3,,221,HOC-Search: Efficient CAD Model and Pose Retrieval from RGB-D Scans,"Stefan Ainetter, Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit","We present an automated and efficient approach for retrieving high-quality CAD models of objects and their poses in a scene captured by a moving RGB-D camera. We first investigate various objective functions to measure similarity between a candidate CAD object model and the available data, and the best objective function appears to be a ""render-and-compare"" method comparing depth and mask rendering. We thus introduce a fast-search method that approximates an exhaustive search based on this objective function for simultaneously retrieving the object category, a CAD model, and the pose of an object given an approximate 3D bounding box. This method involves a search tree that organizes the CAD models and object properties including object category and pose for fast retrieval and an algorithm inspired by Monte Carlo Tree Search, that efficiently searches this tree. We show that this method retrieves CAD models that fit the real objects very well, with a speed-up factor of 10x to 120x compared to  exhaustive search.",https://openreview.net/forum?id=ViFuwg5uxa
Poster 3,,184,Fast Relative Pose Estimation using Relative Depth,"Jonathan Astermark, Yaqing Ding, Viktor Larsson, Anders Heyden","In this paper we revisit the problem of estimating the relative pose from a sparse set of point-correspondences.For each point-correspondence we also assume we know the relative depth, i.e. the relative distance to the scene point.This yields an additional constraint allowing us to use fewer matches in RANSAC to generate the pose candidates.In the paper we propose two novel minimal solvers for this setting.We also consider the case of known vertical direction.To obtain the relative depth estimates we explore using scale estimates obtained from a keypoint detector as well as a neural network that predicts the relative depth for pairs of patches.We show in experiments, that while our estimates are more noisy compared to the purely point-based solvers, the smaller sample size leads to a significantly reduced runtime in settings with high outlier ratios.",https://openreview.net/forum?id=52nVZuxrbF
Poster 3,,177,Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models,"Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, Heng Wang","Zero-shot novel view synthesis (NVS) from a single image is an essential problem in 3D object understanding. While recent approaches that leverage pre-trained generative models can synthesize high-quality novel views from in-the-wild inputs, they still struggle to maintain 3D consistency across different views. In this paper, we present Consistent-1-to-3, which is a generative framework that significantly mitigate this issue.Specifically, we decompose the NVS task into two stages: (i) transforming observed regions to a novel view, and (ii) hallucinating unseen regions. We design a scene representation transformer and view-conditioned diffusion model for performing these two stages respectively. Inside the models, to enforce 3D consistency, we propose to employ epipolor-guided attention to incorporate geometry constraints, and multi-view attention to better aggregate multi-view information. Finally, we design a hierarchy generation paradigm to generate long sequences of consistent views, allowing a full 360 observation of the provided object image.Qualitative and quantitative evaluation over multiple datasets demonstrate the effectiveness of the proposed mechanisms against state-of-the-art approaches.",https://openreview.net/forum?id=FmRQVDWgcP
Poster 3,,303,Unsupervised 3D Keypoint Discovery with Multi-View Geometry,"Sina Honari, Chen Zhao, Mathieu Salzmann, Pascal Fua","Analyzing and training 3D body posture models depends heavily on the availability of joint labels that are commonly acquired through laborious manual annotation of body joints or via marker-based joint localization using carefully curated markers and capturing systems. However, such annotations are not always available, especially for people performing unusual activities. In this paper, we propose an algorithm that learns to discover 3D keypoints on human bodies from multiple-view images without any supervision other than the constraints multiple-view geometry provides. To ensure that the discovered 3D keypoints are meaningful, they are re-projected to each view to estimate the person's mask that the model itself has initially estimated. Our approach discovers more interpretable and accurate 3D keypoints compared to other state-of-the-art unsupervised approaches on Human3.6M and MPI-INF-3DHP benchmark datasets.",https://openreview.net/forum?id=GwU3tCpg8d
Poster 3,,260,TECA: Text-Guided Generation and Editing of Compositional 3D Avatars,"HAO ZHANG, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black","Our goal is to create a realistic 3D facial avatar with hair and accessories using only a text description. While this challenge has attracted significant recent interest, existing methods either lack realism, produce unrealistic shapes, or do not support editing, such as modifications to the hairstyle. We argue that existing methods are limited because they employ a monolithic modeling approach, using a single representation for the head, face, hair, and accessories. Our observation is that the hair and face, for example, have very different structural qualities that benefit from different representations. Building on this insight, we generate avatars with a compositional model, in which the head, face, and upper body are represented with traditional 3D meshes, and the hair, clothing, and accessories with neural radiance fields (NeRF). The model-based mesh representation provides a strong geometric prior for the face region, improving realism while enabling editing of the person’s appearance. By using NeRFs to represent the remaining components, our method is able to model and synthesize parts with complex geometry and appearance, such as curly hair and fluffy scarves. Our novel system synthesizes these high-quality compositional avatars from text descriptions. Specifically, we generate a face image using text, fit a parametric shape model to it, and inpaint texture using diffusion models. Conditioned on the generated face, we sequentially generate style components such as hair or clothing using Score Distillation Sampling (SDS) with guidance from CLIPSeg segmentations. However, this alone is not sufficient to produce avatars with a high degree of realism. Consequently, we introduce a hierarchical approach to refine the non-face regions using a BLIP-based loss combined with SDS. The experimental results demonstrate that our method, Text-guided generation and Editing of Compositional Avatars (TECA), produces avatars that are more realistic than those of recent methods while being editable because of their compositional nature. For example, our TECA enables the seamless transfer of compositional features like hairstyles, scarves, and other accessories between avatars. This capability supports applications such as virtual try-on. We will release the code for research purposes.",https://openreview.net/forum?id=2xMloZcHn7
Poster 3,,170,Compositional 3D Scene Generation using Locally Conditioned Diffusion,"Ryan Po, Gordon Wetzstein","Designing complex 3D scenes has been a tedious, manual process requiring domain expertise. Emerging text-to-3D generative models show great promise for making this task more intuitive, but existing approaches are limited to object-level generation. We introduce locally conditioned diffusion as an approach to compositional scene diffusion, providing control over semantic parts using text prompts and bounding boxes while ensuring seamless transitions between these parts. We demonstrate a score distillation sampling--based text-to-3D synthesis pipeline that enables compositional 3D scene generation at a higher fidelity than relevant baselines.",https://openreview.net/forum?id=HamFWyuiXb
Poster 3,,36,Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects,"Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang","Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.",https://openreview.net/forum?id=X2zWP6qi01
Poster 3,,122,Oriented-grid Encoder for 3D Implicit Representations,"Arihant Gaur, Gonçalo Dias Pais, Pedro Miraldo","Encoding 3D points is one of the primary steps in learning-based implicit scene representation. Using features that gather information from neighbors with multi-resolution grids has proven to be the best geometric encoder for this task. However, prior techniques do not exploit some characteristics of most objects or scenes, such as surface normals and local smoothness. This paper is the first to exploit those 3D characteristics in 3D geometric encoders explicitly. In contrast to prior work on using multiple levels of details, regular cube grids, and trilinear interpolation, we propose 3D-oriented grids with a novel cylindrical volumetric interpolation for modeling local planar invariance. In addition, we explicitly include a local feature aggregation for feature regularization and smoothing of the cylindrical interpolation features. We evaluate our approach on ABC, Thingi10k, ShapeNet, and Matterport3D, for object and scene representation. Compared to the use of regular grids, our geometric encoder is shown to converge in fewer steps and obtain sharper 3D surfaces. When compared to the prior techniques, our method gets state-of-the-art.",https://openreview.net/forum?id=Lxpty9BJ5Y
Poster 3,,135,Robust Point Cloud Processing through Positional Embedding,"Jianqiao Zheng, Xueqian Li, Sameera Ramasinghe, Simon Lucey","End-to-end trained per-point embeddings are an essential ingredient of any state-of-the-art 3D point cloud processing such as detection or alignment. Methods like PointNet, or the more recent point cloud transformer---and its variants---all employ learned per-point embeddings. Despite impressive performance, such approaches are sensitive to out-of-distribution (OOD) noise and outliers. In this paper, we explore the role of an analytical per-point embedding based on the criterion of bandwidth. The concept of bandwidth enables us to draw connections with an alternate per-point embedding---positional embedding, particularly random Fourier features. We present compelling robust results across downstream tasks such as point cloud classification and registration with several categories of OOD noise.",https://openreview.net/forum?id=JER79I1hMg
Poster 3,,228,Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture,"Yixin Chen, Junfeng Ni, Nan Jiang, Yaowei Zhang, Yixin Zhu, Siyuan Huang","Reconstructing detailed 3D scenes from single-view images remains a challenging task due to limitations in existing approaches, which primarily focus on geometric shape recovery, overlooking object appearances and fine shape details. To address these challenges, we propose a novel framework for simultaneous high-fidelity recovery of object shapes and textures from single-view images. Our approach utilizes neural implicit shape and radiance field representations, leveraging explicit 3D shape supervision and volume rendering of color, depth, and surface normal images. To overcome shape-appearance ambiguity under partial observations, we introduce a two-stage learning curriculum that incorporates both 3D and 2D supervisions. A distinctive feature of our framework is its ability to generate fine-grained textured meshes while seamlessly integrating rendering capabilities into the single-view 3D reconstruction model. This integration enables not only improved textured 3D object reconstruction by 27.7 and 11.6 on the 3D-FRONT and Pix3D datasets, respectively, but also supports the rendering of images from novel viewpoints. Beyond individual objects, our approach facilitates composing object-level representations into flexible scene representations, thereby enabling applications such as holistic scene understanding and 3D scene editing. We conduct extensive experiments to demonstrate the effectiveness of our method and will make both the code and model publicly available.",https://openreview.net/forum?id=wgS2rcsD36
Poster 3,,164,Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion,"Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi","We present Farm3D, a method for learning category-specific 3D reconstructors for articulated objects, relying solely on ""free"" virtual supervision from a pre-trained 2D diffusion-based image generator. Recent approaches can learn a monocular network that predicts the 3D shape, albedo, illumination, and viewpoint of any object occurrence, given a collection of single-view images of an object category. However, these approaches heavily rely on manually curated clean training data, which are expensive to obtain. We propose a framework that uses an image generator, such as Stable Diffusion, to generate synthetic training data that are sufficiently clean and do not require further manual curation, enabling the learning of such a reconstruction network from scratch. Additionally, we incorporate the diffusion model as a score to enhance the learning process. The idea involves randomizing certain aspects of the reconstruction, such as viewpoint and illumination, generating virtual views of the reconstructed 3D object, and allowing the 2D network to assess the quality of the resulting image, thus providing feedback to the reconstructor. Unlike work based on distillation, which produces a single 3D asset for each textual prompt in hours, our approach yields a monocular reconstruction network capable of outputting a controllable 3D asset from any given image, whether real or generated, in a single forward pass in a matter of seconds. Our network can be used for analysis, including monocular reconstruction, or for synthesis, generating articulated assets for real-time applications such as video games.",https://openreview.net/forum?id=rwv7TyP8cu
Poster 3,,284,GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar,"Berna Kabadayi, Wojciech Zielonka, Bharat Lal Bhatnagar, Gerard Pons-Moll, Justus Thies","Digital humans and, especially, 3D facial avatars have raised a lot of attention in the past years, as they are the backbone of several applications like immersive telepresence in AR or VR. Despite the progress, facial avatars reconstructed from commodity hardware are incomplete and miss out on parts of the side and back of the head, severely limiting the usability of the avatar. This limitation in prior work stems from their requirement of face tracking, which fails for profile and back views. To address this issue, we propose to learn person-specific animatable avatars from images without assuming to have access to precise facial expression tracking. At the core of our method, we leverage a 3D-aware generative model that is trained to reproduce the distribution of facial expressions from the training data. To train this appearance model, we only assume to have a collection of 2D images with the corresponding camera parameters. For controlling the model, we learn a mapping from 3DMM facial expression parameters to the latent space of the generative model. This mapping can be learned by sampling the latent space of the appearance model and reconstructing the facial parameters from a normalized frontal view where facial expression estimation performs well. With this scheme, we decouple 3D appearance reconstruction and animation control to achieve high fidelity in image synthesis. In a series of experiments, we compare our proposed technique to state-of-the-art monocular methods and show superior quality while not requiring expression tracking of the training data. We will make the code publicly available for research purposes.",https://openreview.net/forum?id=g2UsFOBNH2
Poster 3,,234,NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes,"Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martín Arroyo, Michael Niemeyer, Abhijit Kundu, Federico Tombari","With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward.  At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations.  Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices.",https://openreview.net/forum?id=5gYl4Pg3mK
Poster 3,,37,MonoLSS: Learnable Sample Selection For Monocular 3D Detection,"Zhenjia Li, Jinrang Jia, Yifeng Shi","In the field of autonomous driving, monocular 3D detection is a critical task which estimates 3D properties (depth, dimension, and orientation) of objects in a single RGB image. Previous works have used features in a heuristic way to learn 3D properties, without considering that inappropriate features could have adverse effects. In this paper, sample selection is introduced that only suitable samples should be trained to regress the 3D properties. To select samples adaptively, we propose a Learnable Sample Selection (LSS) module, which is based on Gumbel-Softmax and a relative-distance sample divider. The LSS module works under a warm-up strategy leading to an improvement in training stability. Additionally, since the LSS module dedicated to 3D property sample selection relies on object-level features, we further develop a data augmentation method named MixUp3D to enrich 3D property samples which conforms to imaging principles without introducing ambiguity. As two orthogonal methods, the LSS module and MixUp3D can be utilized independently or in conjunction. Sufficient experiments have shown that their combined use can lead to synergistic effects, yielding improvements that transcend the mere sum of their individual applications. Leveraging the LSS module and the MixUp3D, without any extra data, our method named MonoLSS ranks \textbf{1st} in all three categories (Car, Cyclist, and Pedestrian) on KITTI 3D object detection benchmark, and achieves competitive results on both the Waymo dataset and KITTI-nuScenes cross-dataset evaluation. The code is included in the supplementary material and will be released to facilitate related academic and industrial studies.",https://openreview.net/forum?id=DZJgsp7qKD
Poster 3,,92,Continuous Cost Aggregation for Dual-Pixel Disparity Extraction,"Sagi Monin, Sagi Katz, Georgios Evangelidis","Recent works have shown that depth information can be obtained from Dual-Pixel (DP) sensors.  A DP arrangement provides two views in a single shot, thus resembling a stereo image pair with a tiny baseline. However, the different point spread function (PSF) per view, as well as the small disparity range, makes the use of typical stereo matching algorithms problematic. To address the above shortcomings, we propose a Continuous Cost Aggregation (CCA) scheme within a semi-global matching framework that is able to provide accurate continuous disparities from DP images. The proposed algorithm fits parabolas to matching costs and aggregates parabola coefficients along image paths. The aggregation step is performed subject to a quadratic constraint that not only enforces the disparity smoothness but also maintains the quadratic form of the total costs. This gives rise to an inherently efficient disparity propagation scheme with a pixel-wise minimization in closed-form. Furthermore, the continuous form allows for a robust multi-scale aggregation that better compensates for the varying PSF. Experiments on DP data from both DSLR and phone cameras show that the proposed scheme attains state-of-the-art performance in DP disparity estimation.",https://openreview.net/forum?id=JXJDGxZvQi
Poster 3,,28,GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues,"Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, Michael J. Black","Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract two types of novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to enforce motion temporal consistency in the latent space (LTC), and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP “upgrades” them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets. Our models and code will be available for research purposes.",https://openreview.net/forum?id=Gci7GY9vD9
Poster 3,,299,Control3Diff: Learning Controllable 3D Diffusion Models from Single-view Images,"Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, Joshua M. Susskind","Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging, due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any types of controlling input, allowing us to train it with the same diffusion objective without any auxiliary supervision. We validate the efficacy of Control3Diff on standard image generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various conditioning inputs such as images, sketches, and text prompts.",https://openreview.net/forum?id=AtR5VRRmVG
Poster 4,,174,"Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable, Model-Free Approach using RGB Images","Panwang Pan, Zhiwen Fan, Brandon Yushan Feng, Peihao Wang, Chenxin Li, Zhangyang Wang","The accurate estimation of six degrees-of-freedom (6DoF) object poses is essential for many applications in robotics and augmented reality. However, existing methods for 6DoF pose estimation often depend on CAD templates or dense support views, restricting their usefulness in realworld situations. In this study, we present a new cascade framework named Cas6D for few-shot 6DoF pose estimation that is generalizable and uses only RGB images. To address the false positives of target object detection in the extreme few-shot setting, our framework utilizes a selfsupervised pre-trained ViT to learn robust feature representations. Then, we initialize the nearest top-K pose candidates based on similarity score and refine the initial poses using feature pyramids to formulate and update the cascade warped feature volume, which encodes context at increasingly finer scales. By discretizing the pose search range using multiple pose bins and progressively narrowing the pose search range in each stage using predictions from the previous stage, Cas6D can overcome the large gap between pose candidates and ground truth poses, which is a common failure mode in sparse-view scenarios. Experimental results on the LINEMOD and GenMOP datasets demonstrate that Cas6D outperforms state-of-the-art methods by 9.2 and 3.8 accuracy (Proj-5) under the 32-shot setting compared to OnePose++ and Gen6D.",https://openreview.net/forum?id=lDTFL2uS13
Poster 4,,276,Learning based Infinite Terrain Generation with Level of Detailing,"Aryamaan Jain, Avinash Sharma, K. Rajan","Infinite terrain generation is an important use case for computer graphics, games and simulations. However, current techniques are often procedural which reduces their realism. We introduce a learning-based generative framework for infinite terrain generation along with a novel learning-based approach for level-of-detailing of terrains. Our framework seamlessly integrates with quad-tree-based terrain rendering algorithms. Our approach leverages image completion techniques for infinite generation and progressive super-resolution for terrain enhancement. Notably, we propose a novel quad-tree-based training method for terrain enhancement which enables seamless integration with quad-tree-based rendering algorithms while minimizing the errors along the edges of the enhanced terrain. Comparative evaluations against existing techniques demonstrate our framework's ability to generate highly realistic terrain with effective level-of-detailing.",https://openreview.net/forum?id=TKynRJRgAK
Poster 4,,272,A Benchmark Grocery Dataset of Realworld Point Clouds from Single View,"Shivanand Venkanna Sheshappanavar, Tejas Anvekar, Shivanand Kundargi, Yufan Wang, Chandra Kambhamettu","Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired. Existing datasets on groceries are mainly 2D images. Models trained on these datasets are limited to learning features from the regular 2D grids. While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones. Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery. In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples. Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome. To address these issues, we introduce a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images. We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models. Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks.",https://openreview.net/forum?id=ak03s44aMa
Poster 4,,339,PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Urban Scene Reconstruction,"Fusang WANG, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Guillermo Roldao Jimenez, Dzmitry Tsishkou","Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas such as road surfaces in driving scenarios. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping, simulation and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in patch-based loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstruction for large-scale outdoor scenes and achieves comparable rendering quality to SOTA methods on the KITTI-360 NVS benchmark.",https://openreview.net/forum?id=zJhBMfrKSo
Poster 4,,20,Dynamic Prototype Adaptation with Distillation for Few-shot Point Cloud Segmentation,"Jie Liu, Wenzhe Yin, Haochen Wang, Yunlu Chen, Jan-jakob Sonke, Efstratios Gavves","Few-shot point cloud segmentation seeks to generate per-point mask for previously unseen categories, using only a minimal set of annotated point clouds as reference. Existing prototype-based methods rely on support prototypes to guide the segmentation of query point clouds, but they encounter challenges when significant object variations exist between the support prototypes and query features. In this work, we present dynamic prototype adaptation (DPA), which explicitly learns task-specific prototypes for each query point cloud to tackle the object variations problem. DPA achieves the adaptation through prototype rectification, aligning  vanilla prototypes from support with the query feature distribution, and prototype-to-query attention, extracting task-specific context from query point clouds. Furthermore, we introduce prototype distillation regularization term, enabling knowledge transfer between early-stage prototypes and their deeper counterparts during adaption. By iteratively applying these adaptations, we generate task-specific prototypes for accurate mask predictions on query point clouds. Extensive experiments on two popular benchmarks show that DPA surpasses state-of-the-art methods by a significant margin,  e.g., 7.43 and 6.39 under the 2-way 1-shot setting on S3DIS and ScanNet, respectively. Code will be released.",https://openreview.net/forum?id=tWdFYPtgu3
Poster 4,,216,Mixing-Denoising Generalizable Occupancy Networks,"Amine Ouasfi, Adnane Boukhayma","While current state-of-the-art generalizable implicit neural shape models \cite{peng2020convolutional,boulch2022poco} rely on the inductive bias of convolutions, it is still not entirely clear how properties emerging from such biases are compatible with the task of 3D reconstruction from point cloud.  We explore an alternative approach to generalizability in this context. We relax the intrinsic model bias (\ie using MLPs to encode local features as opposed to convolutions) and constrain the hypothesis space instead with an auxiliary regularization related to the reconstruction task, \ie denoising. The resulting model is the first only-MLP locally conditioned implicit shape reconstruction from point cloud network with fast feed forward inference. Point cloud borne features and denoising offsets are predicted from an exclusively MLP-made network in a single forward pass. A decoder predicts occupancy probabilities for queries anywhere in space by pooling nearby features from the point cloud order-invariantly, guided by denoised relative positional encoding. We outperform the state-of-the-art convolutional method \cite{boulch2022poco} while using half the number of model parameters.",https://openreview.net/forum?id=DichCJ3biJ
Poster 4,,196,SimpleEgo: Predicting probabilistic body pose from egocentric cameras,"Hanz Cuevas Velasquez, Charlie Hewitt, Mohammad Sadegh Aliakbarian, Tadas Baltrusaitis","Our work addresses the problem of egocentric human pose estimation from downwards-facing cameras on head-mounted devices (HMD). This presents a challenging scenario, as parts of the body often fall outside of the image or are occluded. Previous solutions minimize this problem by using fish-eye camera lenses to capture a wider view, but these present hardware design issues. They also predict 2D heat-maps per joint and lift them to 3D space to deal with self-occlusions, but this requires large network architectures which are impractical to deploy on resource-constrained HMDs. We predict pose from images captured using conventional rectilinear camera lenses. This resolves physical design issues, but means body parts are often out of frame. As such, we directly regress probabilistic joint rotations for a parameterized body model and model the joint rotations as probability density functions. This allows us to quantify pose uncertainties and explain out-of-frame or occluded joints. This also removes the need to compute 2D heat-maps and allows for simplified architectures which require less compute resource. Given the lack of any egocentric datasets using rectilinear camera lenses, we generate a large synthetic dataset comprising 60K stereo images with high diversity of pose, shape, clothing and skin tone. Our approach achieves state-of-the-art results for this challenging configuration, reducing mean per-joint position error by 23 overall and 58 for the lower body. Our architecture also has eight times fewer parameters and runs twice as fast as the current state-of-the-art. Experiments show that training on our synthetic dataset leads to good generalization to real world images without fine-tuning.",https://openreview.net/forum?id=TnmuK3lrcF
Poster 4,,183,Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos,"Rama Krishna Prasad Kandukuri, Michael Strecke, Joerg Stueckler","Physics-based understanding of object interactions from sensory observations is an essential capability in augmented reality and robotics. It enables to capture the properties of a scene for simulation and control. In this paper, we propose a novel approach for real-to-sim which tracks rigid objects in 3D from RGB-D images and infers physical properties of the objects. We use a differentiable physics simulation as state-transition model in an Extended Kalman Filter which can model contact and friction for arbitrary mesh-based shapes and in this way estimate physically plausible trajectories. We demonstrate that our approach can filter position, orientation, velocities, and concurrently can estimate the coefficient of friction of the objects. We analyse our approach on various sliding scenarios in synthetic image sequences of single objects and colliding objects. We also demonstrate and evaluate our approach on a real-world dataset. We will make our novel benchmark datasets publicly available to foster future research in this novel problem setting and comparison with our method.",https://openreview.net/forum?id=jLC16x9X5q
Poster 4,,189,PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression,"Jiahao Pang, Kevin Bui, Dong Tian","The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framework is proposed. We unify typical point cloud representations---point-based, voxel-based, and tree-based representations---and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels. Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel transformer for feature aggregation. Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds.",https://openreview.net/forum?id=jB6lnRl1kG
Poster 4,,373,A Cross Branch Fusion-based Contrastive Framework for Point Cloud Self-supervised Learning,"Chengzhi Wu, Qianliang Huang, kun jin, Julius Pfrommer, Jürgen Beyerer","Contrastive learning is an essential method in self-supervised learning. It primarily employs a multi-branch strategy to compare latent representations obtained from different branches and train the encoder. In the case of multi-modal input, diverse modalities of the same object are fed into distinct branches. When using single-modal data, the same input undergoes various augmentations before being fed into different branches. However, all existing contrastive learning frameworks have so far only performed contrastive operations on the learned features at the final loss end, with no information exchange between different branches prior to this stage. In this paper, for point cloud unsupervised learning without the use of extra training data, we propose a Contrastive Cross-branch Attention-based framework for Point cloud data (termed PoCCA), to learn rich 3D point cloud representations. By introducing sub-branches, PoCCA allows information exchange between different branches before the loss end. Experimental results demonstrate that in the case of using no extra training data, the representations learned with our self-supervised model achieve state-of-the-art performances when used for downstream tasks on point clouds.",https://openreview.net/forum?id=HcsJrOcJBK
Poster 4,,90,Exploit Spatiotemporal Contextual Information for 3D Single Object Tracking via Memory Networks,"JONGWON RA, Mengmeng Wang, Jianbiao Mei, Shanqi Liu, Yu Yang, Yong Liu","The point cloud-based 3D single object tracking plays an indispensable role in autonomous driving. However, the application of 3D object tracking in the real world is still challenging due to the inherent sparsity and self-occlusion of point cloud data. Therefore, it is necessary to exploit as much useful information from limited data as we can. Since 3D object tracking is a video-level task, the appearance of objects changes gradually over time, and there is rich spatiotemporal contextual information among historical frames. However, existing methods do not fully utilize this information. To address this, we propose a new method called SCTrack, which utilizes a memory-based paradigm to exploit spatiotemporal contextual information. SCTrack incorporates both long-term and short-term memory banks to store the spatiotemporal features of targets from historical frames. By doing so, the tracker can benefit from the entire video sequence and make more informed predictions. Additionally, SCTrack extracts the mask prior to augmenting the target representation, improving the target-background discriminability. Extensive experiments on KITTI, nuScenes, and Waymo Open datasets verify the effectiveness of our proposed method.",https://openreview.net/forum?id=iDSB0xho0B
Poster 4,,205,Robust and Object-aware Motion Generation using Neural Pose Descriptors,"Wanyue Zhang, Rishabh Dabral, Thomas Leimkuehler, Vladislav Golyanik, Marc Habermann, Christian Theobalt","Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalize well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimize for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation pipeline that is trained to seamlessly transition from locomotion to object interaction with the proposed Bidirectional Pose Blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects.",https://openreview.net/forum?id=KB6L99zaXy
Poster 4,,128,Purposer: Putting Human Motion Generation in Context,"Nicolás Ugrinovic, Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, Grégory Rogez, Francesc Moreno-Noguer","We present a novel method to generate human motion to populate 3D indoor scenes that can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds.    State-of-the-art methods are either models specialized to one single setting , require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information. As a consequence, they have limited applicability and rely on costly training data. To address these limitations, we proposed a new method dubbed Purposer based on neural discrete representation learning. Our model is capable of exploiting, in a flexible manner, different types of information already present in open-access large-scale datasets such as AMASS. First, we encode unconditional human motion into a discrete latent space. Second, an auto-regressive generative model, conditioned with key contextual information either with prompting or additive tokens, trained for next-step prediction in this space, synthesizes sequences of latent indices. We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features. In this manner, P purpose can generate realistic motion sequences in diverse test scenes. Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity. Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene.",https://openreview.net/forum?id=MaKBxaXEb0
Poster 4,,320,NeRF-Feat: 6D Object Pose Estimation using Feature Rendering,"Shishir Reddy Vutukur, Heike Brock, Benjamin Busam, Tolga Birdal, Andreas Hutter, Slobodan Ilic","Object pose Estimation is a crucial component in robotic grasping and augmented reality. Learning based approaches typically require training data from a highly accurate CAD model or labeled training data acquired using a complex setup. We address this by learning to estimate pose from weakly labeled data without a known CAD model. We propose to use a NeRF to learn object shape implicitly which is later used to learn view-invariant features in conjunction with CNN using a contrastive loss. While NeRF helps in learning features that are view-consistent, CNN ensures that the learned features respect symmetry. During inference, CNN is used to predict view-invariant features which can be used to establish correspondences with the implicit 3d model in NeRF. The correspondences are then used to estimate the pose in the reference frame of NeRF. Our approach can also handle symmetric objects unlike other approaches using a similar training setup. Specifically, we learn viewpoint invariant, discriminative features using NeRF which are later used for pose estimation. We evaluated our approach on LM, LM-Occlusion, and T-Less dataset and achieved benchmark accuracy despite using weakly labeled data.",https://openreview.net/forum?id=QoCkOLpKKE
Poster 4,,47,TeCH: Text-guided Reconstruction of Lifelike Clothed Humans,"Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, Justus Thies","Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the ""unseen regions"" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the ""indescribable"" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an implicit distance field. Guided by the descriptive prompts + personalized T2I diffusion model, the geometry and texture of the 3D humans are optimized through multi-view Score Distillation Sampling (SDS) and reconstruction losses based on the original observation. TeCH produces high-fidelity 3D clothed humans with consistent & delicate texture, and detailed full-body geometry. Quantitative and qualitative experiments demonstrate that TeCH outperforms the state-of-the-art methods in terms of reconstruction accuracy and rendering quality. The code will be publicly available for research purposes.",https://openreview.net/forum?id=0ys9E2b8qa
Poster 4,,393,Hyper-SNBRDF: Hypernetwork for Neural BRDF using Sinusoidal Activation,"Zhiqiang Li, Xunkun Shen, Xueyang Zhou, Yong Hu, Bowen Li","Densely captured real-world materials require effective compression for rendering, material generation and reconstruction.   Neural networks with high compression rates and the ability to fit complex functions can encode each BRDF into the corresponding network. However, current works that take advantage of single  implicit neural representations are incapable of effectively modeling the high-frequency details of the highlight region. In this paper, we propose an improved compact neural network representation of BRDF data based on the sinusoidal activation. The lightweight network and the periodic activation function improve the fidelity of the reproduction material appearance under the condition of a high compression rate. Furthermore, the method of building a unified model using neural networks can decode all materials from latent space. However, the deep structure of the network model increases memory consumption. To overcome this challenge, we propose a hypernetwork framework compress measured BRDFs to latent space and generate weights for the neural network-based representation of materials. The lightweight implicit representation of BRDF generated through training directly from original materials shows the characteristics of a low memory footprint and high-precision reproduction of appearance. Additionally, we apply the hypernetwork to reconstruct materials from a single image. Thanks to implicit representation of BRDF that can reproduce the appearance with high fidelity, the reflectance properties can be accurately recovered.",https://openreview.net/forum?id=mjLojqHyzv
Poster 4,,246,PU-SDF: Arbitrary-Scale Uniformly Upsampling Point Clouds via Signed Distance Functions,"Shaohui Pan, Yong Xu, Ruotao Xu","Point cloud upsampling is a crucial technique for improving the performance of 3D data understanding, enabling the creation of dense and uniform point clouds from raw and sparse input data. However, most existing methods are limited in their ability to handle different scaling factors, either requiring multiple networks or producing unevenly distributed points. To overcome these challenges, this paper proposes a deep learning method called PU-SDF, which consists of a local-feature based signed distance function network (LSDF-network) and a 3D-grid query points generation module (GPGM). The local-feature based SDF-network can perform point cloud upsampling at arbitrary rates and can be easily adapted to handle previously unseen datasets. Additionally, we propose a novel GPGM that generates uniform and unlimited query points in sparse voxel space with arbitrary resolution. Extensive qualitative and quantitative evaluations demonstrate the superior performance of the PU-SDF method, achieving state-of-the-art point cloud upsampling performance.",https://openreview.net/forum?id=SH2e62Xp9c
Poster 4,,179,BLiSS: Bootstrapped Linear Shape Space,"Sanjeev Muralikrishnan, Duygu Ceylan, Chun-Hao Paul Huang, Niloy Mitra","Morphable models are fundamental to numerous human-centered processes as they offer a simple yet expressive shape space. Creating such morphable models, however, is both tedious and expensive. The main challenge is establishing dense correspondences across raw scans that capture sufficient shape variation. This is often addressed using a mix of significant manual intervention and non-rigid registration. We observe that creating a shape space and solving for dense correspondence are tightly coupled -- while dense correspondence is needed to build shape spaces, an expressive shape space provides a reduced dimensional space to regularize the search. We introduce BLiSS, a method to solve both progressively. Starting from a small set of manually registered scans to bootstrap the process, we enrich the shape space and then use that to get new unregistered scans into correspondence automatically. The critical component of BLiSS is a non-linear deformation model that captures details missed by the low-dimensional shape space, thus allowing progressive enrichment of the space. In the context of body variations, we show that ours produces a shape space at par with state-of-the-art shape spaces (e.g., SMPL, STAR, GHUM), while requiring much fewer (e.g., 5\) manual registrations. We show that BLiSS can be applied to different types of data such as body and face scans. Code will be released upon publication.",https://openreview.net/forum?id=Lpu1vkA1KM
Poster 5,,130,RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation,"Yiqun Zhao, Zibo Zhao, Jing Li, Sixun Dong, Shenghua Gao","Indoor scene generation aims at creating shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations.In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes.Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion,  object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model. Code will be public available",https://openreview.net/forum?id=vQFgkAg6FV
Poster 5,,62,Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding,"Guocheng Qian, Abdullah Hamdi, Xingdi Zhang, Bernard Ghanem","While Standard Transformer (ST) models have achieved impressive success in natural language processing and computer vision, their performance on 3D point clouds is relatively poor. This is mainly due to the limitation of Transformers: a demanding need for extensive training data. Unfortunately, in the realm of 3D point clouds, the availability of large datasets is a challenge, exacerbating the issue of training ST models for 3D tasks. In this work, we solve the data issue of point cloud ST from two perspectives: (i) introducing more inductive bias to reduce the dependency of ST on data, and (ii) relying on cross-modality pretraining.  More specifically, we first introduce Progressive Point Patch Embedding into ST and present a new ST-based point cloud network namely PViT. PViT is shown to be less hungry for data and enables ST to achieve performance comparable to the state-of-the-art. Second, we formulate a simple yet effective pipeline dubbed \textit{Pix4Point} that allows harnessing Transformers pretrained in the image domain to enhance downstream point cloud understanding. This is achieved through a modality-agnostic ST backbone with the help of a tokenizer and decoder specialized in the different domains. Pretrained on a large number of widely available images, significant gains of PViT are observed in the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS, respectively. Our code and models are available in the Appendix and will be publicly available.",https://openreview.net/forum?id=hE2zNKwfnT
Poster 5,,396,– Visual Tomography – Physically Faithful Volumetric Models of Partially Translucent Objects,"David Nakath, Xiangyu Weng, Mengkun She, Kevin Köser","Digital 3D representations of objects and scenes are required for visualization and simulation. When created faithfully from real-world data they can be useful for human or computer-assisted analysis e.g. in natural sciences. Such models can also serve for generating (extra) training data for machine learning approaches in settings where data is expensive or difficult to obtain or where too few training data exists, e.g. to generate novel views, or images in different conditions such as in different media or illuminataion, for training detection and classification tasks. While the vast amount of visual 3D reconstruction approaches focusses on textured object surfaces or shapes, in this contribution we propose a volumetric reconstruction approach that obtains a physical model including the interior of partially translucent objects such as plankton or insects. Our technique photographs the object under different poses in front of a bright white light source and computes absorption and scattering per voxel. It can be interpreted as visual tomography that we solve by inverse raytracing. We suggest a method to convert NeRF representations into a volumetric grid for initialization and illustrate the usefulness of the approach and its steps using two real-world plankton validation sets, the lab-scanned models being finally also relighted and virtually submerged in an underwater scenarios with augmented medium and illumination conditions.",https://openreview.net/forum?id=GNVETIyUnH
Poster 5,,17,Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis,"Jonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan","We present a method that simultaneously addresses the task of novel-view synthesis of dynamic scenes and the task of 6-DOF tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models a static scene as a collection of 3D Gaussian elements that are ``splatted"" together to create a differentiable volume of occupancy and color. To enable modeling of dynamic scenes, we allow Gaussians to persist over time with consistent color, transparency, and shape, but time-varying 6-DOF position. Given a collection of images from different timesteps and cameras, we differentiably optimize these Gaussian elements so as to reconstruct the input. By enforcing local rigidity with the appropriate regularizers, we show that dense 6-DOF tracks naturally emerge as an internal representation. This enables our representation to support a variety of downstream tasks such as novel view synthesis, non-rigid reconstruction and 6-DOF tracking.",https://openreview.net/forum?id=uN0mE04Umt
Poster 5,,197,Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion,"Daniel Kienzle, Julian Lorenz, Katja Ludwig, Rainer Lienhart","We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.",https://openreview.net/forum?id=pJxA0zQLvY
Poster 5,,264,Photometric visibility matrix for the automatic selection of optimal viewpoints,"Vanessa Staderini, Tobias Glück, Roberto Mecca, Petra Gospodnetic, Philipp Schneider, Andreas Kugi","Automated visual quality inspection is a core topic of robotics and computer vision. In industrial applications, the CAD model of the object to be inspected is often known and can be used to generate appropriate sensor poses (viewpoints) from which to inspect the object's surface and assure the quality of its geometry. Current approaches in this field generate optimal viewpoints by evaluating the geometric coverage but the photometric appearance of the object is usually not considered. This lack of photometric information results in a loss of crucial cues to establish actual visibility, especially when the object to inspect presents specular highlights (e.g., polished metal parts) and a complex geometry. In this paper, we propose integrating photometric information into the viewpoint evaluation to consider the object's appearance. To achieve this, we embed a bidirectional reflectance distribution function (BRDF) within the evaluation of the viewpoint candidates. We benchmark different BRDFs with increasingly realistic rendering to prove the concept of our approach. Specifically, we consider the Blinn-Phong and Cook-Torrance reflectance models. Our simulation results demonstrate the suitability and importance of using a photometric approach that considers material properties and selects optimal viewpoints for specific materials.",https://openreview.net/forum?id=tOCTzfjnf4
Poster 5,,331,Self-Supervised Learning of Skeleton-Aware Morphological Representation for 3D Neuron Segments,"Daiyi Zhu, Qihua Chen, Xuejin Chen","Effective morphological analysis of large-scale 3D neural data plays a crucial role in neuroscience research. However, the ultra-scale data volume from high-resolution microscopy imaging makes manual analysis significantly challenging for 3D rendering, morphological analysis, and morphology-based neuron classification. In this paper, we propose a self-supervised approach to learn skeleton-aware morphological representations from ultra-scale 3D segments to support efficient rendering and morphological analysis. Our approach, named ConSkeletonNet, connects skeleton-aware shape simplification and morphology-based neuron classification to enhance the discriminability of learned morphological representations through multi-task joint training. Through experiments on the neuron segments in a full fly brain EM FAFB-FFN1 and a data volume in the cerebral cortex of a human H01, our ConSkeletonNet shows strong superiority in learning skeletal-aware morphological representation both for both neuron segments skeleton extraction and neuron classification. We also apply our ConSkeletonNet to the man-made dataset ShapeNet and achieve state-of-the-art performance in skeleton extraction task.",https://openreview.net/forum?id=SD4whyVhPQ
Poster 5,,181,Correspondence-free online human motion retargeting,"Rim REKIK DIT NEKHILI, Mathieu Marsot, Stefanie Wuhrer, Jean-Sébastien Franco, Anne-Hélène Olivier","We present a novel data-driven framework for unsupervised human motion retargeting that allows to retarget motions between different characters by animating a target subject with the motion of a source subject. Our method is correspondence-free,i.e. neither spatial correspondences between the source and target shapes nor temporal correspondences between differentframes of the source motion are required. This allows to directly animate a target shape with arbitrary sequences of humans in motion, possibly captured using 4D acquisition platforms or consumer devices. Our framework takes into account long-term temporal context of 1 second during retargeting while accounting for surface details. To achieve this, we take inspiration from two lines of existing work: skeletal motion retargeting, which leverages long-term temporal context at the cost of surface detail, and surface-based retargeting, which preserves surface details without considering long-term temporal context. We unify the advantages of these works by combining a geometry-aware deformation model with a skeleton-aware motion transfer approach. During inference, our method runs online, i.e. the input can be processed in a serial way, and retargeting is performed in a single forward pass per frame. Experiments show that including long-term temporal context during training improves the method’s accuracy both in terms of skeletal motion and detail preservation. Furthermore, our method generalizes well on unobserved motions and body shapes. We demonstrate that the proposed framework achieves state-of-the-art results on two test datasets and that it can be used to animate humanoid virtual avatars directly with the raw reconstruction output of a multi-view acquisition platform. We will release our code for research purposes.",https://openreview.net/forum?id=Cockj7Ux7M
Poster 5,,302,Coherent Enhancement of Depth Images and Normal Maps\\using Second-order Geometric Models on Weighted Finite Graphs,"Andreas Görlitz, Michael Moeller, Andreas Kolb","High-quality depth and normal maps play a crucial role in various image processing and computer vision applications. However, the noisy data from sensors requires meticulous pre-processing. Denoising depth and normals separately, can lead to inconsistent geometric information, negatively impacting the performance of applications relying on this data. While some recent studies have suggested joint denoising approaches that aim to achieve coherent geometric representations, these methods are based on simple geometric assumptions, such as piecewise planar regions, and thus suffer from lower accuracy for non-planar geometry.In this paper, we present a versatile non-planar model specifically designed to handle both planar and non-planar geometry. To achieve this, we formulate the underlying problem as a partitioning of our non-planar model parameters on weighted finite graphs. Solving this problem involves utilizing a modified version of the Cut Pursuit algorithm, which efficiently divides input data into regions of similar geometric models. Finally, we leverage these resulting partitions and their associated model parameters to compute a denoised and infilled depth image, along with its coherent normal map.",https://openreview.net/forum?id=Ty42Er8nLX
Poster 5,,178,Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature,"Shengze Jin, Daniel Barath, Marc Pollefeys, Iro Armeni","Point cloud registration has seen recent success with several learning-based methods that focus on correspondence matching, and as such, optimize only for this objective. Following the learning step of correspondence matching, they evaluate the estimated rigid transformation with a RANSAC-like framework. While it is an indispensable component of these methods, it prevents a fully end-to-end training, leaving the objective to minimize the pose error non-served. We present a novel solution, Q-REG, which utilizes rich geometric information to estimate the rigid pose from a single correspondence. Q-REG allows to formalize the robust estimation as an exhaustive search, hence enabling end-to-end training that optimizes over both objectives of correspondence matching and rigid pose estimation. We demonstrate in the experiments that Q-REG is agnostic to the correspondence matching method and provides consistent improvement both when used only in inference and in end-to-end training. It sets a new state-of-the-art on the 3DMatch, KITTI and ModelNet benchmarks. We will make our code repository and trained models public.",https://openreview.net/forum?id=GAnLy7jw8i
Poster 5,,343,BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds,"Corentin Sautier, Gilles Puy, Alexandre Boulch, Vincent Lepetit, Renaud Marlet","We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast, which uses a contrast at the level of points, to the state-of-the-art TARL, which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird’s Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in downstream semantic segmentation.",https://openreview.net/forum?id=urLpYKlBCi
Poster 5,,208,Generating Continual Human Motion in Diverse 3D Scenes,"Aymen Mir, Xavier Puig, Angjoo Kanazawa, Gerard Pons-Moll","We introduce a method to synthesize animator guided human motion across 3D scenes. Given a set of sparse (3 or 4) joint locations (such as the location of a person's hand and two feet) and a seed motion sequence in a 3D scene, our method generates a plausible motion sequence starting from the seed motion while satisfying the constraints imposed by the provided keypoints. We decompose the continual motion synthesis problem into walking along paths and transitioning in and out of the actions specified by the keypoints, which enables long generation of motions that satisfy scene constraints without explicitly incorporating scene information. Our method is trained only using scene agnostic mocap data. As a result, our approach is deployable across 3D scenes with various geometries. For achieving plausible continual motion synthesis without drift, our key contribution is to iteratively generate motion in a goal-centric canonical coordinate frame where the next immediate target is situated at the origin. This idea simplifies the problem for the motion synthesis network as the network always only has to go to the origin. Our model can generate long sequences of diverse actions such as grabbing, sitting and leaning chained together in arbitrary order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport, and ScanNet scenes. Several experiments demonstrate that our method outperforms existing methods that navigate paths in 3D scenes.",https://openreview.net/forum?id=INbrhygt1K
Poster 5,,150,Practical Measurement and Neural Encoding of Hyperspectral Skin Reflectance,"Xiaohui Li, Giuseppe Claudio Guarnera, Arvin Lin, Abhijeet Ghosh","We propose a practical method to measure spectral skin reflectance as well as a spectral BSSRDF model spanning a wide spectral range from 300nm to 1000nm. We employ a practical capture setup consisting of desktop monitors to illuminate human faces in the visible domain to estimate five parameters of spectral chromophore concentrations including melanin, hemoglobin, and $\beta$ carotene concentration, melanin blend-type fraction, and epidermal hemoglobin fraction. The estimated parameters make use of a novel three-stage lookup table search for faster parameter fitting, and drive our skin model for accurate reconstruction of facial skin reflectance response in both the visible domain as well as in the UVA and near-infrared range. We also propose a novel neural network architecture that given our measurements, predicts the five chromophore parameters of our model at the encoder stage and full hyperspectral reflectance response as the output of the decoder stage.",https://openreview.net/forum?id=2ikxgJL1wp
Poster 5,,33,Synthesizing physically plausible human motions in 3D scenes,"Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, Yangang Wang","Synthesizing physically plausible human motions in 3D scenes is a challenging problem. Kinematics-based methods cannot avoid inherent artifacts (e.g., penetration and foot skating) due to the lack of physical constraints. Meanwhile, existing physics-based methods cannot generalize to multi-object scenarios since the policy trained with reinforcement learning has limited modeling capacity. In this work, we present a framework that enables physically simulated characters to perform long-term interaction tasks in diverse, cluttered, and unseen scenes. The key idea is to decompose human-scene interactions into two fundamental processes, interacting and navigating, which motivates us to construct two reusable controllers, i.e. InterCon and NavCon. Specifically, InterCon contains two complementary policies that enable characters to enter and leave the interacting state (e.g., sitting on a chair and getting up). To generate interaction with objects at different places, we further design NavCon, a trajectory following policy, to keep characters’ locomotion in the free space of 3D scenes. Benefiting from the divide and conquer strategy, we can train the policies in simple environments and generalize to complex multi-object scenes. Experimental results demonstrate that our framework can synthesize physically plausible long-term human motions in complex 3D scenes. Code is attached in supplementary materials and will be publicly released.",https://openreview.net/forum?id=lMyrioN4KC
Poster 5,,317,IS-NEAR: Implicit Semantic Neural Engine and Multi-Sensor Data Rendering with 3D Global Feature,"Tiecheng Sun, Wei Zhang, Xingliang Dong, Tao Lin","Data-driven Computer Vision (CV) tasks are still limited by the amount of labeled data.Recently, some semantic NeRFs have been proposed to render and synthesize novel-view semantic labels.Although current NeRF methods achieve spatially consistent color and semantic rendering, the capability of the geometrical representation is limited.This problem is caused by the lack of global information among raysin the traditional NeRFs since they are trained with independent directional rays.To address this problem, we introduce a point-to-surface global feature in NeRF to associate all rays,which enables the single ray representation capability of global geometry. In particular,the relative distance of each sampled ray point to the learned global surfaces is calculated toweight the geometry density and semantic-color feature. We also carefully design the semantic lossand back-propagation function to solve the problems of unbalanced samples andthe disturbance of implicit semantic field to geometric field.The experiments validate the 3D scene annotation capability with few feed labels.The quantification results show thatour method outperforms the state-of-the-art works in efficiency, geometry, color and semantics on the public datasets. The proposed method is also applied to multiple tasks, such as indoor, outdoor,part segmentation labeling, texture re-rendering and robot simulation.",https://openreview.net/forum?id=3R3QUaCIvs
Poster 5,,312,ActiveNeuS: Neural Signed Distance Fields for Active Stereo,"Kazuto Ichimaru, Takaki Ikeda, Diego Thomas, Takafumi Iwaguchi, Hiroshi Kawasaki","3D-shape reconstruction in extreme environments, such as low illumination or scattering condition, has been an open problem and intensively researched.Active stereo is one of potential solution for such environments for its robustness and high accuracy.However, active stereo systems usually consist of specialized system configurations with complicated algorithms, which narrow their application.In this paper, we propose Neural Signed Distance Fieldfor active stereo systems to enable implicit correspondence search and triangulation in generalized Structured Light.With our technique, textureless or equivalent surfaces by low light condition are successfully reconstructed even with a small number of captured images.Experiments were conducted to confirm that the proposed method could achieve state-of-the-art reconstruction quality under such severe condition.We also demonstrated that the proposed method worked in an underwater scenario.",https://openreview.net/forum?id=TZ2fQ50kTU
Poster 5,,156,MixRT: Mixed Neural Representations For Real-Time NeRF Rendering,"Chaojian Li, Bichen Wu, Peter Vajda, Yingyan Celine Lin","Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry that is represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation, that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices ($>$30 FPS at a resolution of 1280 $\times$ 720 on a Macbook M1 Pro laptop), better rendering quality (0.2 PSNR higher on indoor scenes of the Unbounded-360 datasets), and smaller storage size ($<$80\) compared to state-of-the-art methods.",https://openreview.net/forum?id=29bLJOkCNO
Poster 5,,27,Color-NeuS: Reconstructing Neural Implicit Surfaces with Color,"Licheng Zhong, Lixin Yang, Kailin Li, Haoyu Zhen, Mei Han, Cewu Lu","The reconstruction of object surfaces from multi-view images or monocular video is a fundamental issue in computer vision. However, much of the recent research concentrates on reconstructing geometry through implicit or explicit methods. In this paper, we shift our focus towards reconstructing mesh in conjunction with color. We remove the view-dependent color from neural volume rendering while retaining volume rendering performance through a relighting network. Mesh is extracted from the signed distance function (SDF) network for the surface, and color for each surface vertex is drawn from the global color network. To evaluate our approach, we conceived a in hand object scanning task featuring numerous occlusions and dramatic shifts in lighting conditions. We've gathered several videos for this task, and the results surpass those of any existing methods capable of reconstructing mesh alongside color. Additionally, our method's performance was assessed using public datasets, including DTU, BlendedMVS, and OmniObject3D. The results indicated that our method performs well across all these datasets. Our code and data will be released.",https://openreview.net/forum?id=QTdCH9H6Yf
Poster 6,,51,HumanReg: Self-supervised Non-rigid Registration of Human Point Cloud,"Yifan Chen, Zhiyu Pan, Zhi Chen Zhong, Wenxuan Guo, Jianjiang Feng, Jie Zhou","In this paper, we present a novel registration framework, HumanReg, that learns a non-rigid transformation between two human point clouds end-to-end. We introduce body prior into the registration process to efficiently handle this type of point cloud. Unlike most exsisting supervised registration techniques that require expensive point-wise flow annotations, HumanReg can be trained in a self-supervised manner benefiting from a set of novel loss functions. To make our model better converge on real-world data, we also propose a pretraining strategy, and a synthetic dataset (HumanSyn4D) consists of dynamic, sparse human point clouds and their auto-generated ground truth annotations. Our experiments shows that HumanReg achieves state-of-the-art performance on CAPE-512 dataset and gains a qualitative result on another more chanllenging real-world dataset. Furthermore, our ablation studies demonstrate the effectiveness of our synthetic dataset and novel loss functions. Our code and synthetic dataset will be released soon.",https://openreview.net/forum?id=8aRbSTVItQ
Poster 6,,55,LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation,"Chen Zhao, Yinlin Hu, Mathieu Salzmann","Object location prior has been shown to be critical for the standard 6D object pose estimation setting, where the training and testing objects are the same. Specifically, the prior can be used to initialize the 3D object translation and facilitate 3D object rotation estimation. Unfortunately, the object detectors that are used for this purpose do not generalize to unseen objects, i.e., objects from new categories at test time. Therefore, existing 6D pose estimation methods for previously-unseen objects either assume the ground-truth object location to be known, or yield inaccurate results when it is unavailable. In this paper, we address this problem by developing a method, LocPoseNet, able to robustly learn location prior for unseen objects. Our method builds upon a template matching strategy, where we propose to distribute the reference kernels and convolve them with a query to efficiently compute multi-scale correlations. We then introduce a novel translation estimator, which decouples scale-aware and scale-robust features to predict different object location parameters. Our method outperforms existing works by a large margin on LINEMOD and GenMOP. We further construct a challenging synthetic dataset, which allows us to highlight the better robustness of our method to various noise sources.",https://openreview.net/forum?id=hqgrS1SXMZ
Poster 6,,138,Range-Agnostic Multi-View Depth Estimation with Keyframe Selection,"Andrea Conti, Matteo Poggi, Valerio Cambareri, Stefano Mattoccia","Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios -- e.g., outdoor 3D reconstruction from video sequences -- therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. We achieve state-of-the-art performance on Blended and TartanAir, two challenging benchmarks featuring posed video frames in various scenarios, and demonstrate generalization capabilities and stereo perception applicability on UnrealStereo4K. Finally, we show that our framework is accurate in controlled environments with fixed depth ranges, such as those featured in the DTU dataset.",https://openreview.net/forum?id=rVIwpCOXrU
Poster 6,,338,DAC: Detector-Agnostic Spatial Covariances for Deep Local Features,"Javier Tirado-Garín, Frederik Rahbæk Warburg, Javier Civera","Current deep visual local feature detectors do not model the spatial uncertainty of detected features, producing suboptimal results in downstream applications. In this work, we propose two post-hoc covariance estimates that can be plugged into any pretrained deep feature detector: a simple, isotropic covariance estimate that uses the predicted score at a given pixel location, and a full covariance estimate via the local structure tensor of the learned score maps. Both methods are easy to implement and can be applied to any deep feature detector. We show that these covariances are directly related to errors in feature matching, leading to improvements in downstream tasks, including solving the perspective-n-point problem and motion-only bundle adjustment.",https://openreview.net/forum?id=LVhQe3WTsZ
Poster 6,,191,OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection,"Zhangyang Qi, Jiaqi Wang, Xiaoyang Wu, Hengshuang Zhao","Multi-view 3D object detection is becoming popular in autonomous driving due to its high effectiveness and low cost. Most of the current state-of-the-art detectors follow the query-based bird's-eye-view (BEV) paradigm, which benefits from both BEV's strong perception power and end-to-end pipeline. Despite achieving substantial progress, existing works model objects via globally leveraging temporal and spatial information of BEV features, resulting in problems when handling the challenging complex and dynamic autonomous driving scenarios. In this paper, we proposed an Object-Centric query-BEV detector OCBEV, which can carve the temporal and spatial cues of moving targets more effectively. OCBEV comprises three designs: Object Aligned Temporal Fusion aligns the BEV feature based on ego-motion and estimated current locations of moving objects, leading to a precise instance-level feature fusion. Object Focused Multi-View Sampling samples more 3D features from an adaptive local height ranges of objects for each scene to enrich foreground information. Object Informed Query Enhancement replaces part of pre-defined decoder queries in common DETR-style decoders with positional features of objects on high-confidence locations, introducing more direct object positional priors. Extensive experimental evaluations are conducted on the challenging nuScenes dataset. Our approach achieves a state-of-the-art result, surpassing the traditional BEVFormer by 1.5 NDS points. Moreover, we have a faster convergence speed and only need half of the training iterations to get comparable performance, which further demonstrates its effectiveness.",https://openreview.net/forum?id=jGifzPNIjo
Poster 6,,291,Deep Event Visual Odometry,"Simon Klenk, Marvin Motzet, Lukas Koestler, Daniel Cremers","Event cameras offer the exciting possibility to track the camera's pose during high speed motion and in adverse light conditions. Despite this promise, existing event-based monocular visual odometry (VO) approaches demonstrate limited performance on recent benchmarks. To address this limitation, some methods resort to additional sensors such as IMUs, stereo event cameras, or frame-based cameras.Nonetheless this limits the application of event cameras in real-world devices and products since it increases cost and complicates system requirements. Moreover, relying on a frame-based camera makes the system still susceptible to motion blur and HDR. To remove the dependency on additional sensors and to push the limits of using only a single event camera, we present Deep Event VO (DEVO), the first monocular event-only system with strong performance across a large number of real-world benchmarks. DEVO sparsely tracks selected event patches over time. A key component of DEVO is a novel deep patch selection mechanism tailored to event data. We significantly increase state-of-the-art pose tracking accuracy on seven real-world benchmarks compared to event-only methods, and often surpass or are close to stereo or inertial methods.",https://openreview.net/forum?id=r07xjKaFAF
Poster 6,,18,PanoSSC: Exploring Monocular Panoptic 3D Scene  Reconstruction for Autonomous Driving,"Yining Shi, Jiusi Li, KUN JIANG, Ke Wang, Yunlong Wang, mengmeng yang, Diange Yang","Vision-centric occupancy networks, which represent the surrounding environment with uniform semantic voxels, have become a new trend for safe driving of camera-only autonomous driving perception systems, as they are able to detect obstacles regardless of their shape and occlusion. Modern occupancy networks mainly focus on reconstructing visible voxels from object surfaces with voxel-wise semantic prediction. Usually, they suffer from inconsistent predictions of one object and mixed predictions for adjacent objects. These confusions may harm the safety of downstream planning modules. To this end, we investigate panoptic segmentation on 3D voxel scenarios and propose an instance-aware occupancy network, PanopVoxel. We predict foreground objects and backgrounds separately and merge both in post-processing. For foreground instance grouping, we propose a novel 3D instance mask decoder that can efficiently extract individual objects. we unify geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation into PanopVoxel framework and propose new metrics for evaluating panoptic voxels. Extensive experiments show that our method achieves competitive results on SemanticKITTI semantic scene completion benchmark.",https://openreview.net/forum?id=y2mMDXmWFC
Poster 6,,123,Addressing Low-Shot MVS by Detecting and Completing Planar Surfaces,"Rajbir Kataria, Zhizhong Li, Joseph DeGol, Derek Hoiem","Multiview stereo (MVS) systems typically require at least three views to reconstruct each scene point. This requirement increases the burden of image captures and leads to incomplete reconstructions. Our main idea to address this low-shot MVS problem is to detect planar surfaces in depth maps generated by any MVS system and complete these surfaces by reformulating the MVS depth prediction task to a simpler planar surface assignment problem. We use single and multi-view cues (when available) and employ the DeepLabv3 architecture to infer the extent of planar regions and accurately complete missing surfaces. We show that our approach reconstructs portions of surfaces viewed by only one image, yielding denser models than existing MVS systems.",https://openreview.net/forum?id=0UY9S0Khql
Poster 6,,58,YOLO-6D-Pose:Enhancing YOLO for Single-Stage Monocular Multi-Object 6D Pose Estimation,"Debapriya Maji, Soyeb N Nagori, Manu Mathew, Deepak Poddar","Directly regressing 6 degrees of freedom for all the objects from a single RGB image is not well explored. Even end-to-end pose estimation approaches for a single object are inferior compared to state-of-the-art methods in terms  of accuracy. Most 6D pose estimation frameworks are multi-stage relying on off-the-shelf deep networks for object and keypoint detection to establish correspondences between 3D object keypoints and 2D image locations. This is followed by applying a variant of a RANSAC-based Perspective-n-Point (PnP) followed by complex refinement operation. In this work, we propose a multi-object 6D pose  estimation framework by enhancing the popular YOLOX object detector. The network is end-to-end trainable and detects each object along with its pose from a single RGB image without any additional post-processing . We show that by properly parameterizing the 6D pose and carefully designing the loss function, we can achieve state-of-the- art accuracy without further refinement or any intermediate representations. YOLO-6D-pose achieves SOTA results on YCBV and LMO dataset, surpassing all existing monocular approaches. We systematically analyze various 6D augmentations to verify their correctness and propose a new translation augmentation for this task. The network doesn’t rely on any correspondences and is independent of the CAD model during inference. Our training code will be made publicly available.",https://openreview.net/forum?id=36lRaXHl3D
Poster 6,,330,CoARF: Controllable 3D Artistic Style Transfer for Radiance Fields,"Deheng Zhang, Clara Fernandez Labrador, Christopher Schroers","Creating artistic 3D scenes can be time-consuming and requires specialized knowledge. To address this, recent works such as ARF, use a radiance field-based approach with style constraints to generate 3D scenes that resemble a style image provided by the user. However, these methods lack fine-grained control over the resulting scenes. In this paper, we introduce Controllable Artistic Radiance Fields (CoARF), a novel algorithm for controllable 3D scene stylization. CoARF enables style transfer for specified objects, compositional 3D style transfer and semantic-aware style transfer. We achieve controllability using segmentation masks with different label-dependent loss functions. We also propose a semantic-aware nearest neighbor matching algorithm to improve the style transfer quality. Our extensive experiments demonstrate that CoARF provides user-specified controllability of style transfer and superior style transfer quality with more precise feature matching.",https://openreview.net/forum?id=g7bBX87MdL
Poster 6,,100,Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor,"Junlin Song, Antoine Richard, Miguel Olivares-Mendez","In robotics, motion capture systems have been widely used to measure the accuracy of localization algorithms. Moreover, this infrastructure can also be used for other computer vision tasks, such as the evaluation of Visual (-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic annotation. Yet, to work optimally, these functionalities require having accurate and reliable spatial-temporal calibration parameters between the camera and the global pose sensor. In this study, we provide two novel solutions to estimate these calibration parameters. Firstly, we design an offline target-based method with high accuracy and consistency. Spatial-temporal parameters, camera intrinsic, and trajectory are optimized simultaneously. Then, we propose an online target-less method, eliminating the need for a calibration target and enabling the estimation of time-varying spatial-temporal parameters. Additionally, we perform detailed observability analysis for the target-less method. Our theoretical findings regarding observability are validated by simulation experiments and provide explainable guidelines for calibration. Finally, the accuracy and consistency of two proposed methods are evaluated with hand-held real-world datasets where traditional hand-eye calibration method do not work.",https://openreview.net/forum?id=Pa792k1A7L
Poster 6,,223,Geometrically Consistent Partial Shape Matching,"Viktoria Ehm, Paul Roetzer, Marvin Eisenberger, Maolin Gao, Florian Bernard, Daniel Cremers","Finding correspondences between 3D shapes is a crucial problem in computer vision and graphics, which is for example relevant for tasks like shape interpolation, pose transfer, or texture transfer. An often neglected but essential property of matchings is geometric consistency, which means that neighboring triangles in one shape are consistently matched to neighboring triangles in the other shape. Moreover, while in practice one often has only access to partial observations of a 3D shape (e.g. due to occlusion, or scanning artifacts), there do not exist any methods that directly address geometrically consistent partial shape matching. In this work we fill this gap by proposing to integrate state-of-the-art deep shape features into a novel integer linear programming shape matching formulation. Our optimization yields a globally optimal solution on low resolution shapes, which we then refine using a coarse-to-fine scheme.We show that our method can find more reliable results on partial shapes in comparison to existing geometrically consistent algorithms (for which one first has to fill missing parts with a dummy geometry). Moreover, our matchings are substantially smoother than learning-based state-of-the-art shape matching methods.",https://openreview.net/forum?id=gNki68gZiO
Poster 6,,287,Occlusion Resilient 3D Human Pose Estimation,"SOUMAVA KUMAR ROY, Ilia Badanin, Sina Honari, Pascal Fua","Occlusions remain one of the key challenges in 3D body pose estimation from single-camera video sequences. Temporal consistency has been extensively used to mitigate their impact but the existing algorithms in the literature do not explicitly model them. Here, we apply this by representing the deforming body as a spatio-temporal graph. We then introduce a refinement network that performs graph convolutions over this graph to output 3D poses. To ensure robustness to occlusions, we train this network with a set of binary masks that we use to disable some of the edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods of time and train the network to be immune to that. We demonstrate the effectiveness of this approach compared to state-of-the-art techniques that infer poses from single-camera sequences.",https://openreview.net/forum?id=C46XnAjUvG
Poster 6,,66,RIVQ-VAE: Discrete Rotation-invariant 3D Representation Learning,"Mariem MEZGHANNI, Malika Boulkenafed, Maks Ovsjanikov","Building local surface representations has recently attracted significant attention in 3D vision, allowing to structure complex 3D shapes as sequences of simpler local geometries. Inspired by advances in 2D discrete representation learning, recent approaches have proposed to break up 3D shapes into regular grids, where each cell is associated with a discrete code sampled from a learnable codebook. Unfortunately, existing methods ignore the ambiguities inherent to 3D geometry, especially related to possible translation and rotation on local and global levels. As a result, such techniques require very large codebooks to capture all possible variability in both geometry and pose. In this work, we propose a novel local-based generative model that improves the generation quality by compactly embedding local geometries in a rotation- and translation-invariant manner. This strategy enables our codebook of discrete codes to express a larger range of local geometric forms by avoiding redundancies induced by changes in pose. Crucially, we demonstrate via a careful architecture design that our approach allows to recover meaningful shapes from local embeddings, by disentangling local pose estimation and geometry reconstruction, while ensuring global consistency. The conducted experiments show that our approach outperforms baseline methods by a large margin under similar settings.",https://openreview.net/forum?id=E4eKEG5nCv
Poster 6,,371,DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid,"Mirgahney Mohamed, Lourdes Agapito","We propose DynamicSurf, a model-free neural implicit surface reconstruction method for high-fidelity 3D modelling of non-rigid surfaces from monocular RGB-D video. To cope with the lack of multi-view cues in monocular sequences of deforming surfaces, one of the most challenging settings for 3D reconstruction, DynamicSurf exploits depth, surface normals, and RGB losses to improve reconstruction fidelity and optimisation time. DynamicSurf learns a neural deformation field that maps a canonical representation of the surface geometry to the current frame. We depart from current neural non-rigid surface reconstruction models by designing the canonical representation as a learned feature grid which leads to faster and more accurate surface reconstruction than competing approaches that use a single MLP. We demonstrate DynamicSurf on public datasets and show that it can optimize  sequences of varying frames with $6\times$ speedup over pure MLP-based approaches while achieving comparable results to the state-of-the-art methods.",https://openreview.net/forum?id=nh1w8BWYwz
Poster 6,,60,SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes,"Edith Tretschk, Vladislav Golyanik, Michael Zollhöfer, Aljaz Bozic, Christoph Lassner, Christian Theobalt","Existing methods for the 4D reconstruction of general, non-rigidly deforming objects focus on novel-view synthesis and neglect correspondences. However, time consistency enables advanced downstream tasks like 3D editing, motion analysis, or virtual-asset creation. We propose SceNeRFlow to reconstruct a general, non-rigid scene in a time-consistent manner. Our dynamic-NeRF method takes multi-view RGB videos and background images from static cameras with known camera parameters as input. It then reconstructs the deformations of an estimated canonical model of the geometry and appearance in an online fashion. Since this canonical model is time-invariant, we obtain correspondences even for long-term, long-range motions. We employ neural scene representations to parametrize the components of our method. Like prior dynamic-NeRF methods, we use a backwards deformation model. We find non-trivial adaptations of this model necessary to handle larger motions: We decompose the deformations into a strongly regularized coarse component and a weakly regularized fine component, where the coarse component also extends the deformation field into the space surrounding the object, which enables tracking over time. We show experimentally that, unlike prior work that only handles small motion, our method enables the reconstruction of studio-scale motions.",https://openreview.net/forum?id=l0s0IwjiUr
Poster 6,,268,CloSe: A 3D Clothing Segmentation Dataset and Model,"Dimitrije Antic, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin, Gerard Pons-Moll","3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale 3D clothing segmentation dataset of 2939 scans and 18 classes. Additionally, we propose CloSe-Net, the first learning-based 3D segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a new garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn the appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining GT labels or network predictions. Combining the tool with CloSe-Net in a continual learning setup demonstrates improved generalization on real-world data. We will release the dataset, model, and tool.",https://openreview.net/forum?id=i3w5bkQu7Y