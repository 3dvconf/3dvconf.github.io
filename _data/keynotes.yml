2022:
 - name: "Matthias Niessner"
   title: "The Revolution of Neural Rendering"
   abstract: "In this talk, I will present our research vision in how to create a photo-realistic digital replica of the real world, and how to make holograms become a reality. Eventually, I would like to see photos and videos evolve to become interactive, holographic content indistinguishable from the real world. Imagine taking such 3D photos to share with friends, family, or social media; the ability to fully record historical moments for future generations; or to provide content for upcoming augmented and virtual reality applications. AI-based approaches, such as generative neural networks, are becoming more and more popular in this context since they have the potential to transform existing image synthesis pipelines. I will specifically talk about an avenue towards neural rendering where we can retain the full control of a traditional graphics pipeline but at the same time exploit modern capabilities of deep learning, such as handling the imperfections of content from commodity 3D scans. While the capture and photo-realistic synthesis of imagery open up unbelievable possibilities for applications ranging from entertainment to communication industries, there are also important ethical considerations that must be kept in mind. Specifically, in the content of fabricated news (e.g., fake-news), it is critical to highlight and understand digitally-manipulated content. I believe that media forensics plays an important role in this area, both from an academic standpoint to better understand image and video manipulation, but even more importantly from a societal standpoint to create and raise awareness around the possibilities and moreover, to highlight potential avenues and solutions regarding trust of digital content."
   bio: "Dr. Matthias Nießner is a Professor at the Technical University of Munich where he leads the Visual Computing Lab. Before, he was a Visiting Assistant Professor at Stanford University. Prof. Nießner’s research lies at the intersection of computer vision, graphics, and machine learning, where he is particularly interested in cutting-edge techniques for 3D reconstruction, semantic 3D scene understanding, video editing, and AI-driven video synthesis. In total, he has published over 70 academic publications, including 22 papers at the prestigious ACM Transactions on Graphics (SIGGRAPH / SIGGRAPH Asia) journal and 43 works at the leading vision conferences (CVPR, ECCV, ICCV); several of these works won best paper awards, including at SIGCHI’14, HPG’15, SPG’18, and the SIGGRAPH’16 Emerging Technologies Award for the best Live Demo. Prof. Nießner’s work enjoys wide media coverage, with many articles featured in main-stream media including the New York Times, Wall Street Journal, Spiegel, MIT Technological Review, and many more, and his was work led to several TV appearances such as on Jimmy Kimmel Live, where Prof. Nießner demonstrated the popular Face2Face technique; Prof. Nießner’s academic Youtube channel currently has over 5 million views. For his work, Prof. Nießner received several awards: he is a TUM-IAS Rudolph Moessbauer Fellow (2017 – ongoing), he won the Google Faculty Award for Machine Perception (2017), the Nvidia Professor Partnership Award (2018), as well as the prestigious ERC Starting Grant 2018 which comes with 1.500.000 Euro in research funding; in 2019, he received the Eurographics Young Researcher Award honoring the best upcoming graphics researcher in Europe. In addition to his academic impact, Prof. Nießner is a co-founder and director of Synthesia Inc., a brand-new startup backed by Marc Cuban, whose aim is to empower storytellers with cutting-edge AI-driven video synthesis."
   prez: Virtual
   url: https://niessnerlab.org/ 
   image: img/2022/people/niessner.jpg
   order: 1
   index: 1
 - name: "Vincent Lepetit"
   title: "New Problems in 3D Object Pose Estimation"
   abstract: "3D (or 6D, or 9D) pose estimation of objects from images has made tremendous progress over the recent past years, but is it really enough to answer all needs, especially for the industry?  Requirements for training time and for labeled data are still often a deal breaker to transfer these developments to production.  In the first part of this talk, I will present our recent work on dealing with new objects without specific training on them: How to detect them, how to estimate their 6D pose, how to track them.  I will then present our work on creating accurate annotations of real images automatically for training and evaluating 3D algorithms: In particular, we recently developed a method based on the Monte Carlo Tree Search (MCTS) algorithm to retrieve CAD models for indoor scenes from noisy RGB-D scans without human input."
   bio: "Vincent Lepetit is a Director of Research at ENPC ParisTech, France.  Prior to this position, he was a full professor at the Institute for Computer Graphics and Vision, Graz University of Technology (TU Graz), Austria and before that, a senior researcher at CVLab, Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland.  He also still leads a research group as an associate professor.  His current research focuses on 3D scene understanding, especially at trying to reduce the supervision needed by a system to learn new 3D objects and new 3D environments.  In the past, he has worked on vision-based Augmented Reality, Machine Learning and Deep Learning, in particular, their application to 3D registration, 3D object pose estimation, feature point detection and description and geo-localization from images.  He received the Koenderick “test-of-time” award at the European Conference on Computer Vision 2020 for “Brief: Binary Robust Independent Elementary Features”. He often serves as an area chair of major computer vision conferences (CVPR, ICCV, ECCV, ACCV, BMVC) and as an editor for the International Journal of Computer Vision (IJCV) and the Computer Vision and Image Understanding (CVIU) journal."
   prez: In Person
   image: img/2022/people/vincent_lepetit_hawai.jpg
   url: https://vincentlepetit.github.io/ 
   order: 2
   index: 2
 - name: "Otmar Hilliges"
   title: "Human-Centric 3D Computer Vision for Future AI Systems"
   abstract: "Future AI systems such as personalized healthcare robots, self-driving cars, and AR/VR-based telepresence systems, will only be safe, useful and widely adopted if they are able to interpret human pose, shape and appearance at levels rivaling our own; and if they can interact with us and the world in a human-like and natural fashion. This requires perceiving and analyzing human behavior from images. It also requires generation, control and synthesis of virtual humans. To this end we propose a novel representation of human pose, shape and appearance that combines the advantages of neural implicit surfaces with those of parametric body models: i) a continuous and resolution-independent surface representation that can capture highly detailed geometry and can naturally model topology changes, ii) coupled with the ease of use and generalization capabilities to unseen shapes and poses of polygonal mesh-based models. We also introduce algorithms to learn such representations without requiring manually specified skinning weights or other forms of direct supervision.  We then discuss how to leverage this representation to reconstruct controllable avatars (full body, faces and more) directly from images, videos or short RGB-D sequences via differentiable rendering. Finally, to make 3D human avatars widely available, we will discuss work towards generative modeling of 3D virtual humans with diverse identities and shapes in arbitrary poses and of interactions with 3D objects in a physically plausible manner."
   bio: "Otmar Hilliges is a Professor of Computer Science at ETH Zurich, where he leads the AIT lab (https://ait.ethz.ch) and serves as head of the Institute for Intelligent Interactive Systems (https://iis.ethz.ch). Otmar’s research is in spatio-temporal understanding of how humans move within and interact with the physical world. He researches algorithms, methods and representations for human- and interaction-centric understanding of our world from videos, images and other sensor data. He is interested in many different application domains such as Augmented and Virtual Reality, Human Robot Interaction and more. Prior to joining ETH, he was a Researcher at Microsoft Research Cambridge (2012-2013). His Diplom (equiv. MSc) in Computer Science is from Technische Universität München, Germany (2004) and his PhD in Computer Science from LMU München, Germany (2009). He spent two years as a postdoc at Microsoft Research Cambridge (2010-2012). He has published more than 100 peer-reviewed papers in the major venues on computer vision, computer graphics and HCI. 20+ patents have been filed in his name on a variety of subjects from surface reconstruction to AR/VR. Amongst other sources of funding, Otmar Hilliges is a recipient of the prestigious ERC starting grant and ERC consolidator grant."
   prez: In Person
   image: img/2022/people/otmarhilliges.jpg
   url: https://ait.ethz.ch/people/hilliges/ 
   order: 3
   index: 3
 - name: "Angjoo Kanazawa"
   title: "Towards 4D Reality Capture"
   abstract: "We live in a dynamic three dimensional world that is full of life, with agents like people who interact with each other and their environment in their daily life. But motion is not only restricted to people, but everywhere we see: in the leaves rusted by the breeze, a bird chirping in the tree, a passing cloud over the sun.. How can we perceive and capture this 4D world? In this talk, I will paint the ambitious goal of capturing the photorealistic 4D world in a casual manner, like from everyday smartphone videos. While this is quite a challenge, with recent advances in analysis-by-synthesis and 3D neural field representations like Neural Radiance Fields (NeRFs), we have come far in the ability to capture the static 3D world in a photorealistic manner. I will discuss the recent advancements that make static 3D capture practical, and then discuss the challenges thereby in capturing the dynamic, non-rigid world in the general case from a monocular capture setup. I will then discuss the progress that can be made on deformable objects with known kinematic structure and 3D poses, and recent advancements in 3D perception of people from videos that can come into play."
   bio: "Angjoo Kanazawa is an Assistant Professor in the Department of Electrical Engineering and Computer Science at the University of California at Berkeley. Her research is at the intersection of Computer Vision, Computer Graphics, and Machine Learning, focusing on the visual perception of the dynamic 3D world behind everyday photographs and video. Previously, she was a research scientist at Google NYC with Noah Snavely, and prior to that she was a BAIR postdoc at UC Berkeley advised by Jitendra Malik, Alyosha Efros, and Trevor Darrell. She completed her PhD in Computer Science at the University of Maryland, College Park with her advisor David Jacobs. She also spent time at the Max Planck Institute for Intelligent Systems with Michael Black. She has been named a Rising Star in EECS and is a recipient of Anita Borg Memorial Scholarship, Best Paper Award in Eurographics 2016, Google Research Scholar Award 2021, and a Spark Fellow 2022. She also serves on the advisory board of Wonder Dynamics, whose goal is to utilize AI technologies to make VFX effects more accessible for indie filmmakers."
   prez: Virtual
   image: img/2022/people/kanazawa.jpg
   url: https://www2.eecs.berkeley.edu/Faculty/Homepages/kanazawa.html
   order: 4
   index: 4
 - name: "Lourdes Agapito"
   title: "Learning 3D Representations of Shape and Deformations"
   abstract: "As humans we take the ability to perceive the dynamic world around us in three dimensions for granted. From an early age we can grasp an object by adapting our fingers to its 3D shape; or effortlessly navigate through a busy street. These tasks require some internal 3D representation of shape, deformations, and motion. Building algorithms that can emulate human 3D perception, using as input single images or video sequences taken with a consumer camera, has proved to be an extremely hard task. Machine learning solutions have faced the challenge of the scarcity of 3D annotations, encouraging important advances in weak and self-supervision. In this talk I will describe progress from early optimization-based solutions that captured sequence-specific 3D models with primitive representations of deformation, towards recent and more powerful 3D-aware neural representations that can learn the variation of shapes and textures across a category and be trained from 2D image supervision only. There has been very successful recent commercial uptake of this technology and I will show exciting applications to AI-driven video synthesis."
   bio: "Lourdes Agapito holds the position of Professor of 3D Vision at the Department of Computer Science, University College London (UCL). Her research in computer vision has consistently focused on the inference of 3D information from single images or videos acquired from a moving camera. She received her BSc, MSc and PhD degrees from the Universidad Complutense de Madrid (Spain). In 1997 she joined the Robotics Research Group at the University of Oxford as an EU Marie Curie Postdoctoral Fellow. In 2001 she was appointed as Lecturer at the Department of Computer Science at Queen Mary University of London. From 2008 to 2014 she held an ERC Starting Grant funded by the European Research Council to focus on theoretical and practical aspects of deformable 3D reconstruction from monocular sequences. In 2013 she joined the Department of Computer Science at University College London and was promoted to full professor in 2015. Lourdes serves regularly as Area Chair for the top Computer Vision conferences (CVPR, ICCV, ECCV) was Program Chair for CVPR 2016 and will serve again for ICCV 2023. She was keynote speaker at ICRA 2017 and ICLR 2021. In 2017 she co-founded Synthesia, the London based synthetic media startup responsible for the AI technology behind the Malaria no More video campaign that saw David Beckham speak 9 different languages to call on world leaders to take action to defeat Malaria."
   prez: In Person
   image: img/2022/people/agapito.jpeg
   url: http://www0.cs.ucl.ac.uk/staff/L.Agapito/
   order: 5
   index: 5
 - name: "Yasutaka Furukawa"
   title: "Teaching a Computer to be an Architect"
   abstract: "I will present our recent work on structured geometry reconstruction and generation, which help architects with their workflows. For reconstruction, I will talk about vector floorplan reconstruction from scanned floorplan images or RGBD images acquired on-site: What the key insights were and how we changed the landscape of floorplan reconstruction in the last 5 years. For generation, I will talk about the graph-constrained floorplan generation work (House-GAN): How we fused a reconstruction technique with GAN to build the system. Lastly, I will share my views of how the relationships of structured reconstruction and generation (two once very distant fields) are changing recently."
   bio: "Dr. Yasutaka Furukawa is an associate professor in the School of Computing Science at Simon Fraser University (SFU). Dr. Furukawa's group has made fundamental and practical contributions to 3D reconstruction algorithms, improved localization techniques, and computational architectural modeling. Their open-source software has been widely adopted by tech companies used in surprising applications such as 3D printing of turtle shells and archaeological reconstruction. Dr. Furukawa received the best student paper award at ECCV 2012, the NSF CAREER Award in 2015, CS-CAN Outstanding Young CS Researcher Award 2018, Google Faculty Research Awards in 2016, 2017, and 2018, and PAMI Longuet-Higgins prize in 2020."
   prez: Virtual
   image: img/2022/people/furukawa.jpeg
   url: https://www2.cs.sfu.ca/~furukawa/
   order: 6
   index: 6


2021:
 - name: Michael Black
   title:  "Learning digital humans for the Metaverse"
   abstract: "The Metaverse will require artificial humans that interact with real humans as well as with real and virtual 3D worlds. This requires a real-time understanding of humans and scenes as well as the generation of natural and appropriate behavior. We approach the problem of creating such embodied human behavior through capture, modeling, and synthesis. First, we learn realistic and expressive 3D human avatars from 3D scans. We then train neural networks to estimate human pose and shape from images and video. Specifically, we focus on humans interacting with each other and the 3D world. By capturing people in action, we are able to train neural networks to model and generate human movement and human-scene interaction. To validate our models, we synthesize virtual humans in novel 3D scenes. The goal is to produce realistic human avatars that interact with virtual worlds in ways that are indistinguishable from real humans."
   bio: "Michael Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. from Yale University (1992). He has held positions at the University of Toronto, Xerox PARC, and Brown University. He is one of the founding directors at the Max Planck Institute for Intelligent Systems in Tübingen, Germany, where he leads the Perceiving Systems department. He is a Distinguished Amazon Scholar and an Honorarprofessor at the University of Tuebingen. His work has won several awards including the IEEE Computer Society Outstanding Paper Award (1991), Honorable Mention for the Marr Prize (1999 and 2005), the 2010 Koenderink Prize, the 2013 Helmholtz Prize, and the 2020 Longuet-Higgins Prize. He is a member of the German National Academy of Sciences Leopoldina and a foreign member of the Royal Swedish Academy of Sciences. In 2013 he co-founded Body Labs Inc., which was acquired by Amazon in 2017."
   image: img/2021/keynote/black.jpg
   order: 1
   index: 1   
 
 - name: "Imari Sato"
   title: "Spectral signature analysis for scene understanding"
   abstract: "The spectral absorption of objects provides innate information about material properties that have proven useful in applications such as classification, synthetic relighting, and medical imaging, to name a few. In recent years, the photoacoustic imaging technique (PAI) has received attention. PAI utilizes the photoacoustic effect for shape recovery that materials emit acoustic signals under light irradiation.  What makes PAI different from ordinary 3D sensing is PAI can provide 3D geometric structure associated with wavelength-dependent absorption information of the interior of a target in a non-invasive manner. In this talk, I will introduce various shape recovery methods focusing on the properties of light such as absorption and refraction: 3D modeling by PAI,  shape from water, shape from chromatic aberration, and shape from fluorescence."
   bio: "Imari Sato received the BS degree in policy management from Keio University in 1994. After studying at Robotics Institute of Carnegie Mellon University as a visiting scholar, she received the MS and Ph.D. degrees in interdisciplinary Information Studies from the University of Tokyo in 2002 and 2005, respectively. In 2005, she joined the National Institute of Informatics, where she is currently a professor. Concurrently, she serves as a visiting professor at Tokyo Institute of Technology and a professor at the University of Tokyo. Her primary research interests are in the fields of computer vision (physics-based vision, spectral analysis, image-based modeling). She has received various research awards, including The Young Scientists’ Prize from The Commendation for Science and Technology by the Minister of Education, Culture, Sports, Science and Technology (2009), and Microsoft Research Japan New Faculty award (2011)."
   image: img/2021/keynote/sato.jpg  
   order: 2
   index: 4
 
 - name: "Richard Newcombe"
   title: "Foundational Predictive 3D Models of Reality in the Metaverse, Robotics and the Future of AI"
   abstract: "Future robots, AI assistants and mixed reality in the Metaverse will be greatly enhanced by enabling rich predictive 3D models with far greater context about our physical reality than is possible today. Such models will require a live and updating model of reality capturing what is in our environments and how we interact within them. To obtain this context we will need to observe reality at greater sensing fidelity than has previously been possible, wherever we are, whenever we need it. Upcoming generations of robots and wearable devices like AR glasses enable always-on mobile sensing, computations and communications to push the observability of reality to the limit - observing reality change as a user or robot interacts in the environment. In this talk I will provide an overview of key stages in building a scalable, predictive 3D model of reality that is kept up-to-date by egocentric data from AR glasses and future robots. I'll chart a course in my research that has persuaded me this is the foundation on which contextualized AI can be built to power the future of robotics, AI assistants and mixed reality worlds in the Metaverse."
   bio: "Richard Newcombe is Director of Research at Reality Labs Research, Meta. His team at RL-R is developing LiveMaps - a new generation of always-on 3D computer vision and machine perception technologies, devices, and infrastructure to unlock the potential of Augmented Reality and Contextualized AI. He received his PhD from Imperial College in London with a Postdoctoral Fellowship at the University of Washington and went on to co-found Surreal Vision that was acquired by Facebook in 2015. His original research introduced the Dense SLAM paradigm demonstrated in KinectFusion, DTAM and DynamicFusion, impacting a generation of real-time and interactive systems being developing in the emerging fields of AR/VR and robotics. His interests span sub-disciplines across machine perception and machine learning from hardware-software sensor device co-design to computer vision algorithms and novel infrastructure research."
   image: img/2021/keynote/newcombe.jpg  
   order: 3
   index: 3
 
 - name: "Katerina Fragkiadaki"
   title: "Modular  3D neural scene representations for visuomotor control and language grounding" 
   abstract: "Current state-of-the-art perception models localize rare  object categories in images, yet often miss basic facts that a two-year-old has mastered: that objects have 3D extent, they persist over time despite changes in the camera view, they do not 3D intersect, and others.  We will discuss models that learn to  map 2D and 2.5D  images and videos into amodal completed  3D feature maps of the scene and the objects in it by predicting views. We will show the proposed models learn object permanence, have objects  emerge in 3D without human annotations, can ground language in 3D visual simulations, and learn intuitive physics and controllers that generalize across scene arrangements and camera configurations. In this way, the proposed world-centric  scene representations  overcome many limitations of  image-centric representations for video understanding, model learning and language grounding."   
   bio: "Katerina Fragkiadaki is an Assistant Professor in the Machine Learning Department in Carnegie Mellon University. She received her Ph.D. from University of Pennsylvania and was a postdoctoral fellow in UC Berkeley and Google research after that. Her work is on learning visual representations with little supervision and on combining spatial reasoning in deep visual learning. Her group develops algorithms for mobile computer vision, learning of physics and common sense for agents that move around and interact with the world. Her work has been awarded with a best Ph.D. thesis award, an NSF CAREER award, AFOSR Young Investigator award, a DARPA Young Investigator award, Google, TRI, Amazon and Sony faculty research awards."   
   image: img/2021/keynote/fragkiadaki.png
   order: 4
   index: 2
  
